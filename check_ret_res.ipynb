{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from /mnt/new_volume/outputs_ret/retrieval_result.json\n",
    "\n",
    "with open('/mnt/new_volume/outputs_ret/retrieval_chunk_1_top200_result.json', 'r') as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_23200',\n",
       "  'title': None,\n",
       "  'text': 'def _build_q_network ( self , obs , obs_space , action_space ) : qnet = QNetwork ( ModelCatalog . get_model ( { \"obs\" : obs , \"is_training\" : self . _get_is_training_placeholder ( ) , } , obs_space , action_space , self . num_actions , self . config [ \"model\" ] ) , self . num_actions , self . config [ \"dueling\" ] , self . config [ \"hiddens\" ] , self . config [ \"noisy\" ] , self . config [ \"num_atoms\" ] , self . config [ \"v_min\" ] , self . config [ \"v_max\" ] , self . config [ \"sigma0\" ] , self . config [ \"parameter_noise\" ] ) return qnet . value , qnet . logits , qnet . dist , qnet . model',\n",
       "  'score': '93.940414',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_839620',\n",
       "  'title': None,\n",
       "  'text': \"def deep_q_network ( ) : @ tt . model ( tracker = tf . train . ExponentialMovingAverage ( 1 - .0005 ) , # TODO: replace with original weight freeze optimizer = tf . train . RMSPropOptimizer ( .00025 , .95 , .95 , .01 ) ) def q_network ( x ) : x /= 255 x = layers . conv2d ( x , 32 , 8 , 4 ) x = layers . conv2d ( x , 64 , 4 , 2 ) x = layers . conv2d ( x , 64 , 3 , 1 ) x = layers . flatten ( x ) x = layers . fully_connected ( x , 512 ) x = layers . fully_connected ( x , env . action_space . n , activation_fn = None ) x = tf . identity ( x , name = 'Q' ) return x return q_network\",\n",
       "  'score': '93.84311',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_839622',\n",
       "  'title': None,\n",
       "  'text': \"def dqn_test ( env = 'OneRoundDeterministicReward-v0' ) : def make_env ( env = env ) : e = gym . make ( env ) e = ObservationShapeWrapper ( e ) return e env = make_env ( ) env_test = make_env ( ) @ tt . model ( tracker = tf . train . ExponentialMovingAverage ( 1 - .01 ) , optimizer = tf . train . AdamOptimizer ( .001 ) ) def q_network ( x ) : x = layers . fully_connected ( x , 32 ) x = layers . fully_connected ( x , env . action_space . n , activation_fn = None , weights_initializer = tf . random_normal_initializer ( 0 , 1e-4 ) ) return x dqn = DQN ( env . action_space . n , env . observation_space . shape , q_network ) agent = dqn . make_agent ( ) agent_test = dqn . make_agent ( test = True ) for ep in range ( 4000 ) : r = agent . run_episode ( env ) if ep > 64 : dqn . train_step ( ) if ep % 100 == 0 : rs = [ agent_test . run_episode ( env ) for _ in range ( 100 ) ] print ( f'Return after episode {ep} is {sum(rs)/len(rs)}' )\",\n",
       "  'score': '91.6089',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_23853',\n",
       "  'title': None,\n",
       "  'text': 'def _build_q_network ( self , obs , obs_space , action_space , actions ) : if self . config [ \"use_state_preprocessor\" ] : q_model = ModelCatalog . get_model ( { \"obs\" : obs , \"is_training\" : self . _get_is_training_placeholder ( ) , } , obs_space , action_space , 1 , self . config [ \"model\" ] ) q_out = tf . concat ( [ q_model . last_layer , actions ] , axis = 1 ) else : q_model = None q_out = tf . concat ( [ obs , actions ] , axis = 1 ) activation = getattr ( tf . nn , self . config [ \"critic_hidden_activation\" ] ) for hidden in self . config [ \"critic_hiddens\" ] : q_out = layers . fully_connected ( q_out , num_outputs = hidden , activation_fn = activation ) q_values = layers . fully_connected ( q_out , num_outputs = 1 , activation_fn = None ) return q_values , q_model',\n",
       "  'score': '90.55593',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_839414',\n",
       "  'title': None,\n",
       "  'text': \"def dqn_test ( env = 'OneRoundDeterministicReward-v0' ) : env = gym . make ( env ) env = ObservationShapeWrapper ( env ) @ tt . model ( tracker = tf . train . ExponentialMovingAverage ( 1 - .01 ) , optimizer = tf . train . AdamOptimizer ( .01 ) ) def q_network ( x ) : x = layers . fully_connected ( x , 32 ) x = layers . fully_connected ( x , env . action_space . n , activation_fn = None , weights_initializer = tf . random_normal_initializer ( 0 , 1e-4 ) ) return x agent = DqnAgent ( env , q_network , double_dqn = False , replay_start = 100 , annealing_time = 100 ) rs = [ ] for ep in range ( 10000 ) : r , _ = agent . play_episode ( ) rs . append ( r ) if ep % 100 == 0 : print ( f'Return after episode {ep} is {sum(rs)/len(rs)}' ) rs = [ ]\",\n",
       "  'score': '90.23944',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_42614',\n",
       "  'title': None,\n",
       "  'text': 'def craft_adv ( self ) : with self . sess . as_default ( ) : with self . g . as_default ( ) : craft_adv_obs = deepq . build_adv ( make_obs_tf = lambda name : U . Uint8Input ( self . env . observation_space . shape , name = name ) , q_func = dueling_model if self . dueling else model , num_actions = self . env . action_space . n , epsilon = 1.0 / 255.0 , noisy = self . noisy , ) return craft_adv_obs',\n",
       "  'score': '89.738174',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_22478',\n",
       "  'title': None,\n",
       "  'text': \"def __init__ ( self , model , data , session = None , learning_phase_flags = None ) : # first, we need to find the framework if type ( model ) is tuple : a , b = model try : a . named_parameters ( ) framework = 'pytorch' except : framework = 'tensorflow' else : try : model . named_parameters ( ) framework = 'pytorch' except : framework = 'tensorflow' if framework == 'tensorflow' : self . explainer = TFDeepExplainer ( model , data , session , learning_phase_flags ) elif framework == 'pytorch' : self . explainer = PyTorchDeepExplainer ( model , data ) self . expected_value = self . explainer . expected_value\",\n",
       "  'score': '88.803085',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_257282',\n",
       "  'title': None,\n",
       "  'text': 'def create ( model : ModelFactory , discount_factor : float , target_update_frequency : int , max_grad_norm : float , double_dqn : bool = False ) : return DistributionalDeepQLearning ( model_factory = model , discount_factor = discount_factor , double_dqn = double_dqn , target_update_frequency = target_update_frequency , max_grad_norm = max_grad_norm )',\n",
       "  'score': '88.72157',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_839926',\n",
       "  'title': None,\n",
       "  'text': \"def test_dqn ( env = 'Chain-v0' ) : import gym_mix env = gym . make ( env ) def pp ( x ) : x = layers . fully_connected ( x , 32 ) x = layers . fully_connected ( x , 32 ) return x def head ( x ) : x = layers . fully_connected ( x , 32 ) x = layers . fully_connected ( x , env . action_space . n , activation_fn = None , weights_initializer = tf . random_normal_initializer ( 0 , 1e-4 ) ) return x agent = BootstrappedDQNAg ( env , pp , head , replay_start = 64 ) for ep in range ( 100000 ) : R , _ = agent . play_episode ( ) if ep % 100 == 0 : print ( f'Return after episode {ep} is {R}' )\",\n",
       "  'score': '88.485725',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_138163',\n",
       "  'title': None,\n",
       "  'text': 'def initialize_model ( self ) : return QDemoModel ( states = self . states , actions = self . actions , scope = self . scope , device = self . device , saver = self . saver , summarizer = self . summarizer , execution = self . execution , batching_capacity = self . batching_capacity , variable_noise = self . variable_noise , states_preprocessing = self . states_preprocessing , actions_exploration = self . actions_exploration , reward_preprocessing = self . reward_preprocessing , update_mode = self . update_mode , memory = self . memory , optimizer = self . optimizer , discount = self . discount , network = self . network , distributions = self . distributions , entropy_regularization = self . entropy_regularization , target_sync_frequency = self . target_sync_frequency , target_update_weight = self . target_update_weight , # DQFD always uses double dqn, which is a required key for a q-model. double_q_model = True , huber_loss = self . huber_loss , expert_margin = self . expert_margin , supervised_weight = self . supervised_weight , demo_memory_capacity = self . demo_memory_capacity , demo_batch_size = self . demo_batch_size )',\n",
       "  'score': '88.431786',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_839416',\n",
       "  'title': None,\n",
       "  'text': \"def __init__ ( self , env : gym . Env , q_network : tt . Model , memory = None , double_dqn = True , replay_start = 50000 , annealing_time = 1000000 ) : self . annealing_time = annealing_time self . replay_start = replay_start so = env . observation_space . shape self . env = env self . memory = memory or ReplayMemory ( 1000000 ) def act ( x : [ so ] ) : qs = q_network ( x ) a = tf . argmax ( qs , axis = 1 ) # qm = tf.reduce_max(qs, axis=1) layers . summarize_tensor ( a ) return a , qs self . act = Function ( act ) def train ( o : [ so ] , a : ( tf . int32 , [ [ ] ] ) , r , t : tf . bool , o2 : [ so ] ) : q = q_network ( o ) # ac = tf.argmax(q, axis=1) # compute targets q2 = q_network . tracked ( o2 ) if double_dqn : a2 = tf . argmax ( q_network ( o2 ) , axis = 1 ) # yep, that's really the only difference else : a2 = tf . argmax ( q2 , axis = 1 ) mask2 = tf . one_hot ( a2 , env . action_space . n , 1.0 , 0.0 , axis = 1 ) q_target = tf . where ( t , r , r + 0.99 * tf . reduce_sum ( q2 * mask2 , axis = 1 ) ) q_target = tf . stop_gradient ( q_target ) # compute loss mask = tf . one_hot ( a , env . action_space . n , 1.0 , 0.0 , axis = 1 ) qs = tf . reduce_sum ( q * mask , axis = 1 , name = 'q_max' ) td = tf . subtract ( q_target , qs , name = 'td' ) # td = tf.clip_by_value(td, -10, 10) # loss = tf.reduce_mean(tf.abs(td), axis=0, name='mae') # loss = tf.where(tf.abs(td) < 1.0, 0.5 * tf.square(td), tf.abs(td) - 0.5, name='mse_huber') loss = tf . reduce_mean ( tf . square ( td ) , axis = 0 , name = 'mse' ) loss = q_network . minimize ( loss ) # logging layers . summarize_tensors ( [ td , loss , r , o , a , tf . subtract ( o2 , o , name = 'state_dif' ) , tf . reduce_mean ( tf . cast ( t , tf . float32 ) , name = 'frac_terminal' ) , tf . subtract ( tf . reduce_max ( q , 1 , True ) , q , name = 'av_advantage' ) ] ) # layers.summarize_tensors(chi.activations()) # layers.summarize_tensors(chi.gradients()) return loss self . train = Function ( train , prefetch_fctn = lambda : self . memory . sample_batch ( ) [ : - 1 ] , prefetch_capacity = 3 , async = True ) def log_weigths ( ) : v = q_network . trainable_variables ( ) # print(f'log weights {v}') f = q_network . tracker_variables # print(f'log weights EMA {f}') difs = [ ] for g in v : a = q_network . tracker . average ( g ) difs . append ( tf . subtract ( g , a , name = f'ema/dif{g.name[:-2]}' ) ) layers . summarize_tensors ( v + f + difs ) self . log_weights = Function ( log_weigths , async = True ) def log_returns ( real_return : [ ] , ret : [ ] , qs ) : layers . summarize_tensors ( [ real_return , ret , qs , tf . subtract ( ret , qs , name = 'R-Q' ) ] ) self . log_returns = Function ( log_returns , async = True ) self . t = 0\",\n",
       "  'score': '88.222244',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_13850',\n",
       "  'title': None,\n",
       "  'text': 'def build_q_func ( network , hiddens = [ 256 ] , dueling = True , layer_norm = False , * * network_kwargs ) : if isinstance ( network , str ) : from baselines . common . models import get_network_builder network = get_network_builder ( network ) ( * * network_kwargs ) def q_func_builder ( input_placeholder , num_actions , scope , reuse = False ) : with tf . variable_scope ( scope , reuse = reuse ) : latent = network ( input_placeholder ) if isinstance ( latent , tuple ) : if latent [ 1 ] is not None : raise NotImplementedError ( \"DQN is not compatible with recurrent policies yet\" ) latent = latent [ 0 ] latent = layers . flatten ( latent ) with tf . variable_scope ( \"action_value\" ) : action_out = latent for hidden in hiddens : action_out = layers . fully_connected ( action_out , num_outputs = hidden , activation_fn = None ) if layer_norm : action_out = layers . layer_norm ( action_out , center = True , scale = True ) action_out = tf . nn . relu ( action_out ) action_scores = layers . fully_connected ( action_out , num_outputs = num_actions , activation_fn = None ) if dueling : with tf . variable_scope ( \"state_value\" ) : state_out = latent for hidden in hiddens : state_out = layers . fully_connected ( state_out , num_outputs = hidden , activation_fn = None ) if layer_norm : state_out = layers . layer_norm ( state_out , center = True , scale = True ) state_out = tf . nn . relu ( state_out ) state_score = layers . fully_connected ( state_out , num_outputs = 1 , activation_fn = None ) action_scores_mean = tf . reduce_mean ( action_scores , 1 ) action_scores_centered = action_scores - tf . expand_dims ( action_scores_mean , 1 ) q_out = state_score + action_scores_centered else : q_out = action_scores return q_out return q_func_builder',\n",
       "  'score': '88.1765',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_22333',\n",
       "  'title': None,\n",
       "  'text': \"def __init__ ( self , model , data , session = None , batch_size = 50 , local_smoothing = 0 ) : # first, we need to find the framework if type ( model ) is tuple : a , b = model try : a . named_parameters ( ) framework = 'pytorch' except : framework = 'tensorflow' else : try : model . named_parameters ( ) framework = 'pytorch' except : framework = 'tensorflow' if framework == 'tensorflow' : self . explainer = _TFGradientExplainer ( model , data , session , batch_size , local_smoothing ) elif framework == 'pytorch' : self . explainer = _PyTorchGradientExplainer ( model , data , batch_size , local_smoothing )\",\n",
       "  'score': '88.09209',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_23194',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , observation_space , action_space , config ) : config = dict ( ray . rllib . agents . dqn . dqn . DEFAULT_CONFIG , * * config ) if not isinstance ( action_space , Discrete ) : raise UnsupportedSpaceException ( \"Action space {} is not supported for DQN.\" . format ( action_space ) ) self . config = config self . cur_epsilon = 1.0 self . num_actions = action_space . n # Action inputs self . stochastic = tf . placeholder ( tf . bool , ( ) , name = \"stochastic\" ) self . eps = tf . placeholder ( tf . float32 , ( ) , name = \"eps\" ) self . cur_observations = tf . placeholder ( tf . float32 , shape = ( None , ) + observation_space . shape ) # Action Q network with tf . variable_scope ( Q_SCOPE ) as scope : q_values , q_logits , q_dist , _ = self . _build_q_network ( self . cur_observations , observation_space , action_space ) self . q_values = q_values self . q_func_vars = _scope_vars ( scope . name ) # Noise vars for Q network except for layer normalization vars if self . config [ \"parameter_noise\" ] : self . _build_parameter_noise ( [ var for var in self . q_func_vars if \"LayerNorm\" not in var . name ] ) self . action_probs = tf . nn . softmax ( self . q_values ) # Action outputs self . output_actions , self . action_prob = self . _build_q_value_policy ( q_values ) # Replay inputs self . obs_t = tf . placeholder ( tf . float32 , shape = ( None , ) + observation_space . shape ) self . act_t = tf . placeholder ( tf . int32 , [ None ] , name = \"action\" ) self . rew_t = tf . placeholder ( tf . float32 , [ None ] , name = \"reward\" ) self . obs_tp1 = tf . placeholder ( tf . float32 , shape = ( None , ) + observation_space . shape ) self . done_mask = tf . placeholder ( tf . float32 , [ None ] , name = \"done\" ) self . importance_weights = tf . placeholder ( tf . float32 , [ None ] , name = \"weight\" ) # q network evaluation with tf . variable_scope ( Q_SCOPE , reuse = True ) : prev_update_ops = set ( tf . get_collection ( tf . GraphKeys . UPDATE_OPS ) ) q_t , q_logits_t , q_dist_t , model = self . _build_q_network ( self . obs_t , observation_space , action_space ) q_batchnorm_update_ops = list ( set ( tf . get_collection ( tf . GraphKeys . UPDATE_OPS ) ) - prev_update_ops ) # target q network evalution with tf . variable_scope ( Q_TARGET_SCOPE ) as scope : q_tp1 , q_logits_tp1 , q_dist_tp1 , _ = self . _build_q_network ( self . obs_tp1 , observation_space , action_space ) self . target_q_func_vars = _scope_vars ( scope . name ) # q scores for actions which we know were selected in the given state. one_hot_selection = tf . one_hot ( self . act_t , self . num_actions ) q_t_selected = tf . reduce_sum ( q_t * one_hot_selection , 1 ) q_logits_t_selected = tf . reduce_sum ( q_logits_t * tf . expand_dims ( one_hot_selection , - 1 ) , 1 ) # compute estimate of best possible value starting from state at t + 1 if config [ \"double_q\" ] : with tf . variable_scope ( Q_SCOPE , reuse = True ) : q_tp1_using_online_net , q_logits_tp1_using_online_net , q_dist_tp1_using_online_net , _ = self . _build_q_network ( self . obs_tp1 , observation_space , action_space ) q_tp1_best_using_online_net = tf . argmax ( q_tp1_using_online_net , 1 ) q_tp1_best_one_hot_selection = tf . one_hot ( q_tp1_best_using_online_net , self . num_actions ) q_tp1_best = tf . reduce_sum ( q_tp1 * q_tp1_best_one_hot_selection , 1 ) q_dist_tp1_best = tf . reduce_sum ( q_dist_tp1 * tf . expand_dims ( q_tp1_best_one_hot_selection , - 1 ) , 1 ) else : q_tp1_best_one_hot_selection = tf . one_hot ( tf . argmax ( q_tp1 , 1 ) , self . num_actions ) q_tp1_best = tf . reduce_sum ( q_tp1 * q_tp1_best_one_hot_selection , 1 ) q_dist_tp1_best = tf . reduce_sum ( q_dist_tp1 * tf . expand_dims ( q_tp1_best_one_hot_selection , - 1 ) , 1 ) self . loss = self . _build_q_loss ( q_t_selected , q_logits_t_selected , q_tp1_best , q_dist_tp1_best ) # update_target_fn will be called periodically to copy Q network to # target Q network update_target_expr = [ ] assert len ( self . q_func_vars ) == len ( self . target_q_func_vars ) , ( self . q_func_vars , self . target_q_func_vars ) for var , var_target in zip ( self . q_func_vars , self . target_q_func_vars ) : update_target_expr . append ( var_target . assign ( var ) ) self . update_target_expr = tf . group ( * update_target_expr ) # initialize TFPolicyGraph self . sess = tf . get_default_session ( ) self . loss_inputs = [ ( SampleBatch . CUR_OBS , self . obs_t ) , ( SampleBatch . ACTIONS , self . act_t ) , ( SampleBatch . REWARDS , self . rew_t ) , ( SampleBatch . NEXT_OBS , self . obs_tp1 ) , ( SampleBatch . DONES , self . done_mask ) , ( PRIO_WEIGHTS , self . importance_weights ) , ] LearningRateSchedule . __init__ ( self , self . config [ \"lr\" ] , self . config [ \"lr_schedule\" ] ) TFPolicyGraph . __init__ ( self , observation_space , action_space , self . sess , obs_input = self . cur_observations , action_sampler = self . output_actions , action_prob = self . action_prob , loss = self . loss . loss , model = model , loss_inputs = self . loss_inputs , update_ops = q_batchnorm_update_ops ) self . sess . run ( tf . global_variables_initializer ( ) ) self . stats_fetches = dict ( { \"cur_lr\" : tf . cast ( self . cur_lr , tf . float64 ) , } , * * self . loss . stats )',\n",
       "  'score': '88.08362',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_13834',\n",
       "  'title': None,\n",
       "  'text': 'def learn ( env , network , seed = None , lr = 5e-4 , total_timesteps = 100000 , buffer_size = 50000 , exploration_fraction = 0.1 , exploration_final_eps = 0.02 , train_freq = 1 , batch_size = 32 , print_freq = 100 , checkpoint_freq = 10000 , checkpoint_path = None , learning_starts = 1000 , gamma = 1.0 , target_network_update_freq = 500 , prioritized_replay = False , prioritized_replay_alpha = 0.6 , prioritized_replay_beta0 = 0.4 , prioritized_replay_beta_iters = None , prioritized_replay_eps = 1e-6 , param_noise = False , callback = None , load_path = None , * * network_kwargs ) : # Create all the functions necessary to train the model sess = get_session ( ) set_global_seeds ( seed ) q_func = build_q_func ( network , * * network_kwargs ) # capture the shape outside the closure so that the env object is not serialized # by cloudpickle when serializing make_obs_ph observation_space = env . observation_space def make_obs_ph ( name ) : return ObservationInput ( observation_space , name = name ) act , train , update_target , debug = deepq . build_train ( make_obs_ph = make_obs_ph , q_func = q_func , num_actions = env . action_space . n , optimizer = tf . train . AdamOptimizer ( learning_rate = lr ) , gamma = gamma , grad_norm_clipping = 10 , param_noise = param_noise ) act_params = { \\'make_obs_ph\\' : make_obs_ph , \\'q_func\\' : q_func , \\'num_actions\\' : env . action_space . n , } act = ActWrapper ( act , act_params ) # Create the replay buffer if prioritized_replay : replay_buffer = PrioritizedReplayBuffer ( buffer_size , alpha = prioritized_replay_alpha ) if prioritized_replay_beta_iters is None : prioritized_replay_beta_iters = total_timesteps beta_schedule = LinearSchedule ( prioritized_replay_beta_iters , initial_p = prioritized_replay_beta0 , final_p = 1.0 ) else : replay_buffer = ReplayBuffer ( buffer_size ) beta_schedule = None # Create the schedule for exploration starting from 1. exploration = LinearSchedule ( schedule_timesteps = int ( exploration_fraction * total_timesteps ) , initial_p = 1.0 , final_p = exploration_final_eps ) # Initialize the parameters and copy them to the target network. U . initialize ( ) update_target ( ) episode_rewards = [ 0.0 ] saved_mean_reward = None obs = env . reset ( ) reset = True with tempfile . TemporaryDirectory ( ) as td : td = checkpoint_path or td model_file = os . path . join ( td , \"model\" ) model_saved = False if tf . train . latest_checkpoint ( td ) is not None : load_variables ( model_file ) logger . log ( \\'Loaded model from {}\\' . format ( model_file ) ) model_saved = True elif load_path is not None : load_variables ( load_path ) logger . log ( \\'Loaded model from {}\\' . format ( load_path ) ) for t in range ( total_timesteps ) : if callback is not None : if callback ( locals ( ) , globals ( ) ) : break # Take action and update exploration to the newest value kwargs = { } if not param_noise : update_eps = exploration . value ( t ) update_param_noise_threshold = 0. else : update_eps = 0. # Compute the threshold such that the KL divergence between perturbed and non-perturbed # policy is comparable to eps-greedy exploration with eps = exploration.value(t). # See Appendix C.1 in Parameter Space Noise for Exploration, Plappert et al., 2017 # for detailed explanation. update_param_noise_threshold = - np . log ( 1. - exploration . value ( t ) + exploration . value ( t ) / float ( env . action_space . n ) ) kwargs [ \\'reset\\' ] = reset kwargs [ \\'update_param_noise_threshold\\' ] = update_param_noise_threshold kwargs [ \\'update_param_noise_scale\\' ] = True action = act ( np . array ( obs ) [ None ] , update_eps = update_eps , * * kwargs ) [ 0 ] env_action = action reset = False new_obs , rew , done , _ = env . step ( env_action ) # Store transition in the replay buffer. replay_buffer . add ( obs , action , rew , new_obs , float ( done ) ) obs = new_obs episode_rewards [ - 1 ] += rew if done : obs = env . reset ( ) episode_rewards . append ( 0.0 ) reset = True if t > learning_starts and t % train_freq == 0 : # Minimize the error in Bellman\\'s equation on a batch sampled from replay buffer. if prioritized_replay : experience = replay_buffer . sample ( batch_size , beta = beta_schedule . value ( t ) ) ( obses_t , actions , rewards , obses_tp1 , dones , weights , batch_idxes ) = experience else : obses_t , actions , rewards , obses_tp1 , dones = replay_buffer . sample ( batch_size ) weights , batch_idxes = np . ones_like ( rewards ) , None td_errors = train ( obses_t , actions , rewards , obses_tp1 , dones , weights ) if prioritized_replay : new_priorities = np . abs ( td_errors ) + prioritized_replay_eps replay_buffer . update_priorities ( batch_idxes , new_priorities ) if t > learning_starts and t % target_network_update_freq == 0 : # Update target network periodically. update_target ( ) mean_100ep_reward = round ( np . mean ( episode_rewards [ - 101 : - 1 ] ) , 1 ) num_episodes = len ( episode_rewards ) if done and print_freq is not None and len ( episode_rewards ) % print_freq == 0 : logger . record_tabular ( \"steps\" , t ) logger . record_tabular ( \"episodes\" , num_episodes ) logger . record_tabular ( \"mean 100 episode reward\" , mean_100ep_reward ) logger . record_tabular ( \"% time spent exploring\" , int ( 100 * exploration . value ( t ) ) ) logger . dump_tabular ( ) if ( checkpoint_freq is not None and t > learning_starts and num_episodes > 100 and t % checkpoint_freq == 0 ) : if saved_mean_reward is None or mean_100ep_reward > saved_mean_reward : if print_freq is not None : logger . log ( \"Saving model due to mean reward increase: {} -> {}\" . format ( saved_mean_reward , mean_100ep_reward ) ) save_variables ( model_file ) model_saved = True saved_mean_reward = mean_100ep_reward if model_saved : if print_freq is not None : logger . log ( \"Restored model with mean reward: {}\" . format ( saved_mean_reward ) ) load_variables ( model_file ) return act',\n",
       "  'score': '87.74364',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_256808',\n",
       "  'title': None,\n",
       "  'text': \"def __init__ ( self , model : 'QStochasticPolicyModel' , rollout : Rollout ) : super ( ) . __init__ ( rollout ) self . model = model observations = self . get ( 'rollout:observations' ) logprobs , q = model ( observations ) self . provide ( 'model:logprobs' , logprobs ) self . provide ( 'model:q' , q )\",\n",
       "  'score': '87.56898',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_501977',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , bqm , embedding ) : # this is an awkward construction but we need it to maintain consistency with the other # chain break methods self . chain_to_var = { frozenset ( chain ) : v for v , chain in embedding . items ( ) } self . bqm = bqm',\n",
       "  'score': '87.22894',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_409404',\n",
       "  'title': None,\n",
       "  'text': 'def build_model ( self ) : with tf . variable_scope ( \"model\" , reuse = None , initializer = self . initializer ) : self . _create_placeholders ( ) self . _create_rnn_cells ( ) self . _create_initstate_and_embeddings ( ) self . _create_rnn_architecture ( ) self . _create_optimizer_node ( )',\n",
       "  'score': '86.797134',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_14325',\n",
       "  'title': None,\n",
       "  'text': 'def main ( ) : env = gym . make ( \"PongNoFrameskip-v4\" ) env = deepq . wrap_atari_dqn ( env ) model = deepq . learn ( env , \"conv_only\" , convs = [ ( 32 , 8 , 4 ) , ( 64 , 4 , 2 ) , ( 64 , 3 , 1 ) ] , hiddens = [ 256 ] , dueling = True , total_timesteps = 0 ) while True : obs , done = env . reset ( ) , False episode_rew = 0 while not done : env . render ( ) obs , rew , done , _ = env . step ( model ( obs [ None ] ) [ 0 ] ) episode_rew += rew print ( \"Episode reward\" , episode_rew )',\n",
       "  'score': '86.74553',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_42613',\n",
       "  'title': None,\n",
       "  'text': \"def __init__ ( self , env , dueling , noisy , fname ) : self . g = tf . Graph ( ) self . noisy = noisy self . dueling = dueling self . env = env with self . g . as_default ( ) : self . act = deepq . build_act_enjoy ( make_obs_ph = lambda name : U . Uint8Input ( env . observation_space . shape , name = name ) , q_func = dueling_model if dueling else model , num_actions = env . action_space . n , noisy = noisy ) self . saver = tf . train . Saver ( ) self . sess = tf . Session ( graph = self . g ) if fname is not None : print ( 'Loading Model...' ) self . saver . restore ( self . sess , fname )\",\n",
       "  'score': '86.63748',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_159852',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , qobj_model , * * run_config ) : self . _qobj_model = qobj_model self . _run_config = run_config',\n",
       "  'score': '86.593315',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_138023',\n",
       "  'title': None,\n",
       "  'text': 'def initialize_model ( self ) : return QNAFModel ( states = self . states , actions = self . actions , scope = self . scope , device = self . device , saver = self . saver , summarizer = self . summarizer , execution = self . execution , batching_capacity = self . batching_capacity , variable_noise = self . variable_noise , states_preprocessing = self . states_preprocessing , actions_exploration = self . actions_exploration , reward_preprocessing = self . reward_preprocessing , update_mode = self . update_mode , memory = self . memory , optimizer = self . optimizer , discount = self . discount , network = self . network , distributions = self . distributions , entropy_regularization = self . entropy_regularization , target_sync_frequency = self . target_sync_frequency , target_update_weight = self . target_update_weight , double_q_model = self . double_q_model , huber_loss = self . huber_loss )',\n",
       "  'score': '86.50542',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_13905',\n",
       "  'title': None,\n",
       "  'text': 'def wrap_atari_dqn ( env ) : from baselines . common . atari_wrappers import wrap_deepmind return wrap_deepmind ( env , frame_stack = True , scale = False )',\n",
       "  'score': '86.35025',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_839862',\n",
       "  'title': None,\n",
       "  'text': \"def dqn_atari ( self : Experiment , logdir = None , env = 'Pong' , frameskip = 4 , timesteps = 100000000 , memory_size = 100000 , agents = 2 , replay_start = 50000 , tter = .25 , n_heads = 3 ) : from tensorflow . contrib import layers import gym from gym import wrappers import numpy as np chi . set_loglevel ( 'info' ) log_top ( logdir + '/logs/top' ) log_nvidia_smi ( logdir + '/logs/nvidia-smi' ) memory = chi . rl . ReplayMemory ( memory_size , 32 ) actions = [ [ 0 , 1 ] , [ 0 , - 1 ] , [ 1 , 1 ] , [ 1 , - 1 ] , [ - 1 , 1 ] , [ - 1 , - 1 ] ] action_names = [ 'fw' , 'bw' , 'fw_r' , 'bw_r' , 'fw_l' , 'bw_l' ] class RenderMeta ( chi . rl . Wrapper ) : def __init__ ( self , env , limits = None , mod = False ) : super ( ) . __init__ ( env ) self . mod = mod self . an = env . action_space . n # self.q_plotters = [Plotter(limits=None, title=f'A({n})') for n in action_names] self . f , ax = plt . subplots ( 2 , 3 , figsize = ( 3 * 3 , 2 * 2 ) , dpi = 64 ) self . f . set_tight_layout ( True ) ax = iter ( np . reshape ( ax , - 1 ) ) self . q = Plotter ( next ( ax ) , title = 'A' , legend = action_names ) self . r = Plotter ( next ( ax ) , limits = None , title = 'reward' ) self . mq = Plotter ( next ( ax ) , title = 'mq' ) self . vq = Plotter ( next ( ax ) , title = 'vq' ) self . td = Plotter ( next ( ax ) , title = 'td' , auto_limit = 12 ) self . pe = Plotter ( next ( ax ) , title = 'pred.err.' , auto_limit = 12 ) def _step ( self , action ) : ob , r , done , info = super ( ) . _step ( action ) qs = self . meta . get ( 'action_values' , np . full ( self . an , np . nan ) ) qs -= np . mean ( qs ) # [qp.append(qs[i, ...]) for i, qp in enumerate(self.q_plotters)] self . q . append ( qs ) self . r . append ( r ) self . mq . append ( self . meta . get ( 'mq' , 0 ) ) self . vq . append ( self . meta . get ( 'vq' , 0 ) ) self . td . append ( self . meta . get ( 'td' , 0 ) ) self . pe . append ( self . meta . get ( 'smse' , 0 ) ) return ob , r , done , info def _render ( self , mode = 'human' , close = False ) : f = super ( ) . _render ( mode , close ) # fs = [qp.draw() for qp in self.q_plotters] f2 = draw ( self . f ) return chi . rl . util . concat_frames ( f , self . obs , f2 ) def _observation ( self , observation ) : self . obs = np . tile ( observation [ : , : , - 1 : ] , ( 1 , 1 , 3 ) ) return observation class NoiseWrapper ( chi . rl . Wrapper ) : def _reset ( self ) : self . mask = np . asarray ( np . random . normal ( 0 , 20 , size = self . observation_space . shape ) , dtype = np . uint8 ) return super ( ) . _reset ( ) def _observation ( self , observation ) : np . clip ( observation + self . mask , 0 , 255 , observation ) return observation env_name = env # no clue why this is necessary def make_env ( i ) : env = env_name + 'NoFrameskip-v3' env = gym . make ( env ) env = chi . rl . wrappers . AtariWrapper ( env ) if i == 1 : env = NoiseWrapper ( env ) env = chi . rl . wrappers . StackFrames ( env , 4 ) env = wrappers . SkipWrapper ( 4 ) ( env ) if i in ( 0 , 1 ) : env = RenderMeta ( env ) env = wrappers . Monitor ( env , self . logdir + '/monitor_' + str ( i ) , video_callable = lambda j : j % ( 20 if i in ( 0 , 1 ) else 200 ) == 0 ) return env envs = [ make_env ( i ) for i in range ( agents ) ] env = envs [ 0 ] print_env ( env ) @ chi . model ( tracker = tf . train . ExponentialMovingAverage ( 1 - .0005 ) , optimizer = tf . train . RMSPropOptimizer ( 6.25e-5 , .95 , .95 , .01 ) ) def pp ( x ) : x /= 255 x = layers . conv2d ( x , 32 , 8 , 4 ) x = layers . conv2d ( x , 64 , 4 , 2 ) x = layers . conv2d ( x , 64 , 3 , 1 ) x = layers . flatten ( x ) return x @ chi . model ( tracker = tf . train . ExponentialMovingAverage ( 1 - .0005 ) , optimizer = tf . train . RMSPropOptimizer ( 6.25e-5 , .95 , .95 , .01 ) ) def heads ( x ) : qs = [ ] for _ in range ( n_heads ) : xv = layers . fully_connected ( x , 512 ) val = layers . fully_connected ( xv , 1 , activation_fn = None ) # val = tf.squeeze(val, 1) xa = layers . fully_connected ( x , 512 ) adv = layers . fully_connected ( xa , env . action_space . n , activation_fn = None ) q = val + adv - tf . reduce_mean ( adv , axis = 1 , keep_dims = True ) q = tf . identity ( q , name = 'Q' ) qs . append ( q ) return qs dqn = BootstrappedDQN ( env . action_space . n , env . observation_space . shape , pp , heads , replay_start = replay_start , logdir = logdir ) for i , env in enumerate ( envs ) : agent = dqn . make_agent ( test = i in ( 0 , 1 ) , train = i != 1 , memory_size = memory_size // ( agents - 1 ) , logdir = logdir , name = f'Agent_{i}' ) agent . run ( env , async = True ) dqn . train ( timesteps , tter )\",\n",
       "  'score': '85.96297',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_14068',\n",
       "  'title': None,\n",
       "  'text': 'def main ( ) : logger . configure ( ) env = make_atari ( \\'PongNoFrameskip-v4\\' ) env = bench . Monitor ( env , logger . get_dir ( ) ) env = deepq . wrap_atari_dqn ( env ) model = deepq . learn ( env , \"conv_only\" , convs = [ ( 32 , 8 , 4 ) , ( 64 , 4 , 2 ) , ( 64 , 3 , 1 ) ] , hiddens = [ 256 ] , dueling = True , lr = 1e-4 , total_timesteps = int ( 1e7 ) , buffer_size = 10000 , exploration_fraction = 0.1 , exploration_final_eps = 0.01 , train_freq = 4 , learning_starts = 10000 , target_network_update_freq = 1000 , gamma = 0.99 , ) model . save ( \\'pong_model.pkl\\' ) env . close ( )',\n",
       "  'score': '85.81225',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_256807',\n",
       "  'title': None,\n",
       "  'text': 'def create ( backbone : ModelFactory , input_block : typing . Optional [ ModelFactory ] = None ) : if input_block is None : input_block = IdentityFactory ( ) return QStochasticPolicyModelFactory ( input_block = input_block , backbone = backbone )',\n",
       "  'score': '85.795715',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_23478',\n",
       "  'title': None,\n",
       "  'text': 'def wrap_deepmind ( env , dim = 84 , framestack = True ) : env = MonitorEnv ( env ) env = NoopResetEnv ( env , noop_max = 30 ) if \"NoFrameskip\" in env . spec . id : env = MaxAndSkipEnv ( env , skip = 4 ) env = EpisodicLifeEnv ( env ) if \"FIRE\" in env . unwrapped . get_action_meanings ( ) : env = FireResetEnv ( env ) env = WarpFrame ( env , dim ) # env = ScaledFloatFrame(env)  # TODO: use for dqn? # env = ClipRewardEnv(env)  # reward clipping is handled by policy eval if framestack : env = FrameStack ( env , 4 ) return env',\n",
       "  'score': '85.755066',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_978174',\n",
       "  'title': None,\n",
       "  'text': \"def q_z_xay ( self ) : with tf . variable_scope ( 'q_z_xay' ) : x_to_qz_l = Layers ( self . x_l ) x_to_qz_l . fc ( 500 , activation_fn = None ) y_to_qz_l = Layers ( self . t_l ) y_to_qz_l . fc ( 500 , activation_fn = None ) qa_to_qz_l = Layers ( self . samples [ 'qa_l' ] ) qa_to_qz_l . fc ( 500 , activation_fn = None ) q_z_xay_l = Layers ( x_to_qz_l . get_output ( ) + y_to_qz_l . get_output ( ) + qa_to_qz_l . get_output ( ) ) q_z_xay_l . fc ( 500 ) . fc ( 500 ) qz_l = GaussianLayer ( q_z_xay_l . get_output ( ) , num_latent = 100 , eq_samples = 10 ) self . samples [ 'qz_l' ] = qz_l . compute_samples ( ) with tf . variable_scope ( 'q_z_xay' , reuse = True ) : x_to_qz_u = Layers ( self . x_u_rep ) x_to_qz_u . fc ( 500 , activation_fn = None ) y_to_qz_u = Layers ( self . t_u_rep ) y_to_qz_u . fc ( 500 , activation_fn = None ) qa_to_qz_u = Layers ( self . samples [ 'qa_u' ] ) qa_to_qz_u . fc ( 500 , activation_fn = None ) q_z_xay_u = Layers ( x_to_qz_u . get_output ( ) + y_to_qz_u . get_output ( ) + qa_to_qz_u . get_output ( ) ) q_z_xay_u . fc ( 500 ) . fc ( 500 ) qz_u = GaussianLayer ( q_z_xay_u . get_output ( ) , num_latent = 100 , eq_samples = 10 ) self . samples [ 'qz_u' ] = qz_u . compute_samples ( ) return qz_l , qz_u\",\n",
       "  'score': '85.59354',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_14207',\n",
       "  'title': None,\n",
       "  'text': \"def train ( env_id , num_timesteps , seed ) : from baselines . ppo1 import pposgd_simple , cnn_policy import baselines . common . tf_util as U rank = MPI . COMM_WORLD . Get_rank ( ) sess = U . single_threaded_session ( ) sess . __enter__ ( ) if rank == 0 : logger . configure ( ) else : logger . configure ( format_strs = [ ] ) workerseed = seed + 10000 * MPI . COMM_WORLD . Get_rank ( ) if seed is not None else None set_global_seeds ( workerseed ) env = make_atari ( env_id ) def policy_fn ( name , ob_space , ac_space ) : #pylint: disable=W0613 return cnn_policy . CnnPolicy ( name = name , ob_space = ob_space , ac_space = ac_space ) env = bench . Monitor ( env , logger . get_dir ( ) and osp . join ( logger . get_dir ( ) , str ( rank ) ) ) env . seed ( workerseed ) env = wrap_deepmind ( env ) env . seed ( workerseed ) pposgd_simple . learn ( env , policy_fn , max_timesteps = int ( num_timesteps * 1.1 ) , timesteps_per_actorbatch = 256 , clip_param = 0.2 , entcoeff = 0.01 , optim_epochs = 4 , optim_stepsize = 1e-3 , optim_batchsize = 64 , gamma = 0.99 , lam = 0.95 , schedule = 'linear' ) env . close ( )\",\n",
       "  'score': '85.5408',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_14210',\n",
       "  'title': None,\n",
       "  'text': \"def wrap_deepmind ( env , episode_life = True , clip_rewards = True , frame_stack = False , scale = False ) : if episode_life : env = EpisodicLifeEnv ( env ) if 'FIRE' in env . unwrapped . get_action_meanings ( ) : env = FireResetEnv ( env ) env = WarpFrame ( env ) if scale : env = ScaledFloatFrame ( env ) if clip_rewards : env = ClipRewardEnv ( env ) if frame_stack : env = FrameStack ( env , 4 ) return env\",\n",
       "  'score': '85.40357',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_143158',\n",
       "  'title': None,\n",
       "  'text': 'def run_bell_low_level ( n_shots = 1000 ) : # Step 1. Get some device components qc = get_qc ( \\'9q-generic-qvm\\' ) compiler = qc . compiler qam = qc . qam del qc q = [ 4 , 5 ] # qubits # Step 2. Construct your program program = Program ( ) program += H ( q [ 0 ] ) program += CNOT ( q [ 0 ] , q [ 1 ] ) # Step 2.1. Manage read-out memory ro = program . declare ( \\'ro\\' , memory_type = \\'BIT\\' , memory_size = 2 ) program += MEASURE ( q [ 0 ] , ro [ 0 ] ) program += MEASURE ( q [ 1 ] , ro [ 1 ] ) # Step 2.2. Run the program in a loop program = program . wrap_in_numshots_loop ( n_shots ) # Step 3. Compile and run nq_program = compiler . quil_to_native_quil ( program ) executable = compiler . native_quil_to_executable ( nq_program ) bitstrings = qam . load ( executable ) . run ( ) . wait ( ) . read_memory ( region_name = \"ro\" ) # Bincount bitstrings basis = np . array ( [ 2 ** i for i in range ( len ( q ) ) ] ) ints = np . sum ( bitstrings * basis , axis = 1 ) print ( \\'bincounts\\' , np . bincount ( ints ) ) # Check parity parities = np . sum ( bitstrings , axis = 1 ) % 2 print ( \\'avg parity\\' , np . mean ( parities ) )',\n",
       "  'score': '85.38941',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_6530',\n",
       "  'title': None,\n",
       "  'text': 'def pretrained ( cls , f , data , ps = None , xtra_fc = None , xtra_cut = 0 , custom_head = None , precompute = False , pretrained = True , * * kwargs ) : models = ConvnetBuilder ( f , data . c , data . is_multi , data . is_reg , ps = ps , xtra_fc = xtra_fc , xtra_cut = xtra_cut , custom_head = custom_head , pretrained = pretrained ) return cls ( data , models , precompute , * * kwargs )',\n",
       "  'score': '85.262375',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_33709',\n",
       "  'title': None,\n",
       "  'text': \"def squeezenet1_1 ( pretrained = False , * * kwargs ) : model = SqueezeNet ( version = 1.1 , * * kwargs ) if pretrained : model . load_state_dict ( model_zoo . load_url ( model_urls [ 'squeezenet1_1' ] ) ) return model\",\n",
       "  'score': '85.160065',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_839621',\n",
       "  'title': None,\n",
       "  'text': \"def delling_network ( ) : @ tt . model ( tracker = tf . train . ExponentialMovingAverage ( 1 - .0005 ) , # TODO: replace with original weight freeze optimizer = tf . train . RMSPropOptimizer ( 6.25e-5 , .95 , .95 , .01 ) ) def q_network ( x ) : x /= 255 x = layers . conv2d ( x , 32 , 8 , 4 ) x = layers . conv2d ( x , 64 , 4 , 2 ) x = layers . conv2d ( x , 64 , 3 , 1 ) x = layers . flatten ( x ) xv = layers . fully_connected ( x , 512 ) val = layers . fully_connected ( xv , 1 , activation_fn = None ) # val = tf.squeeze(val, 1) xa = layers . fully_connected ( x , 512 ) adv = layers . fully_connected ( xa , env . action_space . n , activation_fn = None ) q = val + adv - tf . reduce_mean ( adv , axis = 1 , keep_dims = True ) q = tf . identity ( q , name = 'Q' ) return q\",\n",
       "  'score': '85.140396',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_839623',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , n_actions , observation_shape , q_network : tt . Model , double_dqn = True , replay_start = 50000 , clip_td = False , logdir = \"\" , clip_gradients = 10 ) : self . logdir = logdir self . replay_start = replay_start self . n_actions = n_actions self . observation_shape = observation_shape self . memory = ShardedMemory ( ) self . discount = .99 self . step = 0 def act ( x : [ observation_shape ] ) : qs = q_network ( x ) a = tf . argmax ( qs , axis = 1 ) # qm = tf.reduce_max(qs, axis=1) return a , qs self . act = Function ( act ) def train_step ( o : [ observation_shape ] , a : ( tf . int32 , [ [ ] ] ) , r , t : tf . bool , o2 : [ observation_shape ] ) : q = q_network ( o ) # ac = tf.argmax(q, axis=1) # compute targets q2 = q_network . tracked ( o2 ) if double_dqn : a2 = tf . argmax ( q_network ( o2 ) , axis = 1 ) # yep, that\\'s really the only difference else : a2 = tf . argmax ( q2 , axis = 1 ) mask2 = tf . one_hot ( a2 , n_actions , 1.0 , 0.0 , axis = 1 ) q_target = tf . where ( t , r , r + self . discount * tf . reduce_sum ( q2 * mask2 , axis = 1 ) ) q_target = tf . stop_gradient ( q_target ) # compute loss mask = tf . one_hot ( a , n_actions , 1.0 , 0.0 , axis = 1 ) qs = tf . reduce_sum ( q * mask , axis = 1 , name = \\'q_max\\' ) td = tf . subtract ( q_target , qs , name = \\'td\\' ) if clip_td : td = tf . clip_by_value ( td , - .5 , .5 , name = \\'clipped_td\\' ) # loss = tf.reduce_mean(tf.abs(td), axis=0, name=\\'mae\\') # loss = tf.where(tf.abs(td) < 1.0, 0.5 * tf.square(td), tf.abs(td) - 0.5, name=\\'mse_huber\\') loss = tf . reduce_mean ( tf . square ( td ) , axis = 0 , name = \\'mse\\' ) gav = q_network . compute_gradients ( loss ) if clip_gradients : gav = [ ( tf . clip_by_norm ( g , clip_gradients ) , v ) for g , v in gav ] loss_update = q_network . apply_gradients ( gav ) # logging layers . summarize_tensors ( [ td , loss , r , o , a , tf . subtract ( o2 , o , name = \\'state_dif\\' ) , tf . reduce_mean ( tf . cast ( t , tf . float32 ) , name = \\'frac_terminal\\' ) , tf . subtract ( tf . reduce_max ( q , 1 , True ) , q , name = \\'av_advantage\\' ) ] ) # layers.summarize_tensors(chi.activations()) # layers.summarize_tensors(chi.gradients()) return loss_update self . train_step = Function ( train_step , prefetch_fctn = lambda : self . memory . sample_batch ( ) [ : - 1 ] , prefetch_capacity = 10 , prefetch_threads = 3 ) def log_weigths ( ) : v = q_network . trainable_variables ( ) # print(f\\'log weights {v}\\') f = q_network . tracker_variables # print(f\\'log weights EMA {f}\\') difs = [ ] for g in v : a = q_network . tracker . average ( g ) difs . append ( tf . subtract ( g , a , name = f\\'ema/dif{g.name[:-2]}\\' ) ) layers . summarize_tensors ( v + f + difs ) self . log_weights = Function ( log_weigths , async = True )',\n",
       "  'score': '85.10956',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_14904',\n",
       "  'title': None,\n",
       "  'text': 'def dqn_atari_base ( ) : # These params are based on agents/dqn/configs/dqn.gin # with some modifications taking into account our code return hparam . HParams ( agent_gamma = 0.99 , agent_update_horizon = 1 , agent_min_replay_history = 20000 , # agent steps agent_update_period = 4 , agent_target_update_period = 8000 , # agent steps agent_epsilon_train = 0.01 , agent_epsilon_eval = 0.001 , agent_epsilon_decay_period = 250000 , # agent steps agent_generates_trainable_dones = True , optimizer_class = \"RMSProp\" , optimizer_learning_rate = 0.00025 , optimizer_decay = 0.95 , optimizer_momentum = 0.0 , optimizer_epsilon = 0.00001 , optimizer_centered = True , # TODO(kozak): change names maybe replay_buffer -> agent? # Also batch_size is now buffer_batch_size in _DQNAgent. replay_buffer_replay_capacity = 1000000 , replay_buffer_buffer_batch_size = 32 , time_limit = 27000 , save_every_steps = 50000 , num_frames = int ( 20 * 1e6 ) , # TODO(konradczechowski) this is not used in trainer_model_free, clean # this up after evaluation refactor eval_episodes_num = 3 , )',\n",
       "  'score': '85.09629',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_33600',\n",
       "  'title': None,\n",
       "  'text': \"def alexnet ( pretrained = False , * * kwargs ) : model = AlexNet ( * * kwargs ) if pretrained : model . load_state_dict ( model_zoo . load_url ( model_urls [ 'alexnet' ] ) ) return model\",\n",
       "  'score': '85.09578',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_256794',\n",
       "  'title': None,\n",
       "  'text': \"def qbert_ppo ( ) : device = torch . device ( 'cuda:0' ) seed = 1001 # Set random seed in python std lib, numpy and pytorch set_seed ( seed ) # Create 16 environments evaluated in parallel in sub processess with all usual DeepMind wrappers # These are just helper functions for that vec_env = SubprocVecEnvWrapper ( ClassicAtariEnv ( 'QbertNoFrameskip-v4' ) , frame_history = 4 ) . instantiate ( parallel_envs = 8 , seed = seed ) # Again, use a helper to create a model # But because model is owned by the reinforcer, model should not be accessed using this variable # but from reinforcer.model property model = StochasticPolicyModelFactory ( input_block = ImageToTensorFactory ( ) , backbone = NatureCnnFactory ( input_width = 84 , input_height = 84 , input_channels = 4 ) ) . instantiate ( action_space = vec_env . action_space ) # Set schedule for gradient clipping. cliprange = LinearSchedule ( initial_value = 0.1 , final_value = 0.0 ) # Reinforcer - an object managing the learning process reinforcer = OnPolicyIterationReinforcer ( device = device , settings = OnPolicyIterationReinforcerSettings ( batch_size = 256 , experience_replay = 4 , number_of_steps = 128 ) , model = model , algo = PpoPolicyGradient ( entropy_coefficient = 0.01 , value_coefficient = 0.5 , max_grad_norm = 0.5 , discount_factor = 0.99 , gae_lambda = 0.95 , cliprange = cliprange ) , env_roller = StepEnvRoller ( environment = vec_env , device = device , ) ) # Model optimizer optimizer = optim . Adam ( reinforcer . model . parameters ( ) , lr = 2.5e-4 , eps = 1.0e-5 ) # Overall information store for training information training_info = TrainingInfo ( metrics = [ EpisodeRewardMetric ( 'episode_rewards' ) , # Calculate average reward from episode ] , callbacks = [ StdoutStreaming ( ) , # Print live metrics every epoch to standard output FrameTracker ( 1.1e7 ) # We need frame tracker to track the progress of learning ] ) # A bit of training initialization bookkeeping... training_info . initialize ( ) reinforcer . initialize_training ( training_info ) training_info . on_train_begin ( ) # Let's make 10 batches per epoch to average metrics nicely # Rollout size is 8 environments times 128 steps num_epochs = int ( 1.1e7 / ( 128 * 8 ) / 10 ) # Normal handrolled training loop for i in range ( 1 , num_epochs + 1 ) : epoch_info = EpochInfo ( training_info = training_info , global_epoch_idx = i , batches_per_epoch = 10 , optimizer = optimizer ) reinforcer . train_epoch ( epoch_info ) training_info . on_train_end ( )\",\n",
       "  'score': '85.00336',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_491217',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self ) : self . measurers = [ accuracy , BCR ] self . model_options = { } self . verbose = True self . REPAIR_STEPS = 10 self . RETRAIN_MODEL_PER_REPAIR = False self . WRITE_ORIGINAL_PREDICTIONS = True self . ModelFactory = Weka_SVM self . kdd = False',\n",
       "  'score': '84.97092',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_502006',\n",
       "  'title': None,\n",
       "  'text': 'def embed_qubo ( source_Q , embedding , target_adjacency , chain_strength = 1.0 ) : source_bqm = dimod . BinaryQuadraticModel . from_qubo ( source_Q ) target_bqm = embed_bqm ( source_bqm , embedding , target_adjacency , chain_strength = chain_strength ) target_Q , __ = target_bqm . to_qubo ( ) return target_Q',\n",
       "  'score': '84.90059',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_256761',\n",
       "  'title': None,\n",
       "  'text': \"def model_action_q ( self ) : q = self . get ( 'model:q' ) actions = self . get ( 'rollout:actions' ) return q . gather ( 1 , actions . unsqueeze ( 1 ) ) . squeeze ( 1 )\",\n",
       "  'score': '84.88558',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_894594',\n",
       "  'title': None,\n",
       "  'text': 'def model ( self , p ) : model = nengo . Network ( ) with model : stim = nengo . Node ( [ 0 ] * p . D ) ens = nengo . Ensemble ( n_neurons = p . N , dimensions = p . D ) nengo . Connection ( stim , ens ) self . probe = nengo . Probe ( ens , synapse = 0.01 ) return model',\n",
       "  'score': '84.65433',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_256815',\n",
       "  'title': None,\n",
       "  'text': \"def instantiate ( self , * * extra_args ) : input_block = self . input_block . instantiate ( ) backbone = self . backbone . instantiate ( * * extra_args ) return QStochasticPolicyModel ( input_block , backbone , extra_args [ 'action_space' ] )\",\n",
       "  'score': '84.57206',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_531719',\n",
       "  'title': None,\n",
       "  'text': \"def build_model ( ) : # This resets all parameters and variables, leave this here tf . reset_default_graph ( ) #### Your code #### net = tflearn . input_data ( [ None , vector_len ] ) # Input net = tflearn . fully_connected ( net , 200 , activation = 'ReLU' ) # Hidden net = tflearn . fully_connected ( net , 25 , activation = 'ReLU' ) # Hidden net = tflearn . fully_connected ( net , 2 , activation = 'softmax' ) # Output net = tflearn . regression ( net , optimizer = 'sgd' , learning_rate = 0.1 , loss = 'categorical_crossentropy' ) model = tflearn . DNN ( net ) return model\",\n",
       "  'score': '84.38306',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_349944',\n",
       "  'title': None,\n",
       "  'text': \"def create_atari_environment ( name ) : env = gym . make ( name ) env = NoopResetEnv ( env , noop_max = 30 ) env = EpisodicLifeEnv ( env ) env = ClipRewardEnv ( env ) action_names = env . unwrapped . get_action_meanings ( ) if 'FIRE' in action_names : env = FireResetEnv ( env ) env = WarpFrame ( env ) env = FrameStack ( env , k = 4 ) return env\",\n",
       "  'score': '84.34021',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_13968',\n",
       "  'title': None,\n",
       "  'text': 'def learn ( network , env , seed = None , nsteps = 20 , total_timesteps = int ( 80e6 ) , q_coef = 0.5 , ent_coef = 0.01 , max_grad_norm = 10 , lr = 7e-4 , lrschedule = \\'linear\\' , rprop_epsilon = 1e-5 , rprop_alpha = 0.99 , gamma = 0.99 , log_interval = 100 , buffer_size = 50000 , replay_ratio = 4 , replay_start = 10000 , c = 10.0 , trust_region = True , alpha = 0.99 , delta = 1 , load_path = None , * * network_kwargs ) : print ( \"Running Acer Simple\" ) print ( locals ( ) ) set_global_seeds ( seed ) if not isinstance ( env , VecFrameStack ) : env = VecFrameStack ( env , 1 ) policy = build_policy ( env , network , estimate_q = True , * * network_kwargs ) nenvs = env . num_envs ob_space = env . observation_space ac_space = env . action_space nstack = env . nstack model = Model ( policy = policy , ob_space = ob_space , ac_space = ac_space , nenvs = nenvs , nsteps = nsteps , ent_coef = ent_coef , q_coef = q_coef , gamma = gamma , max_grad_norm = max_grad_norm , lr = lr , rprop_alpha = rprop_alpha , rprop_epsilon = rprop_epsilon , total_timesteps = total_timesteps , lrschedule = lrschedule , c = c , trust_region = trust_region , alpha = alpha , delta = delta ) runner = Runner ( env = env , model = model , nsteps = nsteps ) if replay_ratio > 0 : buffer = Buffer ( env = env , nsteps = nsteps , size = buffer_size ) else : buffer = None nbatch = nenvs * nsteps acer = Acer ( runner , model , buffer , log_interval ) acer . tstart = time . time ( ) for acer . steps in range ( 0 , total_timesteps , nbatch ) : #nbatch samples, 1 on_policy call and multiple off-policy calls acer . call ( on_policy = True ) if replay_ratio > 0 and buffer . has_atleast ( replay_start ) : n = np . random . poisson ( replay_ratio ) for _ in range ( n ) : acer . call ( on_policy = False ) # no simulation steps in this return model',\n",
       "  'score': '84.307495',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_314461',\n",
       "  'title': None,\n",
       "  'text': \"def __init__ ( self , model , nwalkers , checkpoint_interval = None , checkpoint_signal = None , logpost_function = None , nprocesses = 1 , use_mpi = False ) : self . model = model # create a wrapper for calling the model if logpost_function is None : logpost_function = 'logposterior' model_call = models . CallModel ( model , logpost_function ) # Set up the pool if nprocesses > 1 : # these are used to help paralleize over multiple cores / MPI models . _global_instance = model_call model_call = models . _call_global_model pool = choose_pool ( mpi = use_mpi , processes = nprocesses ) if pool is not None : pool . count = nprocesses # set up emcee self . _nwalkers = nwalkers ndim = len ( model . variable_params ) self . _sampler = emcee . EnsembleSampler ( nwalkers , ndim , model_call , pool = pool ) # emcee uses it's own internal random number generator; we'll set it # to have the same state as the numpy generator rstate = numpy . random . get_state ( ) self . _sampler . random_state = rstate self . _checkpoint_interval = checkpoint_interval self . _checkpoint_signal = checkpoint_signal\",\n",
       "  'score': '84.30148',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_143144',\n",
       "  'title': None,\n",
       "  'text': 'def run_and_measure ( self , quil_program , qubits , trials = 1 ) : # Developer note: This code is for backwards compatibility. It can\\'t be replaced with # ForestConnection._run_and_measure because we\\'ve turned off the ability to set # `needs_compilation` (that usually indicates the user is doing something iffy like # using a noise model with this function) payload = self . _run_and_measure_payload ( quil_program , qubits , trials ) response = post_json ( self . session , self . sync_endpoint + \"/qvm\" , payload ) return response . json ( )',\n",
       "  'score': '84.221214',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_349714',\n",
       "  'title': None,\n",
       "  'text': 'def main ( opts ) : try : os . mkdir ( opts . results_path ) except OSError : pass random . seed ( opts . seed ) np . random . seed ( opts . seed ) torch . manual_seed ( opts . seed ) # Create environment env = create_atari_environment ( opts . env ) env . seed ( opts . seed ) # Look for trained model path = os . path . join ( opts . results_path , \"{0}.pt\" . format ( env . spec . id ) ) if os . path . exists ( path ) : learn = False model = torch . load ( path , map_location = device ) else : learn = True if opts . sparse : model = SparseQ ( actions = env . action_space . n ) . to ( device ) else : model = DenseQ ( actions = env . action_space . n ) . to ( device ) run ( env = env , model = model , learn = learn , numSteps = opts . timesteps , render = opts . render , resultsPath = opts . results_path )',\n",
       "  'score': '84.22093',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_50544',\n",
       "  'title': None,\n",
       "  'text': 'def help_preamble_for ( algo ) : if algo == \"aggregator\" : return \"\"\"\\n            Build an Aggregated Frame\\n\\n            Builds an Aggregated Frame of an H2OFrame.\\n        \"\"\" if algo == \"deeplearning\" : return \"\"\"\\n            Build a Deep Neural Network model using CPUs\\n\\n            Builds a feed-forward multilayer artificial neural network on an H2OFrame.\\n        \"\"\" if algo == \"stackedensemble\" : return \"\"\"\\n            Builds a Stacked Ensemble\\n\\n            Build a stacked ensemble (aka. Super Learner) using the H2O base\\n            learning algorithms specified by the user.\\n        \"\"\" if algo == \"deepwater\" : return \"\"\"\\n            Build a Deep Learning model using multiple native GPU backends\\n\\n            Builds a deep neural network on an H2OFrame containing various data sources.\\n        \"\"\" if algo == \"xgboost\" : return \"\"\"\\n            Build an eXtreme Gradient Boosting model\\n\\n            Builds a eXtreme Gradient Boosting model using the native XGBoost backend.\\n        \"\"\" if algo == \"drf\" : return \"\"\"\\n            Build a Random Forest model\\n\\n            Builds a Random Forest model on an H2OFrame.\\n        \"\"\" if algo == \"gbm\" : return \"\"\"\\n            Build gradient boosted classification or regression trees\\n\\n            Builds gradient boosted classification trees and gradient boosted regression trees on a parsed data set.\\n            The default distribution function will guess the model type based on the response column type.\\n            In order to run properly, the response column must be an numeric for \"gaussian\" or an\\n            enum for \"bernoulli\" or \"multinomial\".\\n        \"\"\" if algo == \"glm\" : return \"\"\"\\n            Fit a generalized linear model\\n\\n            Fits a generalized linear model, specified by a response variable, a set of predictors, and a\\n            description of the error distribution.\\n        \"\"\" if algo == \"glrm\" : return \"\"\"\\n            Generalized low rank decomposition of an H2O data frame\\n\\n            Builds a generalized low rank decomposition of an H2O data frame\\n        \"\"\" if algo == \"kmeans\" : return \"\"\"\\n             Performs k-means clustering on an H2O dataset\\n        \"\"\" if algo == \"naivebayes\" : return \"\"\"\\n            Compute naive Bayes probabilities on an H2O dataset.\\n\\n            The naive Bayes classifier assumes independence between predictor variables conditional\\n            on the response, and a Gaussian distribution of numeric predictors with mean and standard\\n            deviation computed from the training dataset. When building a naive Bayes classifier,\\n            every row in the training dataset that contains at least one NA will be skipped completely.\\n            If the test dataset has missing values, then those predictors are omitted in the probability\\n            calculation during prediction.\\n        \"\"\" if algo == \"pca\" : return \"\"\"\\n            Principal component analysis of an H2O data frame\\n\\n            Principal components analysis of an H2O data frame using the power method\\n            to calculate the singular value decomposition of the Gram matrix.\\n        \"\"\" if algo == \"svd\" : return \"\"\"\\n            Singular value decomposition of an H2O data frame using the power method\\n       \"\"\" if algo == \"word2vec\" : return \"\"\"\\n            Trains a word2vec model on a String column of an H2O data frame\\n        \"\"\" if algo == \"coxph\" : return \"\"\"\\n            Trains a Cox Proportional Hazards Model (CoxPH) on an H2O dataset\\n        \"\"\" if algo == \"isolationforest\" : return \"\"\"\\n            Trains an Isolation Forest model\\n        \"\"\" if algo == \"generic\" : return \"\"\"\\n            Imports a generic model into H2O. Such model can be used then used for scoring and obtaining\\n            additional information about the model. The imported model has to be supported by H2O.\\n        \"\"\"',\n",
       "  'score': '84.173676',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_23793',\n",
       "  'title': None,\n",
       "  'text': 'def forward ( self , agent_qs , states ) : bs = agent_qs . size ( 0 ) states = states . reshape ( - 1 , self . state_dim ) agent_qs = agent_qs . view ( - 1 , 1 , self . n_agents ) # First layer w1 = th . abs ( self . hyper_w_1 ( states ) ) b1 = self . hyper_b_1 ( states ) w1 = w1 . view ( - 1 , self . n_agents , self . embed_dim ) b1 = b1 . view ( - 1 , 1 , self . embed_dim ) hidden = F . elu ( th . bmm ( agent_qs , w1 ) + b1 ) # Second layer w_final = th . abs ( self . hyper_w_final ( states ) ) w_final = w_final . view ( - 1 , self . embed_dim , 1 ) # State-dependent bias v = self . V ( states ) . view ( - 1 , 1 , 1 ) # Compute final output y = th . bmm ( hidden , w_final ) + v # Reshape and return q_tot = y . view ( bs , - 1 , 1 ) return q_tot',\n",
       "  'score': '84.14322',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_839415',\n",
       "  'title': None,\n",
       "  'text': \"def test_dqn ( ) : with tf . Graph ( ) . as_default ( ) , tf . Session ( ) . as_default ( ) : dqn_test ( ) # optimal expected return = 1 with tf . Graph ( ) . as_default ( ) , tf . Session ( ) . as_default ( ) : dqn_test ( 'OneRoundNondeterministicReward-v0' ) # optimal expected return = 2.5 with tf . Graph ( ) . as_default ( ) , tf . Session ( ) . as_default ( ) : dqn_test ( 'TwoRoundDeterministicReward-v0' )\",\n",
       "  'score': '84.14036',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_1086039',\n",
       "  'title': None,\n",
       "  'text': \"def build_training_model ( self ) : self . encoder_input = self . encoder . encoder_input self . encoder_embed = self . encoder . encoder_embed self . decoder_input = self . decoder . decoder_input self . decoder_embed = self . decoder . decoder_embed self . decoder_output = self . decoder . decoder_output if self . config [ 'hierarchical' ] : # do something pass else : self . training_model = Model ( [ self . encoder_input , self . decoder_input ] , self . decoder_output )\",\n",
       "  'score': '83.991135',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_256859',\n",
       "  'title': None,\n",
       "  'text': 'def run ( self ) : device = self . model_config . torch_device ( ) env = self . env_factory . instantiate ( parallel_envs = self . parallel_envs , preset = \\'record\\' , seed = self . model_config . seed ) model = self . model_factory . instantiate ( action_space = env . action_space ) . to ( device ) if self . action_noise_factory is not None : action_noise = self . action_noise_factory . instantiate ( environment = env ) . to ( device ) else : action_noise = None training_info = TrainingInfo ( start_epoch_idx = self . storage . last_epoch_idx ( ) , run_name = self . model_config . run_name ) model_state , hidden_state = self . storage . load ( training_info ) model . load_state_dict ( model_state ) print ( \"Loading model trained for {} epochs\" . format ( training_info . start_epoch_idx ) ) model . eval ( ) episode_rewards = [ ] episode_lengths = [ ] observations = env . reset ( ) observations_tensor = torch . from_numpy ( observations ) . to ( device ) if model . is_recurrent : hidden_state = model . zero_state ( observations . shape [ 0 ] ) . to ( device ) with tqdm . tqdm ( total = self . takes ) as progress_bar : while len ( episode_rewards ) < self . takes : if model . is_recurrent : output = model . step ( observations_tensor , hidden_state , * * self . sample_args ) hidden_state = output [ \\'state\\' ] actions = output [ \\'actions\\' ] else : actions = model . step ( observations_tensor , * * self . sample_args ) [ \\'actions\\' ] if action_noise is not None : actions = action_noise ( actions ) observations , rewards , dones , infos = env . step ( actions . cpu ( ) . numpy ( ) ) observations_tensor = torch . from_numpy ( observations ) . to ( device ) for info in infos : if \\'episode\\' in info : episode_rewards . append ( info [ \\'episode\\' ] [ \\'r\\' ] ) episode_lengths . append ( info [ \\'episode\\' ] [ \\'l\\' ] ) progress_bar . update ( 1 ) if model . is_recurrent : # Zero state belongiong to finished episodes dones_tensor = torch . from_numpy ( dones . astype ( np . float32 ) ) . to ( device ) hidden_state = hidden_state * ( 1.0 - dones_tensor . unsqueeze ( - 1 ) ) print ( pd . DataFrame ( { \\'lengths\\' : episode_lengths , \\'rewards\\' : episode_rewards } ) . describe ( ) )',\n",
       "  'score': '83.88347',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_5942',\n",
       "  'title': None,\n",
       "  'text': 'def create_cnn_model ( base_arch : Callable , nc : int , cut : Union [ int , Callable ] = None , pretrained : bool = True , lin_ftrs : Optional [ Collection [ int ] ] = None , ps : Floats = 0.5 , custom_head : Optional [ nn . Module ] = None , split_on : Optional [ SplitFuncOrIdxList ] = None , bn_final : bool = False , concat_pool : bool = True ) : body = create_body ( base_arch , pretrained , cut ) if custom_head is None : nf = num_features_model ( nn . Sequential ( * body . children ( ) ) ) * ( 2 if concat_pool else 1 ) head = create_head ( nf , nc , lin_ftrs , ps = ps , concat_pool = concat_pool , bn_final = bn_final ) else : head = custom_head return nn . Sequential ( body , head )',\n",
       "  'score': '83.86006',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_342541',\n",
       "  'title': None,\n",
       "  'text': \"def insert_deep_learning_model ( pipeline_step , file_name ) : # This is where we saved the random_name for this model random_name = pipeline_step . model # Load the Keras model here keras_file_name = file_name [ : - 5 ] + random_name + '_keras_deep_learning_model.h5' model = keras_load_model ( keras_file_name ) # Put the model back in place so that we can still use it to get predictions without having to load it back in from disk return model\",\n",
       "  'score': '83.810425',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_42797',\n",
       "  'title': None,\n",
       "  'text': \"def make_model ( model_type = 'madry' , * * kwargs ) : if model_type == 'basic' : return make_basic_ngpu ( * * kwargs ) elif model_type == 'madry' : return make_madry_ngpu ( * * kwargs ) elif model_type == 'resnet_tf' : return ResNetTF ( * * kwargs ) else : raise Exception ( 'model type not defined.' )\",\n",
       "  'score': '83.69082',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_14983',\n",
       "  'title': None,\n",
       "  'text': 'def get_create_agent ( agent_kwargs ) : def create_agent ( sess , environment , summary_writer = None ) : \"\"\"Creates a DQN agent.\\n\\n    Simplified version of `dopamine.discrete_domains.train.create_agent`\\n\\n    Args:\\n      sess: a session\\n      environment: an environment\\n      summary_writer: a summary writer.\\n\\n    Returns:\\n      a DQN agent.\\n    \"\"\" return BatchDQNAgent ( env_batch_size = environment . batch_size , sess = sess , num_actions = environment . action_space . n , summary_writer = summary_writer , tf_device = \"/gpu:*\" , * * agent_kwargs ) return create_agent',\n",
       "  'score': '83.68073',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_257194',\n",
       "  'title': None,\n",
       "  'text': 'def create ( backbone : ModelFactory , input_block : typing . Optional [ ModelFactory ] = None ) : if input_block is None : input_block = IdentityFactory ( ) return QDuelingModelFactory ( input_block = input_block , backbone = backbone )',\n",
       "  'score': '83.613205',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_39711',\n",
       "  'title': None,\n",
       "  'text': 'def build_model ( input_size , hidden_size , output_size ) : model = Sequential ( ) model . add ( Recurrent ( ) . add ( RnnCell ( input_size , hidden_size , Tanh ( ) ) ) ) . add ( TimeDistributed ( Linear ( hidden_size , output_size ) ) ) model . reset ( ) return model',\n",
       "  'score': '83.50784',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_40290',\n",
       "  'title': None,\n",
       "  'text': 'def build_model ( class_num ) : model = Sequential ( ) if model_type . lower ( ) == \"cnn\" : model . add ( TemporalConvolution ( embedding_dim , 256 , 5 ) ) . add ( ReLU ( ) ) . add ( TemporalMaxPooling ( sequence_len - 5 + 1 ) ) . add ( Squeeze ( 2 ) ) elif model_type . lower ( ) == \"lstm\" : model . add ( Recurrent ( ) . add ( LSTM ( embedding_dim , 256 , p ) ) ) model . add ( Select ( 2 , - 1 ) ) elif model_type . lower ( ) == \"gru\" : model . add ( Recurrent ( ) . add ( GRU ( embedding_dim , 256 , p ) ) ) model . add ( Select ( 2 , - 1 ) ) model . add ( Linear ( 256 , 128 ) ) . add ( Dropout ( 0.2 ) ) . add ( ReLU ( ) ) . add ( Linear ( 128 , class_num ) ) . add ( LogSoftMax ( ) ) return model',\n",
       "  'score': '83.48299',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_895092',\n",
       "  'title': None,\n",
       "  'text': 'def make_model ( self , * * kwargs ) : p = self . _create_parameters ( * * kwargs ) return self . model ( p )',\n",
       "  'score': '83.45454',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_109088',\n",
       "  'title': None,\n",
       "  'text': 'def runModel ( gymName , plot = False ) : print \"Creating model from %s...\" % gymName model = createModel ( getModelParamsFromName ( gymName ) ) inputData = \"%s/%s.csv\" % ( DATA_DIR , gymName . replace ( \" \" , \"_\" ) ) runIoThroughNupic ( inputData , model , gymName , plot )',\n",
       "  'score': '83.40049',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_626748',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , q , job_model , input_node , path , images , controller ) : Thread . __init__ ( self ) self . q = q self . job_model = job_model self . input_node = input_node self . path = path self . images = images self . controller = controller',\n",
       "  'score': '83.381645',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_14590',\n",
       "  'title': None,\n",
       "  'text': 'def rlmb_dqn_base ( ) : hparams = _rlmb_base ( ) simulated_rollout_length = 10 dqn_params = dict ( base_algo = \"dqn\" , base_algo_params = \"dqn_original_params\" , real_batch_size = 1 , simulated_batch_size = 16 , dqn_agent_generates_trainable_dones = False , eval_batch_size = 1 , # Must be equal to dqn_time_limit for now simulated_rollout_length = simulated_rollout_length , dqn_time_limit = simulated_rollout_length , simulation_flip_first_random_for_beginning = False , dqn_eval_episodes_num = 3 , # TODO(kc): only for model-free compatibility, remove this epochs_num = - 1 , ) update_hparams ( hparams , dqn_params ) return hparams',\n",
       "  'score': '83.34092',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_47792',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , model , policy = None , test_policy = None , enable_double_dqn = False , enable_dueling_network = False , dueling_type = \\'avg\\' , * args , * * kwargs ) : super ( DQNAgent , self ) . __init__ ( * args , * * kwargs ) # Validate (important) input. if hasattr ( model . output , \\'__len__\\' ) and len ( model . output ) > 1 : raise ValueError ( \\'Model \"{}\" has more than one output. DQN expects a model that has a single output.\\' . format ( model ) ) if model . output . _keras_shape != ( None , self . nb_actions ) : raise ValueError ( \\'Model output \"{}\" has invalid shape. DQN expects a model that has one dimension for each action, in this case {}.\\' . format ( model . output , self . nb_actions ) ) # Parameters. self . enable_double_dqn = enable_double_dqn self . enable_dueling_network = enable_dueling_network self . dueling_type = dueling_type if self . enable_dueling_network : # get the second last layer of the model, abandon the last layer layer = model . layers [ - 2 ] nb_action = model . output . _keras_shape [ - 1 ] # layer y has a shape (nb_action+1,) # y[:,0] represents V(s;theta) # y[:,1:] represents A(s,a;theta) y = Dense ( nb_action + 1 , activation = \\'linear\\' ) ( layer . output ) # caculate the Q(s,a;theta) # dueling_type == \\'avg\\' # Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-Avg_a(A(s,a;theta))) # dueling_type == \\'max\\' # Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-max_a(A(s,a;theta))) # dueling_type == \\'naive\\' # Q(s,a;theta) = V(s;theta) + A(s,a;theta) if self . dueling_type == \\'avg\\' : outputlayer = Lambda ( lambda a : K . expand_dims ( a [ : , 0 ] , - 1 ) + a [ : , 1 : ] - K . mean ( a [ : , 1 : ] , axis = 1 , keepdims = True ) , output_shape = ( nb_action , ) ) ( y ) elif self . dueling_type == \\'max\\' : outputlayer = Lambda ( lambda a : K . expand_dims ( a [ : , 0 ] , - 1 ) + a [ : , 1 : ] - K . max ( a [ : , 1 : ] , axis = 1 , keepdims = True ) , output_shape = ( nb_action , ) ) ( y ) elif self . dueling_type == \\'naive\\' : outputlayer = Lambda ( lambda a : K . expand_dims ( a [ : , 0 ] , - 1 ) + a [ : , 1 : ] , output_shape = ( nb_action , ) ) ( y ) else : assert False , \"dueling_type must be one of {\\'avg\\',\\'max\\',\\'naive\\'}\" model = Model ( inputs = model . input , outputs = outputlayer ) # Related objects. self . model = model if policy is None : policy = EpsGreedyQPolicy ( ) if test_policy is None : test_policy = GreedyQPolicy ( ) self . policy = policy self . test_policy = test_policy # State. self . reset_states ( )',\n",
       "  'score': '83.244576',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_159655',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , qobj_model , default_qubit_los , default_meas_los , * * run_config ) : self . qobj_model = qobj_model self . default_qubit_los = default_qubit_los self . default_meas_los = default_meas_los self . run_config = run_config',\n",
       "  'score': '83.22694',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_667029',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , env , model ) : self . env = env self . model = model',\n",
       "  'score': '83.20547',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_978172',\n",
       "  'title': None,\n",
       "  'text': \"def q_a_x ( self ) : with tf . variable_scope ( 'q_a_x' ) : q_a_x_l = Layers ( self . x_l ) q_a_x_l . fc ( 500 ) . fc ( 500 ) qa_l = GaussianLayer ( q_a_x_l . get_output ( ) , num_latent = 100 , eq_samples = 10 ) self . samples [ 'qa_l' ] = qa_l . compute_samples ( ) with tf . variable_scope ( 'q_a_x' , reuse = True ) : q_a_x_u = Layers ( self . x_u ) q_a_x_u . fc ( 500 ) . fc ( 500 ) qa_u = GaussianLayer ( q_a_x_u . get_output ( ) , num_latent = 100 , eq_samples = 10 ) q_a_samples = qa_u . compute_samples ( ) self . samples [ 'qa_u' ] = tf . tile ( q_a_samples , [ self . num_classes , 1 ] ) return qa_l , qa_u\",\n",
       "  'score': '83.12608',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_256760',\n",
       "  'title': None,\n",
       "  'text': 'def create ( backbone : ModelFactory , input_block : typing . Optional [ ModelFactory ] = None ) : if input_block is None : input_block = IdentityFactory ( ) return QModelFactory ( input_block = input_block , backbone = backbone )',\n",
       "  'score': '83.10129',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_364031',\n",
       "  'title': None,\n",
       "  'text': 'def create_network ( available_actions_num ) : # Creates the input variables s1 = tensor . tensor4 ( \"States\" ) a = tensor . vector ( \"Actions\" , dtype = \"int32\" ) q2 = tensor . vector ( \"Next State best Q-Value\" ) r = tensor . vector ( \"Rewards\" ) nonterminal = tensor . vector ( \"Nonterminal\" , dtype = \"int8\" ) # Creates the input layer of the network. dqn = InputLayer ( shape = [ None , 1 , downsampled_y , downsampled_x ] , input_var = s1 ) # Adds 3 convolutional layers, each followed by a max pooling layer. dqn = Conv2DLayer ( dqn , num_filters = 32 , filter_size = [ 8 , 8 ] , nonlinearity = rectify , W = GlorotUniform ( \"relu\" ) , b = Constant ( .1 ) ) dqn = MaxPool2DLayer ( dqn , pool_size = [ 2 , 2 ] ) dqn = Conv2DLayer ( dqn , num_filters = 64 , filter_size = [ 4 , 4 ] , nonlinearity = rectify , W = GlorotUniform ( \"relu\" ) , b = Constant ( .1 ) ) dqn = MaxPool2DLayer ( dqn , pool_size = [ 2 , 2 ] ) dqn = Conv2DLayer ( dqn , num_filters = 64 , filter_size = [ 3 , 3 ] , nonlinearity = rectify , W = GlorotUniform ( \"relu\" ) , b = Constant ( .1 ) ) dqn = MaxPool2DLayer ( dqn , pool_size = [ 2 , 2 ] ) # Adds a single fully connected layer. dqn = DenseLayer ( dqn , num_units = 512 , nonlinearity = rectify , W = GlorotUniform ( \"relu\" ) , b = Constant ( .1 ) ) # Adds a single fully connected layer which is the output layer. # (no nonlinearity as it is for approximating an arbitrary real function) dqn = DenseLayer ( dqn , num_units = available_actions_num , nonlinearity = None ) # Theano stuff q = get_output ( dqn ) # Only q for the chosen actions is updated more or less according to following formula: # target Q(s,a,t) = r + gamma * max Q(s2,_,t+1) target_q = tensor . set_subtensor ( q [ tensor . arange ( q . shape [ 0 ] ) , a ] , r + discount_factor * nonterminal * q2 ) loss = squared_error ( q , target_q ) . mean ( ) # Updates the parameters according to the computed gradient using rmsprop. params = get_all_params ( dqn , trainable = True ) updates = rmsprop ( loss , params , learning_rate ) # Compiles theano functions print \"Compiling the network ...\" function_learn = theano . function ( [ s1 , q2 , a , r , nonterminal ] , loss , updates = updates , name = \"learn_fn\" ) function_get_q_values = theano . function ( [ s1 ] , q , name = \"eval_fn\" ) function_get_best_action = theano . function ( [ s1 ] , tensor . argmax ( q ) , name = \"test_fn\" ) print \"Network compiled.\" # Returns Theano objects for the net and functions. # We wouldn\\'t need the net anymore but it is nice to save your model. return dqn , function_learn , function_get_q_values , function_get_best_action',\n",
       "  'score': '83.09384',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_23201',\n",
       "  'title': None,\n",
       "  'text': 'def _build_q_value_policy ( self , q_values ) : policy = QValuePolicy ( q_values , self . cur_observations , self . num_actions , self . stochastic , self . eps , self . config [ \"soft_q\" ] , self . config [ \"softmax_temp\" ] ) return policy . action , policy . action_prob',\n",
       "  'score': '83.01218',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_50447',\n",
       "  'title': None,\n",
       "  'text': 'def help_preamble_for ( algo ) : if algo == \"deeplearning\" : return \"\"\"\\n            Build a Deep Neural Network model using CPUs\\n            Builds a feed-forward multilayer artificial neural network on an H2OFrame\"\"\" if algo == \"deepwater\" : return \"\"\"\\n            Build a Deep Learning model using multiple native GPU backends\\n            Builds a deep neural network on an H2OFrame containing various data sources\"\"\" if algo == \"kmeans\" : return \"\"\"Performs k-means clustering on an H2O dataset.\"\"\" if algo == \"glrm\" : return \"\"\"Builds a generalized low rank model of a H2O dataset.\"\"\" if algo == \"glm\" : return \"\"\"\\n            Fits a generalized linear model, specified by a response variable, a set of predictors, and a\\n            description of the error distribution.\"\"\" if algo == \"gbm\" : return \"\"\"\\n            Builds gradient boosted trees on a parsed data set, for regression or classification.\\n            The default distribution function will guess the model type based on the response column type.\\n            Otherwise, the response column must be an enum for \"bernoulli\" or \"multinomial\", and numeric\\n            for all other distributions.\"\"\" if algo == \"xgboost\" : return \"\"\"Builds a eXtreme Gradient Boosting model using the native XGBoost backend.\"\"\" if algo == \"naivebayes\" : return \"\"\"\\n            The naive Bayes classifier assumes independence between predictor variables\\n            conditional on the response, and a Gaussian distribution of numeric predictors with\\n            mean and standard deviation computed from the training dataset. When building a naive\\n            Bayes classifier, every row in the training dataset that contains at least one NA will\\n            be skipped completely. If the test dataset has missing values, then those predictors\\n            are omitted in the probability calculation during prediction.\"\"\" if algo == \"stackedensemble\" : return \"\"\"\\n            Builds a stacked ensemble (aka \"super learner\") machine learning method that uses two\\n            or more H2O learning algorithms to improve predictive performance. It is a loss-based\\n            supervised learning method that finds the optimal combination of a collection of prediction\\n            algorithms.This method supports regression and binary classification. \"\"\" if algo == \"isolationforest\" : return \"\"\"\\n            Builds an Isolation Forest model. Isolation Forest algorithm samples the training frame\\n            and in each iteration builds a tree that partitions the space of the sample observations until\\n            it isolates each observation. Length of the path from root to a leaf node of the resulting tree\\n            is used to calculate the anomaly score. Anomalies are easier to isolate and their average\\n            tree path is expected to be shorter than paths of regular observations.\\n        \"\"\"',\n",
       "  'score': '82.98151',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_257223',\n",
       "  'title': None,\n",
       "  'text': 'def create ( fc_layers = None , dropout = None , pretrained = True ) : def instantiate ( * * _ ) : return Resnet34 ( fc_layers , dropout , pretrained ) return ModelFactory . generic ( instantiate )',\n",
       "  'score': '82.97776',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_19734',\n",
       "  'title': None,\n",
       "  'text': \"def train_episode ( agent , envs , preprocessors , t_max , render ) : num_envs = len ( envs ) # Buffers to hold trajectories, e.g. `env_xs[i]` will hold the observations # for environment `i`. env_xs , env_as = _2d_list ( num_envs ) , _2d_list ( num_envs ) env_rs , env_vs = _2d_list ( num_envs ) , _2d_list ( num_envs ) episode_rs = np . zeros ( num_envs , dtype = np . float ) for p in preprocessors : p . reset ( ) observations = [ p . preprocess ( e . reset ( ) ) for p , e in zip ( preprocessors , envs ) ] done = np . array ( [ False for _ in range ( num_envs ) ] ) all_done = False t = 1 while not all_done : if render : envs [ 0 ] . render ( ) # NOTE(reed): Reshape to set the data shape. agent . model . reshape ( [ ( 'data' , ( num_envs , preprocessors [ 0 ] . obs_size ) ) ] ) step_xs = np . vstack ( [ o . ravel ( ) for o in observations ] ) # Get actions and values for all environments in a single forward pass. step_xs_nd = mx . nd . array ( step_xs , ctx = agent . ctx ) data_batch = mx . io . DataBatch ( data = [ step_xs_nd ] , label = None ) agent . model . forward ( data_batch , is_train = False ) _ , step_vs , _ , step_ps = agent . model . get_outputs ( ) step_ps = step_ps . asnumpy ( ) step_vs = step_vs . asnumpy ( ) step_as = agent . act ( step_ps ) # Step each environment whose episode has not completed. for i , env in enumerate ( envs ) : if not done [ i ] : obs , r , done [ i ] , _ = env . step ( step_as [ i ] ) # Record the observation, action, value, and reward in the # buffers. env_xs [ i ] . append ( step_xs [ i ] . ravel ( ) ) env_as [ i ] . append ( step_as [ i ] ) env_vs [ i ] . append ( step_vs [ i ] [ 0 ] ) env_rs [ i ] . append ( r ) episode_rs [ i ] += r # Add 0 as the state value when done. if done [ i ] : env_vs [ i ] . append ( 0.0 ) else : observations [ i ] = preprocessors [ i ] . preprocess ( obs ) # Perform an update every `t_max` steps. if t == t_max : # If the episode has not finished, add current state's value. This # will be used to 'bootstrap' the final return (see Algorithm S3 # in A3C paper). step_xs = np . vstack ( [ o . ravel ( ) for o in observations ] ) step_xs_nd = mx . nd . array ( step_xs , ctx = agent . ctx ) data_batch = mx . io . DataBatch ( data = [ step_xs_nd ] , label = None ) agent . model . forward ( data_batch , is_train = False ) _ , extra_vs , _ , _ = agent . model . get_outputs ( ) extra_vs = extra_vs . asnumpy ( ) for i in range ( num_envs ) : if not done [ i ] : env_vs [ i ] . append ( extra_vs [ i ] [ 0 ] ) # Perform update and clear buffers. env_xs = np . vstack ( list ( chain . from_iterable ( env_xs ) ) ) agent . train_step ( env_xs , env_as , env_rs , env_vs ) env_xs , env_as = _2d_list ( num_envs ) , _2d_list ( num_envs ) env_rs , env_vs = _2d_list ( num_envs ) , _2d_list ( num_envs ) t = 0 all_done = np . all ( done ) t += 1 return episode_rs\",\n",
       "  'score': '82.96161',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_349715',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , actions ) : super ( DenseQ , self ) . __init__ ( ) inputShape = ( 4 , 84 , 84 ) self . model = nn . Sequential ( nn . Conv2d ( in_channels = inputShape [ 0 ] , out_channels = 16 , kernel_size = 8 , stride = 4 ) , nn . ReLU ( ) , nn . Conv2d ( in_channels = 16 , out_channels = 32 , kernel_size = 4 , stride = 2 ) , nn . ReLU ( ) , Flatten ( ) , nn . Linear ( in_features = 32 * 9 * 9 , out_features = 256 ) , nn . LayerNorm ( 256 ) , nn . ReLU ( ) , nn . Linear ( in_features = 256 , out_features = actions ) , )',\n",
       "  'score': '82.94469',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_20301',\n",
       "  'title': None,\n",
       "  'text': \"def alexnet ( pretrained = False , ctx = cpu ( ) , root = os . path . join ( base . data_dir ( ) , 'models' ) , * * kwargs ) : net = AlexNet ( * * kwargs ) if pretrained : from . . model_store import get_model_file net . load_parameters ( get_model_file ( 'alexnet' , root = root ) , ctx = ctx ) return net\",\n",
       "  'score': '82.87547',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_13959',\n",
       "  'title': None,\n",
       "  'text': 'def learn ( network , env , seed , total_timesteps = int ( 40e6 ) , gamma = 0.99 , log_interval = 1 , nprocs = 32 , nsteps = 20 , ent_coef = 0.01 , vf_coef = 0.5 , vf_fisher_coef = 1.0 , lr = 0.25 , max_grad_norm = 0.5 , kfac_clip = 0.001 , save_interval = None , lrschedule = \\'linear\\' , load_path = None , is_async = True , * * network_kwargs ) : set_global_seeds ( seed ) if network == \\'cnn\\' : network_kwargs [ \\'one_dim_bias\\' ] = True policy = build_policy ( env , network , * * network_kwargs ) nenvs = env . num_envs ob_space = env . observation_space ac_space = env . action_space make_model = lambda : Model ( policy , ob_space , ac_space , nenvs , total_timesteps , nprocs = nprocs , nsteps = nsteps , ent_coef = ent_coef , vf_coef = vf_coef , vf_fisher_coef = vf_fisher_coef , lr = lr , max_grad_norm = max_grad_norm , kfac_clip = kfac_clip , lrschedule = lrschedule , is_async = is_async ) if save_interval and logger . get_dir ( ) : import cloudpickle with open ( osp . join ( logger . get_dir ( ) , \\'make_model.pkl\\' ) , \\'wb\\' ) as fh : fh . write ( cloudpickle . dumps ( make_model ) ) model = make_model ( ) if load_path is not None : model . load ( load_path ) runner = Runner ( env , model , nsteps = nsteps , gamma = gamma ) epinfobuf = deque ( maxlen = 100 ) nbatch = nenvs * nsteps tstart = time . time ( ) coord = tf . train . Coordinator ( ) if is_async : enqueue_threads = model . q_runner . create_threads ( model . sess , coord = coord , start = True ) else : enqueue_threads = [ ] for update in range ( 1 , total_timesteps // nbatch + 1 ) : obs , states , rewards , masks , actions , values , epinfos = runner . run ( ) epinfobuf . extend ( epinfos ) policy_loss , value_loss , policy_entropy = model . train ( obs , states , rewards , masks , actions , values ) model . old_obs = obs nseconds = time . time ( ) - tstart fps = int ( ( update * nbatch ) / nseconds ) if update % log_interval == 0 or update == 1 : ev = explained_variance ( values , rewards ) logger . record_tabular ( \"nupdates\" , update ) logger . record_tabular ( \"total_timesteps\" , update * nbatch ) logger . record_tabular ( \"fps\" , fps ) logger . record_tabular ( \"policy_entropy\" , float ( policy_entropy ) ) logger . record_tabular ( \"policy_loss\" , float ( policy_loss ) ) logger . record_tabular ( \"value_loss\" , float ( value_loss ) ) logger . record_tabular ( \"explained_variance\" , float ( ev ) ) logger . record_tabular ( \"eprewmean\" , safemean ( [ epinfo [ \\'r\\' ] for epinfo in epinfobuf ] ) ) logger . record_tabular ( \"eplenmean\" , safemean ( [ epinfo [ \\'l\\' ] for epinfo in epinfobuf ] ) ) logger . dump_tabular ( ) if save_interval and ( update % save_interval == 0 or update == 1 ) and logger . get_dir ( ) : savepath = osp . join ( logger . get_dir ( ) , \\'checkpoint%.5i\\' % update ) print ( \\'Saving to\\' , savepath ) model . save ( savepath ) coord . request_stop ( ) coord . join ( enqueue_threads ) return model',\n",
       "  'score': '82.794876',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_36156',\n",
       "  'title': None,\n",
       "  'text': 'def deep_exponential_family_variational ( data_size , feature_size , units ) : qw2 = trainable_positive_deterministic ( [ units [ 2 ] , units [ 1 ] ] , name = \"qw2\" ) qw1 = trainable_positive_deterministic ( [ units [ 1 ] , units [ 0 ] ] , name = \"qw1\" ) qw0 = trainable_positive_deterministic ( [ units [ 0 ] , feature_size ] , name = \"qw0\" ) qz2 = trainable_gamma ( [ data_size , units [ 2 ] ] , name = \"qz2\" ) qz1 = trainable_gamma ( [ data_size , units [ 1 ] ] , name = \"qz1\" ) qz0 = trainable_gamma ( [ data_size , units [ 0 ] ] , name = \"qz0\" ) return qw2 , qw1 , qw0 , qz2 , qz1 , qz0',\n",
       "  'score': '82.76985',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_14242',\n",
       "  'title': None,\n",
       "  'text': 'def learn ( * , network , env , total_timesteps , timesteps_per_batch = 1024 , # what to train on max_kl = 0.001 , cg_iters = 10 , gamma = 0.99 , lam = 1.0 , # advantage estimation seed = None , ent_coef = 0.0 , cg_damping = 1e-2 , vf_stepsize = 3e-4 , vf_iters = 3 , max_episodes = 0 , max_iters = 0 , # time constraint callback = None , load_path = None , * * network_kwargs ) : if MPI is not None : nworkers = MPI . COMM_WORLD . Get_size ( ) rank = MPI . COMM_WORLD . Get_rank ( ) else : nworkers = 1 rank = 0 cpus_per_worker = 1 U . get_session ( config = tf . ConfigProto ( allow_soft_placement = True , inter_op_parallelism_threads = cpus_per_worker , intra_op_parallelism_threads = cpus_per_worker ) ) policy = build_policy ( env , network , value_network = \\'copy\\' , * * network_kwargs ) set_global_seeds ( seed ) np . set_printoptions ( precision = 3 ) # Setup losses and stuff # ---------------------------------------- ob_space = env . observation_space ac_space = env . action_space ob = observation_placeholder ( ob_space ) with tf . variable_scope ( \"pi\" ) : pi = policy ( observ_placeholder = ob ) with tf . variable_scope ( \"oldpi\" ) : oldpi = policy ( observ_placeholder = ob ) atarg = tf . placeholder ( dtype = tf . float32 , shape = [ None ] ) # Target advantage function (if applicable) ret = tf . placeholder ( dtype = tf . float32 , shape = [ None ] ) # Empirical return ac = pi . pdtype . sample_placeholder ( [ None ] ) kloldnew = oldpi . pd . kl ( pi . pd ) ent = pi . pd . entropy ( ) meankl = tf . reduce_mean ( kloldnew ) meanent = tf . reduce_mean ( ent ) entbonus = ent_coef * meanent vferr = tf . reduce_mean ( tf . square ( pi . vf - ret ) ) ratio = tf . exp ( pi . pd . logp ( ac ) - oldpi . pd . logp ( ac ) ) # advantage * pnew / pold surrgain = tf . reduce_mean ( ratio * atarg ) optimgain = surrgain + entbonus losses = [ optimgain , meankl , entbonus , surrgain , meanent ] loss_names = [ \"optimgain\" , \"meankl\" , \"entloss\" , \"surrgain\" , \"entropy\" ] dist = meankl all_var_list = get_trainable_variables ( \"pi\" ) # var_list = [v for v in all_var_list if v.name.split(\"/\")[1].startswith(\"pol\")] # vf_var_list = [v for v in all_var_list if v.name.split(\"/\")[1].startswith(\"vf\")] var_list = get_pi_trainable_variables ( \"pi\" ) vf_var_list = get_vf_trainable_variables ( \"pi\" ) vfadam = MpiAdam ( vf_var_list ) get_flat = U . GetFlat ( var_list ) set_from_flat = U . SetFromFlat ( var_list ) klgrads = tf . gradients ( dist , var_list ) flat_tangent = tf . placeholder ( dtype = tf . float32 , shape = [ None ] , name = \"flat_tan\" ) shapes = [ var . get_shape ( ) . as_list ( ) for var in var_list ] start = 0 tangents = [ ] for shape in shapes : sz = U . intprod ( shape ) tangents . append ( tf . reshape ( flat_tangent [ start : start + sz ] , shape ) ) start += sz gvp = tf . add_n ( [ tf . reduce_sum ( g * tangent ) for ( g , tangent ) in zipsame ( klgrads , tangents ) ] ) #pylint: disable=E1111 fvp = U . flatgrad ( gvp , var_list ) assign_old_eq_new = U . function ( [ ] , [ ] , updates = [ tf . assign ( oldv , newv ) for ( oldv , newv ) in zipsame ( get_variables ( \"oldpi\" ) , get_variables ( \"pi\" ) ) ] ) compute_losses = U . function ( [ ob , ac , atarg ] , losses ) compute_lossandgrad = U . function ( [ ob , ac , atarg ] , losses + [ U . flatgrad ( optimgain , var_list ) ] ) compute_fvp = U . function ( [ flat_tangent , ob , ac , atarg ] , fvp ) compute_vflossandgrad = U . function ( [ ob , ret ] , U . flatgrad ( vferr , vf_var_list ) ) @ contextmanager def timed ( msg ) : if rank == 0 : print ( colorize ( msg , color = \\'magenta\\' ) ) tstart = time . time ( ) yield print ( colorize ( \"done in %.3f seconds\" % ( time . time ( ) - tstart ) , color = \\'magenta\\' ) ) else : yield def allmean ( x ) : assert isinstance ( x , np . ndarray ) if MPI is not None : out = np . empty_like ( x ) MPI . COMM_WORLD . Allreduce ( x , out , op = MPI . SUM ) out /= nworkers else : out = np . copy ( x ) return out U . initialize ( ) if load_path is not None : pi . load ( load_path ) th_init = get_flat ( ) if MPI is not None : MPI . COMM_WORLD . Bcast ( th_init , root = 0 ) set_from_flat ( th_init ) vfadam . sync ( ) print ( \"Init param sum\" , th_init . sum ( ) , flush = True ) # Prepare for rollouts # ---------------------------------------- seg_gen = traj_segment_generator ( pi , env , timesteps_per_batch , stochastic = True ) episodes_so_far = 0 timesteps_so_far = 0 iters_so_far = 0 tstart = time . time ( ) lenbuffer = deque ( maxlen = 40 ) # rolling buffer for episode lengths rewbuffer = deque ( maxlen = 40 ) # rolling buffer for episode rewards if sum ( [ max_iters > 0 , total_timesteps > 0 , max_episodes > 0 ] ) == 0 : # noththing to be done return pi assert sum ( [ max_iters > 0 , total_timesteps > 0 , max_episodes > 0 ] ) < 2 , \\'out of max_iters, total_timesteps, and max_episodes only one should be specified\\' while True : if callback : callback ( locals ( ) , globals ( ) ) if total_timesteps and timesteps_so_far >= total_timesteps : break elif max_episodes and episodes_so_far >= max_episodes : break elif max_iters and iters_so_far >= max_iters : break logger . log ( \"********** Iteration %i ************\" % iters_so_far ) with timed ( \"sampling\" ) : seg = seg_gen . __next__ ( ) add_vtarg_and_adv ( seg , gamma , lam ) # ob, ac, atarg, ret, td1ret = map(np.concatenate, (obs, acs, atargs, rets, td1rets)) ob , ac , atarg , tdlamret = seg [ \"ob\" ] , seg [ \"ac\" ] , seg [ \"adv\" ] , seg [ \"tdlamret\" ] vpredbefore = seg [ \"vpred\" ] # predicted value function before udpate atarg = ( atarg - atarg . mean ( ) ) / atarg . std ( ) # standardized advantage function estimate if hasattr ( pi , \"ret_rms\" ) : pi . ret_rms . update ( tdlamret ) if hasattr ( pi , \"ob_rms\" ) : pi . ob_rms . update ( ob ) # update running mean/std for policy args = seg [ \"ob\" ] , seg [ \"ac\" ] , atarg fvpargs = [ arr [ : : 5 ] for arr in args ] def fisher_vector_product ( p ) : return allmean ( compute_fvp ( p , * fvpargs ) ) + cg_damping * p assign_old_eq_new ( ) # set old parameter values to new parameter values with timed ( \"computegrad\" ) : * lossbefore , g = compute_lossandgrad ( * args ) lossbefore = allmean ( np . array ( lossbefore ) ) g = allmean ( g ) if np . allclose ( g , 0 ) : logger . log ( \"Got zero gradient. not updating\" ) else : with timed ( \"cg\" ) : stepdir = cg ( fisher_vector_product , g , cg_iters = cg_iters , verbose = rank == 0 ) assert np . isfinite ( stepdir ) . all ( ) shs = .5 * stepdir . dot ( fisher_vector_product ( stepdir ) ) lm = np . sqrt ( shs / max_kl ) # logger.log(\"lagrange multiplier:\", lm, \"gnorm:\", np.linalg.norm(g)) fullstep = stepdir / lm expectedimprove = g . dot ( fullstep ) surrbefore = lossbefore [ 0 ] stepsize = 1.0 thbefore = get_flat ( ) for _ in range ( 10 ) : thnew = thbefore + fullstep * stepsize set_from_flat ( thnew ) meanlosses = surr , kl ,  * _ = allmean ( np . array ( compute_losses ( * args ) ) ) improve = surr - surrbefore logger . log ( \"Expected: %.3f Actual: %.3f\" % ( expectedimprove , improve ) ) if not np . isfinite ( meanlosses ) . all ( ) : logger . log ( \"Got non-finite value of losses -- bad!\" ) elif kl > max_kl * 1.5 : logger . log ( \"violated KL constraint. shrinking step.\" ) elif improve < 0 : logger . log ( \"surrogate didn\\'t improve. shrinking step.\" ) else : logger . log ( \"Stepsize OK!\" ) break stepsize *= .5 else : logger . log ( \"couldn\\'t compute a good step\" ) set_from_flat ( thbefore ) if nworkers > 1 and iters_so_far % 20 == 0 : paramsums = MPI . COMM_WORLD . allgather ( ( thnew . sum ( ) , vfadam . getflat ( ) . sum ( ) ) ) # list of tuples assert all ( np . allclose ( ps , paramsums [ 0 ] ) for ps in paramsums [ 1 : ] ) for ( lossname , lossval ) in zip ( loss_names , meanlosses ) : logger . record_tabular ( lossname , lossval ) with timed ( \"vf\" ) : for _ in range ( vf_iters ) : for ( mbob , mbret ) in dataset . iterbatches ( ( seg [ \"ob\" ] , seg [ \"tdlamret\" ] ) , include_final_partial_batch = False , batch_size = 64 ) : g = allmean ( compute_vflossandgrad ( mbob , mbret ) ) vfadam . update ( g , vf_stepsize ) logger . record_tabular ( \"ev_tdlam_before\" , explained_variance ( vpredbefore , tdlamret ) ) lrlocal = ( seg [ \"ep_lens\" ] , seg [ \"ep_rets\" ] ) # local values if MPI is not None : listoflrpairs = MPI . COMM_WORLD . allgather ( lrlocal ) # list of tuples else : listoflrpairs = [ lrlocal ] lens , rews = map ( flatten_lists , zip ( * listoflrpairs ) ) lenbuffer . extend ( lens ) rewbuffer . extend ( rews ) logger . record_tabular ( \"EpLenMean\" , np . mean ( lenbuffer ) ) logger . record_tabular ( \"EpRewMean\" , np . mean ( rewbuffer ) ) logger . record_tabular ( \"EpThisIter\" , len ( lens ) ) episodes_so_far += len ( lens ) timesteps_so_far += sum ( lens ) iters_so_far += 1 logger . record_tabular ( \"EpisodesSoFar\" , episodes_so_far ) logger . record_tabular ( \"TimestepsSoFar\" , timesteps_so_far ) logger . record_tabular ( \"TimeElapsed\" , time . time ( ) - tstart ) if rank == 0 : logger . dump_tabular ( ) return pi',\n",
       "  'score': '82.755615',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_563979',\n",
       "  'title': None,\n",
       "  'text': \"def __init__ ( self , model , xw = 0 , yw = 0 , Qw = 100.0 , rw = 0.1 , res = 0.0 , layers = 0 , label = None ) : self . storeinput ( inspect . currentframe ( ) ) WellBase . __init__ ( self , model , xw , yw , Qw , rw , res , layers = layers , name = 'Well' , label = label ) self . Qc = float ( Qw ) if self . nlayers == 1 : self . nunknowns = 0 else : self . nunknowns = self . nparam\",\n",
       "  'score': '82.67651',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_213492',\n",
       "  'title': None,\n",
       "  'text': 'def multihead_self_attention_incremental ( query_antecedent , prev_k , prev_v , step_num , master_dtype , slice_dtype , name = \"multihead_attention\" ) : batch_dims = query_antecedent . shape . dims [ : - 1 ] io_channels = query_antecedent . shape . dims [ - 1 ] heads , memory_length , kv_channels = prev_k . shape . dims [ - 3 : ] with tf . variable_scope ( name , default_name = \"multihead_attention\" ) : wq , wk , wv , wo = multihead_attention_vars ( query_antecedent . mesh , heads , io_channels , kv_channels , master_dtype , slice_dtype , query_antecedent . dtype ) memory_antecedent = query_antecedent q = mtf . einsum ( [ query_antecedent , wq ] , mtf . Shape ( batch_dims + [ heads , kv_channels ] ) ) k = mtf . einsum ( [ memory_antecedent , wk ] , mtf . Shape ( batch_dims + [ heads , kv_channels ] ) ) v = mtf . einsum ( [ memory_antecedent , wv ] , mtf . Shape ( batch_dims + [ heads , kv_channels ] ) ) k = prev_k + mtf . multiply ( k , mtf . one_hot ( step_num , memory_length , dtype = prev_k . dtype ) , output_shape = prev_k . shape ) v = prev_v + mtf . multiply ( v , mtf . one_hot ( step_num , memory_length , dtype = prev_v . dtype ) , output_shape = prev_v . shape ) mask = mtf . cast ( mtf . greater ( mtf . range ( query_antecedent . mesh , memory_length , dtype = tf . int32 ) , step_num ) , q . dtype ) * - 1e9 o = dot_product_attention ( q , k , v , mask ) y = mtf . einsum ( [ o , wo ] , query_antecedent . shape ) return y , k , v',\n",
       "  'score': '82.61078',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_14193',\n",
       "  'title': None,\n",
       "  'text': \"def train ( self ) : # Get a batch. batch = self . memory . sample ( batch_size = self . batch_size ) if self . normalize_returns and self . enable_popart : old_mean , old_std , target_Q = self . sess . run ( [ self . ret_rms . mean , self . ret_rms . std , self . target_Q ] , feed_dict = { self . obs1 : batch [ 'obs1' ] , self . rewards : batch [ 'rewards' ] , self . terminals1 : batch [ 'terminals1' ] . astype ( 'float32' ) , } ) self . ret_rms . update ( target_Q . flatten ( ) ) self . sess . run ( self . renormalize_Q_outputs_op , feed_dict = { self . old_std : np . array ( [ old_std ] ) , self . old_mean : np . array ( [ old_mean ] ) , } ) # Run sanity check. Disabled by default since it slows down things considerably. # print('running sanity check') # target_Q_new, new_mean, new_std = self.sess.run([self.target_Q, self.ret_rms.mean, self.ret_rms.std], feed_dict={ #     self.obs1: batch['obs1'], #     self.rewards: batch['rewards'], #     self.terminals1: batch['terminals1'].astype('float32'), # }) # print(target_Q_new, target_Q, new_mean, new_std) # assert (np.abs(target_Q - target_Q_new) < 1e-3).all() else : target_Q = self . sess . run ( self . target_Q , feed_dict = { self . obs1 : batch [ 'obs1' ] , self . rewards : batch [ 'rewards' ] , self . terminals1 : batch [ 'terminals1' ] . astype ( 'float32' ) , } ) # Get all gradients and perform a synced update. ops = [ self . actor_grads , self . actor_loss , self . critic_grads , self . critic_loss ] actor_grads , actor_loss , critic_grads , critic_loss = self . sess . run ( ops , feed_dict = { self . obs0 : batch [ 'obs0' ] , self . actions : batch [ 'actions' ] , self . critic_target : target_Q , } ) self . actor_optimizer . update ( actor_grads , stepsize = self . actor_lr ) self . critic_optimizer . update ( critic_grads , stepsize = self . critic_lr ) return critic_loss , actor_loss\",\n",
       "  'score': '82.55232',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_47811',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , model , nb_actions , policy = None , test_policy = None , gamma = .99 , nb_steps_warmup = 10 , train_interval = 1 , delta_clip = np . inf , * args , * * kwargs ) : super ( SarsaAgent , self ) . __init__ ( * args , * * kwargs ) # Do not use defaults in constructor because that would mean that each instance shares the same # policy. if policy is None : policy = EpsGreedyQPolicy ( ) if test_policy is None : test_policy = GreedyQPolicy ( ) self . model = model self . nb_actions = nb_actions self . policy = policy self . test_policy = test_policy self . gamma = gamma self . nb_steps_warmup = nb_steps_warmup self . train_interval = train_interval self . delta_clip = delta_clip self . compiled = False self . actions = None self . observations = None self . rewards = None',\n",
       "  'score': '82.51904',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_22370',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , model , data , session = None , learning_phase_flags = None ) : # warnings.warn( #     \"Please keep in mind DeepExplainer is brand new, and we are still developing it and working on \" + #     \"characterizing/testing it on large networks. This means you should keep an eye out for odd \" + #     \"behavior. Post any issues you run into on github.\" # ) # try and import keras and tensorflow global tf , tf_ops , tf_gradients_impl if tf is None : from tensorflow . python . framework import ops as tf_ops # pylint: disable=E0611 from tensorflow . python . ops import gradients_impl as tf_gradients_impl # pylint: disable=E0611 import tensorflow as tf if LooseVersion ( tf . __version__ ) < LooseVersion ( \"1.4.0\" ) : warnings . warn ( \"Your TensorFlow version is older than 1.4.0 and not supported.\" ) global keras if keras is None : try : import keras if LooseVersion ( keras . __version__ ) < LooseVersion ( \"2.1.0\" ) : warnings . warn ( \"Your Keras version is older than 2.1.0 and not supported.\" ) except : pass # determine the model inputs and outputs if str ( type ( model ) ) . endswith ( \"keras.engine.sequential.Sequential\\'>\" ) : self . model_inputs = model . inputs self . model_output = model . layers [ - 1 ] . output elif str ( type ( model ) ) . endswith ( \"keras.models.Sequential\\'>\" ) : self . model_inputs = model . inputs self . model_output = model . layers [ - 1 ] . output elif str ( type ( model ) ) . endswith ( \"keras.engine.training.Model\\'>\" ) : self . model_inputs = model . inputs self . model_output = model . layers [ - 1 ] . output elif str ( type ( model ) ) . endswith ( \"tuple\\'>\" ) : self . model_inputs = model [ 0 ] self . model_output = model [ 1 ] else : assert False , str ( type ( model ) ) + \" is not currently a supported model type!\" assert type ( self . model_output ) != list , \"The model output to be explained must be a single tensor!\" assert len ( self . model_output . shape ) < 3 , \"The model output must be a vector or a single value!\" self . multi_output = True if len ( self . model_output . shape ) == 1 : self . multi_output = False # check if we have multiple inputs self . multi_input = True if type ( self . model_inputs ) != list or len ( self . model_inputs ) == 1 : self . multi_input = False if type ( self . model_inputs ) != list : self . model_inputs = [ self . model_inputs ] if type ( data ) != list and ( hasattr ( data , \\'__call__\\' ) == False ) : data = [ data ] self . data = data self . _vinputs = { } # used to track what op inputs depends on the model inputs self . orig_grads = { } # if we are not given a session find a default session if session is None : # if keras is installed and already has a session then use it if keras is not None and keras . backend . tensorflow_backend . _SESSION is not None : session = keras . backend . get_session ( ) else : session = tf . keras . backend . get_session ( ) self . session = tf . get_default_session ( ) if session is None else session # if no learning phase flags were given we go looking for them # ...this will catch the one that keras uses # we need to find them since we want to make sure learning phase flags are set to False if learning_phase_flags is None : self . learning_phase_ops = [ ] for op in self . session . graph . get_operations ( ) : if \\'learning_phase\\' in op . name and op . type == \"Const\" and len ( op . outputs [ 0 ] . shape ) == 0 : if op . outputs [ 0 ] . dtype == tf . bool : self . learning_phase_ops . append ( op ) self . learning_phase_flags = [ op . outputs [ 0 ] for op in self . learning_phase_ops ] else : self . learning_phase_ops = [ t . op for t in learning_phase_flags ] # save the expected output of the model # if self.data is a function, set self.expected_value to None if ( hasattr ( self . data , \\'__call__\\' ) ) : self . expected_value = None else : if self . data [ 0 ] . shape [ 0 ] > 5000 : warnings . warn ( \"You have provided over 5k background samples! For better performance consider using smaller random sample.\" ) self . expected_value = self . run ( self . model_output , self . model_inputs , self . data ) . mean ( 0 ) # find all the operations in the graph between our inputs and outputs tensor_blacklist = tensors_blocked_by_false ( self . learning_phase_ops ) # don\\'t follow learning phase branches dependence_breakers = [ k for k in op_handlers if op_handlers [ k ] == break_dependence ] back_ops = backward_walk_ops ( [ self . model_output . op ] , tensor_blacklist , dependence_breakers ) self . between_ops = forward_walk_ops ( [ op for input in self . model_inputs for op in input . consumers ( ) ] , tensor_blacklist , dependence_breakers , within_ops = back_ops ) # save what types are being used self . used_types = { } for op in self . between_ops : self . used_types [ op . type ] = True # make a blank array that will get lazily filled in with the SHAP value computation # graphs for each output. Lazy is important since if there are 1000 outputs and we # only explain the top 5 it would be a waste to build graphs for the other 995 if not self . multi_output : self . phi_symbolics = [ None ] else : noutputs = self . model_output . shape . as_list ( ) [ 1 ] if noutputs is not None : self . phi_symbolics = [ None for i in range ( noutputs ) ] else : raise Exception ( \"The model output tensor to be explained cannot have a static shape in dim 1 of None!\" )',\n",
       "  'score': '82.51095',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_510895',\n",
       "  'title': None,\n",
       "  'text': 'def build_model ( self ) : model = Sequential ( ) model . add ( Dense ( 64 , input_shape = nn_input_dim_NN ) ) #model.add(Dropout(0.5)) model . add ( Dense ( 1 ) ) model . add ( Activation ( \\'linear\\' ) ) sgd = SGD ( lr = 0.001 ) model . compile ( loss = \\'mean_squared_error\\' , optimizer = sgd , metrics = [ \"accuracy\" ] ) return KerasRegressor ( nn = model , * * self . params )',\n",
       "  'score': '82.48193',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_31219',\n",
       "  'title': None,\n",
       "  'text': 'def build_model ( layers ) : model = Sequential ( ) model . add ( LSTM ( input_dim = layers [ 0 ] , output_dim = layers [ 1 ] , return_sequences = True ) ) model . add ( Dropout ( 0.2 ) ) model . add ( LSTM ( layers [ 2 ] , return_sequences = False ) ) model . add ( Dropout ( 0.2 ) ) model . add ( Dense ( output_dim = layers [ 2 ] ) ) model . add ( Activation ( \"linear\" ) ) start = time . time ( ) model . compile ( loss = \"mse\" , optimizer = \"rmsprop\" , metrics = [ \\'accuracy\\' ] ) print ( \"Compilation Time : \" , time . time ( ) - start ) return model',\n",
       "  'score': '82.47215',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_839442',\n",
       "  'title': None,\n",
       "  'text': \"def bdpg_chains ( self : Experiment , logdir = None , env = 3 , heads = 3 , n = 10 , bootstrap = True ) : from tensorflow . contrib import layers import gym from gym import spaces from gym import wrappers import numpy as np from tensorflow . contrib . framework import arg_scope def gym_make ( id ) -> gym . Env : return gym . make ( id ) chi . set_loglevel ( 'debug' ) import gym_mix from chi . rl . wrappers import PenalizeAction env = gym_mix . envs . ChainEnv ( n ) env = PenalizeAction ( env , .001 , 1 ) print_env ( env ) def ac ( x ) : with tf . name_scope ( 'actor_head' ) : x = layers . fully_connected ( x , 50 , biases_initializer = layers . xavier_initializer ( ) ) x = layers . fully_connected ( x , 50 , biases_initializer = layers . xavier_initializer ( ) ) # a = layers.fully_connected(x, env.action_space.shape[0], None, weights_initializer=tf.random_normal_initializer(0, 1e-4)) a = layers . fully_connected ( x , env . action_space . shape [ 0 ] , None ) return a def cr ( x , a ) : with tf . name_scope ( 'critic_head' ) : x = layers . fully_connected ( x , 50 , biases_initializer = layers . xavier_initializer ( ) ) x = tf . concat ( [ x , a ] , axis = 1 ) x = layers . fully_connected ( x , 50 , biases_initializer = layers . xavier_initializer ( ) ) # q = layers.fully_connected(x, 1, None, weights_initializer=tf.random_normal_initializer(0, 1e-4)) q = layers . fully_connected ( x , 1 , None ) return tf . squeeze ( q , 1 ) if bootstrap : agent = BdpgAgent ( env , ac , cr , heads = heads , replay_start = 5000 ) else : agent = DdpgAgent ( env , ac , cr , replay_start = 5000 ) threshold = getattr ( getattr ( env , 'spec' , None ) , 'reward_threshold' , None ) for ep in range ( 100000 ) : R , info = agent . play_episode ( ) if ep % 20 == 0 : head = info . get ( 'head' ) print ( f'Return of episode {ep} after timestep {agent.t}: {R} (head = {head}, threshold = {threshold})' )\",\n",
       "  'score': '82.444016',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_20228',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , env_spec ) : super ( ContinuousMLPQ , self ) . __init__ ( env_spec ) self . obs = mx . symbol . Variable ( \"obs\" ) self . act = mx . symbol . Variable ( \"act\" ) self . qval = define_qfunc ( self . obs , self . act ) self . yval = mx . symbol . Variable ( \"yval\" )',\n",
       "  'score': '82.43527',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_338505',\n",
       "  'title': None,\n",
       "  'text': \"def get_model ( with_pipeline = False ) : model = NeuralNetClassifier ( MLPClassifier ) if with_pipeline : model = Pipeline ( [ ( 'scale' , FeatureUnion ( [ ( 'minmax' , MinMaxScaler ( ) ) , ( 'normalize' , Normalizer ( ) ) , ] ) ) , ( 'select' , SelectKBest ( k = N_FEATURES ) ) , # keep input size constant ( 'net' , model ) , ] ) return model\",\n",
       "  'score': '82.41861',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_422278',\n",
       "  'title': None,\n",
       "  'text': \"def _build_model ( self , * * kwargs ) : model = keras . models . Sequential ( ) hyperparameters = self . hyperparameters . copy ( ) hyperparameters . update ( kwargs ) for layer in self . layers : layer_kwargs = layer [ 'parameters' ] . copy ( ) for key , value in layer_kwargs . items ( ) : if isinstance ( value , str ) : layer_kwargs [ key ] = hyperparameters . get ( value , value ) model . add ( layer [ 'class' ] ( * * layer_kwargs ) ) model . compile ( loss = self . loss , optimizer = self . optimizer ( ) , metrics = self . metrics ) return model\",\n",
       "  'score': '82.41178',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_6990',\n",
       "  'title': None,\n",
       "  'text': \"def get_model ( model_name : str , pretrained : bool , seq : bool = False , pname : str = 'imagenet' , * * kwargs ) : pretrained = pname if pretrained else None model = getattr ( pretrainedmodels , model_name ) ( pretrained = pretrained , * * kwargs ) return nn . Sequential ( * model . children ( ) ) if seq else model\",\n",
       "  'score': '82.37373',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_510897',\n",
       "  'title': None,\n",
       "  'text': 'def build_model ( self ) : model = Sequential ( ) model . add ( Dense ( 64 , input_shape = nn_input_dim_NN , init = \\'he_normal\\' ) ) model . add ( LeakyReLU ( alpha = .00001 ) ) model . add ( Dropout ( 0.5 ) ) model . add ( Dense ( 2 , init = \\'he_normal\\' ) ) model . add ( Activation ( \\'softmax\\' ) ) sgd = SGD ( lr = 0.1 , decay = 1e-5 , momentum = 0.9 , nesterov = True ) model . compile ( optimizer = sgd , loss = \\'binary_crossentropy\\' , metrics = [ \"accuracy\" ] ) return KerasClassifier ( nn = model , * * self . params )',\n",
       "  'score': '82.32927',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_38564',\n",
       "  'title': None,\n",
       "  'text': \"def get_config ( model ) : global args expreplay = ExpReplay ( predictor_io_names = ( [ 'state' ] , [ 'Qvalue' ] ) , get_player = lambda : get_player ( train = True ) , num_parallel_players = NUM_PARALLEL_PLAYERS , state_shape = model . state_shape , batch_size = BATCH_SIZE , memory_size = MEMORY_SIZE , init_memory_size = INIT_MEMORY_SIZE , update_frequency = UPDATE_FREQ , history_len = FRAME_HISTORY , state_dtype = model . state_dtype . as_numpy_dtype ) # Set to other values if you need a different initial exploration # (e.g., # if you're resuming a training half-way) # expreplay.exploration = 1.0 return TrainConfig ( data = QueueInput ( expreplay ) , model = model , callbacks = [ ModelSaver ( ) , PeriodicTrigger ( RunOp ( DQNModel . update_target_param , verbose = True ) , every_k_steps = 10000 // UPDATE_FREQ ) , # update target network every 10k steps expreplay , ScheduledHyperParamSetter ( 'learning_rate' , [ ( 0 , 1e-3 ) , ( 60 , 4e-4 ) , ( 100 , 2e-4 ) , ( 500 , 5e-5 ) ] ) , ScheduledHyperParamSetter ( ObjAttrParam ( expreplay , 'exploration' ) , [ ( 0 , 1 ) , ( 10 , 0.1 ) , ( 320 , 0.01 ) ] , # 1->0.1 in the first million steps interp = 'linear' ) , PeriodicTrigger ( Evaluator ( EVAL_EPISODE , [ 'state' ] , [ 'Qvalue' ] , get_player ) , every_k_epochs = 5 if 'pong' in args . env . lower ( ) else 10 ) , # eval more frequently for easy games HumanHyperParamSetter ( 'learning_rate' ) , ] , steps_per_epoch = STEPS_PER_EPOCH , max_epoch = 800 , )\",\n",
       "  'score': '82.327705',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_256765',\n",
       "  'title': None,\n",
       "  'text': \"def instantiate ( self , * * extra_args ) : input_block = self . input_block . instantiate ( ) backbone = self . backbone . instantiate ( * * extra_args ) return QModel ( input_block , backbone , extra_args [ 'action_space' ] )\",\n",
       "  'score': '82.313095',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_256894',\n",
       "  'title': None,\n",
       "  'text': \"def model_action_q_dist ( self ) : q = self . get ( 'model:q_dist' ) actions = self . get ( 'rollout:actions' ) return q [ range ( q . size ( 0 ) ) , actions ]\",\n",
       "  'score': '82.290695',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_143157',\n",
       "  'title': None,\n",
       "  'text': \"def run_bell_medium_level ( n_shots = 1000 ) : # Step 1. Get a device. Either a QVM or a QPU qc = get_qc ( '9q-generic-qvm' ) q = [ 4 , 5 ] # qubits # Step 2. Construct your program program = Program ( ) program += H ( q [ 0 ] ) program += CNOT ( q [ 0 ] , q [ 1 ] ) # Step 2.1. Manage read-out memory ro = program . declare ( 'ro' , memory_type = 'BIT' , memory_size = 2 ) program += MEASURE ( q [ 0 ] , ro [ 0 ] ) program += MEASURE ( q [ 1 ] , ro [ 1 ] ) # Step 2.2. Run the program in a loop program = program . wrap_in_numshots_loop ( n_shots ) # Step 3. Compile and run executable = qc . compile ( program ) bitstrings = qc . run ( executable ) # Bincount bitstrings basis = np . array ( [ 2 ** i for i in range ( len ( q ) ) ] ) ints = np . sum ( bitstrings * basis , axis = 1 ) print ( 'bincounts' , np . bincount ( ints ) ) # Check parity parities = np . sum ( bitstrings , axis = 1 ) % 2 print ( 'avg parity' , np . mean ( parities ) )\",\n",
       "  'score': '82.279205',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_122555',\n",
       "  'title': None,\n",
       "  'text': 'def fit_quadratic ( X , y ) : model = make_pipeline ( PolynomialFeatures ( 2 ) , linear_model . LinearRegression ( ) ) model . fit ( X , y ) return model',\n",
       "  'score': '82.197586',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_839451',\n",
       "  'title': None,\n",
       "  'text': \"def dqn_atari ( self : Experiment , logdir = None , env = 'Pong' , frameskip = 4 , timesteps = 100000000 , memory_size = 100000 , agents = 2 , replay_start = 50000 , tter = .25 , duelling = True ) : from tensorflow . contrib import layers import gym from gym import wrappers import numpy as np chi . set_loglevel ( 'debug' ) log_nvidia_smi ( logdir + '/logs/nvidia-smi' ) class RenderMeta ( chi . rl . Wrapper ) : def __init__ ( self , env , limits = None ) : super ( ) . __init__ ( env ) self . an = env . action_space . n # self.q_plotters = [Plotter(limits=None, title=f'A({n})') for n in action_names] self . f , ax = plt . subplots ( 2 , 2 , figsize = ( 2 * 3 , 2 * 2 ) , dpi = 64 ) self . f . set_tight_layout ( True ) ax = iter ( np . reshape ( ax , - 1 ) ) self . q = Plotter ( next ( ax ) , legend = [ str ( i ) for i in range ( self . an ) ] , title = 'Q - mean Q' ) self . qm = Plotter ( next ( ax ) , title = 'mean Q' ) self . r = Plotter ( next ( ax ) , legend = [ 'wrapped' , 'unwrapped' ] , title = 'reward' ) def _reset ( self ) : obs = super ( ) . _reset ( ) self . last_frame = np . tile ( obs [ : , : , - 1 : ] , ( 1 , 1 , 3 ) ) self . last_q = None return obs def _step ( self , action ) : ob , r , done , info = super ( ) . _step ( action ) self . last_frame = np . tile ( ob [ : , : , - 1 : ] , ( 1 , 1 , 3 ) ) qs = self . meta . get ( 'action_values' , np . full ( self . an , np . nan ) ) qm = np . mean ( qs ) qs -= qm # [qp.append(qs[i, ...]) for i, qp in enumerate(self.q_plotters)] self . qm . append ( qm ) self . q . append ( qs ) self . r . append ( ( r , info . get ( 'unwrapped_reward' , r ) , self . meta . get ( 'td' , np . nan ) ) ) return ob , r , done , info def _render ( self , mode = 'human' , close = False ) : f = super ( ) . _render ( mode , close ) # fs = [qp.draw() for qp in self.q_plotters] f2 = draw ( self . f ) return chi . rl . util . concat_frames ( f , self . last_frame , f2 ) def make_env ( i ) : e = env + 'NoFrameskip-v3' e = gym . make ( e ) e = chi . rl . wrappers . AtariWrapper ( e ) e = chi . rl . wrappers . StackFrames ( e , 4 ) e = chi . rl . wrappers . SkipWrapper ( e , 4 ) if i == 0 : e = RenderMeta ( e ) e = wrappers . Monitor ( e , self . logdir + '/monitor_' + str ( i ) , video_callable = lambda j : j % ( 20 if i == 0 else 200 ) == 0 if i < 4 else False ) return e envs = [ make_env ( i ) for i in range ( agents ) ] env = envs [ 0 ] print_env ( env ) if duelling : # https://arxiv.org/abs/1511.06581 @ chi . model ( tracker = tf . train . ExponentialMovingAverage ( 1 - .0005 ) , # TODO: replace with original weight freeze optimizer = tf . train . RMSPropOptimizer ( 6.25e-5 , .95 , .95 , .01 ) ) def q_network ( x ) : x /= 255 x = layers . conv2d ( x , 32 , 8 , 4 ) x = layers . conv2d ( x , 64 , 4 , 2 ) x = layers . conv2d ( x , 64 , 3 , 1 ) x = layers . flatten ( x ) xv = layers . fully_connected ( x , 512 ) val = layers . fully_connected ( xv , 1 , activation_fn = None ) # val = tf.squeeze(val, 1) xa = layers . fully_connected ( x , 512 ) adv = layers . fully_connected ( xa , env . action_space . n , activation_fn = None ) q = val + adv - tf . reduce_mean ( adv , axis = 1 , keep_dims = True ) q = tf . identity ( q , name = 'Q' ) return q else : @ chi . model ( tracker = tf . train . ExponentialMovingAverage ( 1 - .0005 ) , # TODO: replace with original weight freeze optimizer = tf . train . RMSPropOptimizer ( .00025 , .95 , .95 , .01 ) ) def q_network ( x ) : x /= 255 x = layers . conv2d ( x , 32 , 8 , 4 ) x = layers . conv2d ( x , 64 , 4 , 2 ) x = layers . conv2d ( x , 64 , 3 , 1 ) x = layers . flatten ( x ) x = layers . fully_connected ( x , 512 ) x = layers . fully_connected ( x , env . action_space . n , activation_fn = None ) x = tf . identity ( x , name = 'Q' ) return x dqn = DQN ( env . action_space . n , env . observation_space . shape , q_network , replay_start = replay_start , logdir = logdir ) for i , env in enumerate ( envs ) : agent = dqn . make_agent ( test = i in ( 0 , 1 ) , memory_size = memory_size // agents , logdir = logdir , name = f'Agent_{i}' ) agent . run ( env , async = True ) log_top ( logdir + '/logs/top' ) dqn . train ( timesteps , tter )\",\n",
       "  'score': '82.18376',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_9822',\n",
       "  'title': None,\n",
       "  'text': 'def test_build_model ( ) : vectors = np . ndarray ( ( 100 , 8 ) , dtype = \"float32\" ) shape = ( 10 , 16 , 3 ) settings = { \"lr\" : 0.001 , \"dropout\" : 0.2 , \"gru_encode\" : True , \"entail_dir\" : \"both\" } model = build_model ( vectors , shape , settings )',\n",
       "  'score': '82.15876',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_210896',\n",
       "  'title': None,\n",
       "  'text': \"def run ( self , q , q_direction = None , lang = 'C' ) : if self . _derivative_order is not None or lang != 'C' : self . _run_py ( q , q_direction = q_direction ) else : self . _run_c ( q , q_direction = q_direction )\",\n",
       "  'score': '82.137375',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_701998',\n",
       "  'title': None,\n",
       "  'text': 'def load ( self , strategy = None ) : if not strategy : self . _compile_strategy ( CONF . QUARK . default_net_strategy ) else : self . _compile_strategy ( strategy )',\n",
       "  'score': '82.12363',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_544132',\n",
       "  'title': None,\n",
       "  'text': \"def save_model ( self , path ) : with open ( path , 'w' ) as model : for ( x , word ) , p in self . q1 . items ( ) : model . write ( dumps ( [ 'Q1' , x , word , p ] ) + '\\\\n' ) for ( x , y1 , y2 ) , p in self . q2 . items ( ) : model . write ( dumps ( [ 'Q2' , x , y1 , y2 , p ] ) + '\\\\n' ) model . write ( dumps ( [ 'WORDS' , list ( self . well_known_words ) ] ) + '\\\\n' )\",\n",
       "  'score': '82.12347',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_14054',\n",
       "  'title': None,\n",
       "  'text': 'def learn ( network , env , seed = None , nsteps = 5 , total_timesteps = int ( 80e6 ) , vf_coef = 0.5 , ent_coef = 0.01 , max_grad_norm = 0.5 , lr = 7e-4 , lrschedule = \\'linear\\' , epsilon = 1e-5 , alpha = 0.99 , gamma = 0.99 , log_interval = 100 , load_path = None , * * network_kwargs ) : set_global_seeds ( seed ) # Get the nb of env nenvs = env . num_envs policy = build_policy ( env , network , * * network_kwargs ) # Instantiate the model object (that creates step_model and train_model) model = Model ( policy = policy , env = env , nsteps = nsteps , ent_coef = ent_coef , vf_coef = vf_coef , max_grad_norm = max_grad_norm , lr = lr , alpha = alpha , epsilon = epsilon , total_timesteps = total_timesteps , lrschedule = lrschedule ) if load_path is not None : model . load ( load_path ) # Instantiate the runner object runner = Runner ( env , model , nsteps = nsteps , gamma = gamma ) epinfobuf = deque ( maxlen = 100 ) # Calculate the batch_size nbatch = nenvs * nsteps # Start total timer tstart = time . time ( ) for update in range ( 1 , total_timesteps // nbatch + 1 ) : # Get mini batch of experiences obs , states , rewards , masks , actions , values , epinfos = runner . run ( ) epinfobuf . extend ( epinfos ) policy_loss , value_loss , policy_entropy = model . train ( obs , states , rewards , masks , actions , values ) nseconds = time . time ( ) - tstart # Calculate the fps (frame per second) fps = int ( ( update * nbatch ) / nseconds ) if update % log_interval == 0 or update == 1 : # Calculates if value function is a good predicator of the returns (ev > 1) # or if it\\'s just worse than predicting nothing (ev =< 0) ev = explained_variance ( values , rewards ) logger . record_tabular ( \"nupdates\" , update ) logger . record_tabular ( \"total_timesteps\" , update * nbatch ) logger . record_tabular ( \"fps\" , fps ) logger . record_tabular ( \"policy_entropy\" , float ( policy_entropy ) ) logger . record_tabular ( \"value_loss\" , float ( value_loss ) ) logger . record_tabular ( \"explained_variance\" , float ( ev ) ) logger . record_tabular ( \"eprewmean\" , safemean ( [ epinfo [ \\'r\\' ] for epinfo in epinfobuf ] ) ) logger . record_tabular ( \"eplenmean\" , safemean ( [ epinfo [ \\'l\\' ] for epinfo in epinfobuf ] ) ) logger . dump_tabular ( ) return model',\n",
       "  'score': '82.03446',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_6991',\n",
       "  'title': None,\n",
       "  'text': \"def inceptionv4 ( pretrained : bool = False ) : model = get_model ( 'inceptionv4' , pretrained ) all_layers = list ( model . children ( ) ) return nn . Sequential ( * all_layers [ 0 ] , * all_layers [ 1 : ] )\",\n",
       "  'score': '82.03433',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_189082',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , data , p , q , target = None ) : # Initialize TSM object super ( EGARCHM , self ) . __init__ ( \\'EGARCHM\\' ) # Latent variables self . p = p self . q = q self . z_no = self . p + self . q + 4 self . max_lag = max ( self . p , self . q ) self . leverage = False self . model_name = \"EGARCHM(\" + str ( self . p ) + \",\" + str ( self . q ) + \")\" self . _z_hide = 0 # Whether to cutoff latent variables from results table self . supported_methods = [ \"MLE\" , \"PML\" , \"Laplace\" , \"M-H\" , \"BBVI\" ] self . default_method = \"MLE\" self . multivariate_model = False # Format the data self . data , self . data_name , self . is_pandas , self . index = dc . data_check ( data , target ) self . data_length = self . data . shape [ 0 ] self . _create_latent_variables ( )',\n",
       "  'score': '81.99252',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_19672',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , env , policy , qfunc , strategy , ctx = mx . gpu ( 0 ) , batch_size = 32 , n_epochs = 1000 , epoch_length = 1000 , memory_size = 1000000 , memory_start_size = 1000 , discount = 0.99 , max_path_length = 1000 , eval_samples = 10000 , qfunc_updater = \"adam\" , qfunc_lr = 1e-4 , policy_updater = \"adam\" , policy_lr = 1e-4 , soft_target_tau = 1e-3 , n_updates_per_sample = 1 , include_horizon_terminal = False , seed = 12345 ) : mx . random . seed ( seed ) np . random . seed ( seed ) self . env = env self . ctx = ctx self . policy = policy self . qfunc = qfunc self . strategy = strategy self . batch_size = batch_size self . n_epochs = n_epochs self . epoch_length = epoch_length self . memory_size = memory_size self . memory_start_size = memory_start_size self . discount = discount self . max_path_length = max_path_length self . eval_samples = eval_samples self . qfunc_updater = qfunc_updater self . qfunc_lr = qfunc_lr self . policy_updater = policy_updater self . policy_lr = policy_lr self . soft_target_tau = soft_target_tau self . n_updates_per_sample = n_updates_per_sample self . include_horizon_terminal = include_horizon_terminal self . init_net ( ) # logging self . qfunc_loss_averages = [ ] self . policy_loss_averages = [ ] self . q_averages = [ ] self . y_averages = [ ] self . strategy_path_returns = [ ]',\n",
       "  'score': '81.97375',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_176578',\n",
       "  'title': None,\n",
       "  'text': \"def create_model ( model_name : Optional [ str ] , params : ModelParams ) -> 'Sequential' : if model_name and isfile ( model_name ) : print ( 'Loading from ' + model_name + '...' ) model = load_precise_model ( model_name ) else : from keras . layers . core import Dense from keras . layers . recurrent import GRU from keras . models import Sequential model = Sequential ( ) model . add ( GRU ( params . recurrent_units , activation = 'linear' , input_shape = ( pr . n_features , pr . feature_size ) , dropout = params . dropout , name = 'net' ) ) model . add ( Dense ( 1 , activation = 'sigmoid' ) ) load_keras ( ) metrics = [ 'accuracy' ] + params . extra_metrics * [ false_pos , false_neg ] set_loss_bias ( params . loss_bias ) for i in model . layers [ : params . freeze_till ] : i . trainable = False model . compile ( 'rmsprop' , weighted_log_loss , metrics = ( not params . skip_acc ) * metrics ) return model\",\n",
       "  'score': '81.95119',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_6809',\n",
       "  'title': None,\n",
       "  'text': 'def xresnet ( expansion , n_layers , name , pretrained = False , * * kwargs ) : model = XResNet ( expansion , n_layers , * * kwargs ) if pretrained : model . load_state_dict ( model_zoo . load_url ( model_urls [ name ] ) ) return model',\n",
       "  'score': '81.931305',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_666898',\n",
       "  'title': None,\n",
       "  'text': 'def create_keras_model ( self , input_tensor , training = False ) : with tf . variable_scope ( \"model\" ) : output_tensors , debug_tensors = self . __call__ ( input_tensor , training , for_tensorflow = False , for_keras = True ) output_tensors . update ( debug_tensors ) input_tensor_order = sorted ( list ( input_tensor . keys ( ) ) ) inputs = [ input_tensor [ k ] for k in input_tensor_order ] outputs = [ tf . keras . layers . Lambda ( lambda x : x , name = k ) ( output_tensors [ k ] ) for k in output_tensors ] model = tf . keras . Model ( inputs = inputs , outputs = outputs ) return model',\n",
       "  'score': '81.90555',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_531687',\n",
       "  'title': None,\n",
       "  'text': \"def _build_model ( trigram_count ) : # This resets all parameters and variables, leave this here tf . reset_default_graph ( ) net = tflearn . input_data ( [ None , trigram_count ] ) net = tflearn . fully_connected ( net , 200 , activation = 'ReLU' ) # Hidden net = tflearn . fully_connected ( net , 25 , activation = 'ReLU' ) # Hidden net = tflearn . fully_connected ( net , 2 , activation = 'softmax' ) # Output net = tflearn . regression ( net , optimizer = 'sgd' , learning_rate = 0.1 , loss = 'categorical_crossentropy' ) model = tflearn . DNN ( net ) return model\",\n",
       "  'score': '81.808174',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_44197',\n",
       "  'title': None,\n",
       "  'text': \"def _quantize_nn_spec ( spec , nbits , qm , * * kwargs ) : quantized_layers = [ 'convolution' , 'innerProduct' , 'embedding' , 'batchnorm' , 'scale' , 'bias' , 'loadConstant' , 'simpleRecurrent' , 'gru' , 'uniDirectionalLSTM' , 'biDirectionalLSTM' ] # Bump up to appropriate spec version if required if nbits is not None and nbits > 8 and nbits != 16 : raise Exception ( 'Only half precision (16-bit), 8-bit and lower ' 'quantization is supported' ) if nbits == 16 : spec . specificationVersion = max ( _MINIMUM_FP16_SPEC_VERSION , spec . specificationVersion ) else : spec . specificationVersion = max ( _MINIMUM_QUANTIZED_MODEL_SPEC_VERSION , spec . specificationVersion ) if qm not in _SUPPORTED_QUANTIZATION_MODES : raise Exception ( 'Quantization mode {} not supported' . format ( qm ) ) layers = _get_nn_layers ( spec ) # Perform optimization step if nbits is not None and nbits < 16 and qm != _QUANTIZATION_MODE_DEQUANTIZE : print ( 'Optimizing Neural Network before Quantization:' ) _optimize_nn ( layers ) print ( 'Finished optimizing network. Quantizing neural network..' ) for layer in layers : layer_type = layer . WhichOneof ( 'layer' ) if layer_type not in quantized_layers : continue print ( 'Quantizing layer {}' . format ( layer . name ) ) if layer_type == 'convolution' : output_channels = layer . convolution . outputChannels kernel_channels = layer . convolution . kernelChannels kernel_height = layer . convolution . kernelSize [ 0 ] kernel_width = layer . convolution . kernelSize [ 1 ] if layer . convolution . isDeconvolution : groups = layer . convolution . nGroups shape = ( kernel_channels , int ( output_channels / groups ) , kernel_height , kernel_width ) _quantize_wp_field ( layer . convolution . weights , nbits , qm , shape , axis = 1 , * * kwargs ) else : shape = ( output_channels , kernel_channels , kernel_height , kernel_width ) _quantize_wp_field ( layer . convolution . weights , nbits , qm , shape , * * kwargs ) if layer . convolution . hasBias : _quantize_wp_field ( layer . convolution . bias , nbits , qm , shape = ( output_channels , ) , * * kwargs ) # Batchnorm elif layer_type == 'batchnorm' : nw = layer . batchnorm . channels _quantize_wp_field ( layer . batchnorm . gamma , nbits , qm , shape = ( nw , ) , * * kwargs ) _quantize_wp_field ( layer . batchnorm . beta , nbits , qm , shape = ( nw , ) , * * kwargs ) _quantize_wp_field ( layer . batchnorm . mean , nbits , qm , shape = ( nw , ) , * * kwargs ) _quantize_wp_field ( layer . batchnorm . variance , nbits , qm , shape = ( nw , ) , * * kwargs ) # InnerProduct elif layer_type == 'innerProduct' : output_channels = layer . innerProduct . outputChannels input_channels = layer . innerProduct . inputChannels _quantize_wp_field ( layer . innerProduct . weights , nbits , qm , shape = ( output_channels , input_channels ) , * * kwargs ) if layer . innerProduct . hasBias : _quantize_wp_field ( layer . innerProduct . bias , nbits , qm , shape = ( output_channels , ) , * * kwargs ) # Embedding layer elif layer_type == 'embedding' : output_channels = layer . embedding . outputChannels input_channels = layer . embedding . inputDim _quantize_wp_field ( layer . embedding . weights , nbits , qm , shape = ( output_channels , input_channels ) , * * kwargs ) if layer . embedding . hasBias : _quantize_wp_field ( layer . embedding . bias , nbits , qm , shape = ( output_channels , ) , * * kwargs ) # Scale layer elif layer_type == 'scale' : nw = _np . prod ( layer . scale . shapeScale ) _quantize_wp_field ( layer . scale . scale , nbits , qm , shape = ( nw , ) , * * kwargs ) if layer . scale . hasBias : nw = _np . prod ( layer . scale . shapeBias ) _quantize_wp_field ( layer . scale . bias , nbits , qm , shape = ( nw , ) , * * kwargs ) # Bias layer elif layer_type == 'bias' : nw = _np . prod ( layer . bias . shape ) _quantize_wp_field ( layer . bias . bias , nbits , qm , shape = ( nw , ) , * * kwargs ) # LoadConstant layer elif layer_type == 'loadConstant' : nw = _np . prod ( layer . loadConstant . shape ) _quantize_wp_field ( layer . loadConstant . data , nbits , qm , shape = ( nw , ) , * * kwargs ) # Simple Recurrent elif layer_type == 'simpleRecurrent' : i_size = layer . simpleRecurrent . inputVectorSize o_size = layer . simpleRecurrent . outputVectorSize _quantize_wp_field ( layer . simpleRecurrent . weightMatrix , nbits , qm , shape = ( o_size , i_size ) , * * kwargs ) _quantize_wp_field ( layer . simpleRecurrent . recursionMatrix , nbits , qm , shape = ( o_size , o_size ) , * * kwargs ) if layer . simpleRecurrent . hasBiasVector : _quantize_wp_field ( layer . simpleRecurrent . biasVector , nbits , qm , shape = ( o_size , ) , * * kwargs ) # GRU elif layer_type == 'gru' : i_size = layer . gru . inputVectorSize o_size = layer . gru . outputVectorSize # Weight Matrix _quantize_wp_field ( layer . gru . updateGateWeightMatrix , nbits , qm , shape = ( o_size , i_size ) , * * kwargs ) _quantize_wp_field ( layer . gru . resetGateWeightMatrix , nbits , qm , shape = ( o_size , i_size ) , * * kwargs ) _quantize_wp_field ( layer . gru . outputGateWeightMatrix , nbits , qm , shape = ( o_size , i_size ) , * * kwargs ) # Recursion Weights _quantize_wp_field ( layer . gru . updateGateRecursionMatrix , nbits , qm , shape = ( o_size , o_size ) , * * kwargs ) _quantize_wp_field ( layer . gru . resetGateRecursionMatrix , nbits , qm , shape = ( o_size , o_size ) , * * kwargs ) _quantize_wp_field ( layer . gru . outputGateRecursionMatrix , nbits , qm , shape = ( o_size , o_size ) , * * kwargs ) # Bias if layer . gru . hasBiasVectors : _quantize_wp_field ( layer . gru . updateGateBiasVector , nbits , qm , shape = ( o_size , ) , * * kwargs ) _quantize_wp_field ( layer . gru . resetGateBiasVector , nbits , qm , shape = ( o_size , ) , * * kwargs ) _quantize_wp_field ( layer . gru . outputGateBiasVector , nbits , qm , shape = ( o_size , ) , * * kwargs ) # LSTM Layers elif layer_type in [ 'uniDirectionalLSTM' , 'biDirectionalLSTM' ] : def _lstmwp_to_fp16_lstmwp ( lstm_wp , nbits , qm , i_size , o_size , has_peephole = True ) : assert lstm_wp _quantize_wp_field ( lstm_wp . inputGateWeightMatrix , nbits , qm , shape = ( o_size , i_size ) , * * kwargs ) _quantize_wp_field ( lstm_wp . forgetGateWeightMatrix , nbits , qm , shape = ( o_size , i_size ) , * * kwargs ) _quantize_wp_field ( lstm_wp . blockInputWeightMatrix , nbits , qm , shape = ( o_size , i_size ) , * * kwargs ) _quantize_wp_field ( lstm_wp . outputGateWeightMatrix , nbits , qm , shape = ( o_size , i_size ) , * * kwargs ) _quantize_wp_field ( lstm_wp . inputGateRecursionMatrix , nbits , qm , shape = ( o_size , o_size ) , * * kwargs ) _quantize_wp_field ( lstm_wp . forgetGateRecursionMatrix , nbits , qm , shape = ( o_size , o_size ) , * * kwargs ) _quantize_wp_field ( lstm_wp . blockInputRecursionMatrix , nbits , qm , shape = ( o_size , o_size ) , * * kwargs ) _quantize_wp_field ( lstm_wp . outputGateRecursionMatrix , nbits , qm , shape = ( o_size , o_size ) , * * kwargs ) _quantize_wp_field ( lstm_wp . inputGateBiasVector , nbits , qm , shape = ( o_size , ) , * * kwargs ) _quantize_wp_field ( lstm_wp . forgetGateBiasVector , nbits , qm , shape = ( o_size , ) , * * kwargs ) _quantize_wp_field ( lstm_wp . blockInputBiasVector , nbits , qm , shape = ( o_size , ) , * * kwargs ) _quantize_wp_field ( lstm_wp . outputGateBiasVector , nbits , qm , shape = ( o_size , ) , * * kwargs ) if has_peephole : _quantize_wp_field ( lstm_wp . inputGatePeepholeVector , nbits , qm , shape = ( o_size , ) , * * kwargs ) _quantize_wp_field ( lstm_wp . forgetGatePeepholeVector , nbits , qm , shape = ( o_size , ) , * * kwargs ) _quantize_wp_field ( lstm_wp . outputGatePeepholeVector , nbits , qm , shape = ( o_size , ) , * * kwargs ) if layer_type == 'uniDirectionalLSTM' : _lstmwp_to_fp16_lstmwp ( lstm_wp = layer . uniDirectionalLSTM . weightParams , nbits = nbits , qm = qm , i_size = layer . uniDirectionalLSTM . inputVectorSize , o_size = layer . uniDirectionalLSTM . outputVectorSize , has_peephole = layer . uniDirectionalLSTM . params . hasPeepholeVectors ) elif layer_type == 'biDirectionalLSTM' : for lstm_wp in layer . biDirectionalLSTM . weightParams : _lstmwp_to_fp16_lstmwp ( lstm_wp = lstm_wp , nbits = nbits , qm = qm , i_size = layer . biDirectionalLSTM . inputVectorSize , o_size = layer . biDirectionalLSTM . outputVectorSize , has_peephole = layer . biDirectionalLSTM . params . hasPeepholeVectors ) elif layer_type == 'custom' : print ( 'Skipping custom layer {}. Weights for this layer need to' 'be converted manually' . format ( layer . name ) ) elif layer_type in quantized_layers : raise Exception ( 'Quantization for ' + layer_type + ' not yet implemented\\\\n' ) else : raise Exception ( 'Unknown layer ' + layer_type + ' to be quantized' ) return spec\",\n",
       "  'score': '81.79698',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_1083248',\n",
       "  'title': None,\n",
       "  'text': 'def build_model ( self ) : if self . model_config [ \\'model-type\\' ] : return self . build_red ( ) elif self . model_config [ \\'model-type\\' ] : return self . buidl_hred ( ) else : raise Error ( \"Unrecognized model type \\'{}\\'\" . format ( self . model_config [ \\'model-type\\' ] ) )',\n",
       "  'score': '81.78693',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_1005446',\n",
       "  'title': None,\n",
       "  'text': 'def run ( self ) : print ( \"Starting \" + self . name ) process_data ( self . name , self . q , self . result_q , self . einstein , self . model_id , self . the_lock ) print ( \"Exiting \" + self . name )',\n",
       "  'score': '81.7813',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_256845',\n",
       "  'title': None,\n",
       "  'text': \"def run ( self ) : device = self . model_config . torch_device ( ) env = self . vec_env_factory . instantiate_single ( preset = 'record' , seed = self . model_config . seed ) model = self . model_factory . instantiate ( action_space = env . action_space ) . to ( device ) training_info = TrainingInfo ( start_epoch_idx = self . storage . last_epoch_idx ( ) , run_name = self . model_config . run_name ) self . storage . load ( training_info , model ) model . eval ( ) self . run_model ( model , env , device )\",\n",
       "  'score': '81.77353',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_650505',\n",
       "  'title': None,\n",
       "  'text': 'def to_bqm ( self , model ) : linear = ( ( v , float ( model . get_py_value ( bias ) ) ) for v , bias in self . linear . items ( ) ) quadratic = ( ( u , v , float ( model . get_py_value ( bias ) ) ) for ( u , v ) , bias in self . quadratic . items ( ) ) offset = float ( model . get_py_value ( self . offset ) ) return dimod . BinaryQuadraticModel ( linear , quadratic , offset , dimod . SPIN )',\n",
       "  'score': '81.76727',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_597306',\n",
       "  'title': None,\n",
       "  'text': \"def get_model ( input_dim ) : model = Sequential ( ) model . add ( Dense ( 100 , input_dim = input_dim , activation = 'sigmoid' ) ) model . add ( Dense ( 1 ) ) model . compile ( loss = 'mean_squared_error' , optimizer = 'SGD' ) return model\",\n",
       "  'score': '81.76564',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_137936',\n",
       "  'title': None,\n",
       "  'text': \"def as_local_model ( self ) : super ( QModel , self ) . as_local_model ( ) self . target_optimizer_spec = dict ( type = 'global_optimizer' , optimizer = self . target_optimizer_spec )\",\n",
       "  'score': '81.74795',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_137940',\n",
       "  'title': None,\n",
       "  'text': 'def tf_loss_per_instance ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : embedding = self . network . apply ( x = states , internals = internals , update = update ) # fix if self . double_q_model : next_embedding = self . network . apply ( x = next_states , internals = next_internals , update = update ) # Both networks can use the same internals, could that be a problem? # Otherwise need to handle internals indices correctly everywhere target_embedding = self . target_network . apply ( x = next_states , internals = next_internals , update = update ) deltas = list ( ) for name in sorted ( self . distributions ) : distribution = self . distributions [ name ] target_distribution = self . target_distributions [ name ] distr_params = distribution . parameterize ( x = embedding ) target_distr_params = target_distribution . parameterize ( x = target_embedding ) q_value = self . tf_q_value ( embedding = embedding , distr_params = distr_params , action = actions [ name ] , name = name ) if self . double_q_model : # fix next_distr_params = distribution . parameterize ( x = next_embedding ) action_taken = distribution . sample ( distr_params = next_distr_params , deterministic = True ) else : action_taken = target_distribution . sample ( distr_params = target_distr_params , deterministic = True ) next_q_value = target_distribution . state_action_value ( distr_params = target_distr_params , action = action_taken ) delta = self . tf_q_delta ( q_value = q_value , next_q_value = next_q_value , terminal = terminal , reward = reward ) collapsed_size = util . prod ( util . shape ( delta ) [ 1 : ] ) delta = tf . reshape ( tensor = delta , shape = ( - 1 , collapsed_size ) ) deltas . append ( delta ) # Surrogate loss as the mean squared error between actual observed rewards and expected rewards loss_per_instance = tf . reduce_mean ( input_tensor = tf . concat ( values = deltas , axis = 1 ) , axis = 1 ) # Optional Huber loss if self . huber_loss is not None and self . huber_loss > 0.0 : loss = tf . where ( condition = ( tf . abs ( x = loss_per_instance ) <= self . huber_loss ) , x = ( 0.5 * tf . square ( x = loss_per_instance ) ) , y = ( self . huber_loss * ( tf . abs ( x = loss_per_instance ) - 0.5 * self . huber_loss ) ) ) else : loss = tf . square ( x = loss_per_instance ) return loss',\n",
       "  'score': '81.71629',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_29269',\n",
       "  'title': None,\n",
       "  'text': 'def main ( ) : global args , best_prec1 args = parser . parse_args ( ) # create model print ( \"=> creating model \\'{}\\'\" . format ( args . arch ) ) if args . pretrained . lower ( ) not in [ \\'false\\' , \\'none\\' , \\'not\\' , \\'no\\' , \\'0\\' ] : print ( \"=> using pre-trained parameters \\'{}\\'\" . format ( args . pretrained ) ) model = pretrainedmodels . __dict__ [ args . arch ] ( num_classes = 1000 , pretrained = args . pretrained ) else : model = pretrainedmodels . __dict__ [ args . arch ] ( ) # optionally resume from a checkpoint if args . resume : if os . path . isfile ( args . resume ) : print ( \"=> loading checkpoint \\'{}\\'\" . format ( args . resume ) ) checkpoint = torch . load ( args . resume ) args . start_epoch = checkpoint [ \\'epoch\\' ] best_prec1 = checkpoint [ \\'best_prec1\\' ] model . load_state_dict ( checkpoint [ \\'state_dict\\' ] ) print ( \"=> loaded checkpoint \\'{}\\' (epoch {})\" . format ( args . resume , checkpoint [ \\'epoch\\' ] ) ) else : print ( \"=> no checkpoint found at \\'{}\\'\" . format ( args . resume ) ) cudnn . benchmark = True # Data loading code # traindir = os.path.join(args.data, \\'train\\') valdir = os . path . join ( args . data , \\'val\\' ) # train_loader = torch.utils.data.DataLoader( #     datasets.ImageFolder(traindir, transforms.Compose([ #         transforms.RandomSizedCrop(max(model.input_size)), #         transforms.RandomHorizontalFlip(), #         transforms.ToTensor(), #         normalize, #     ])), #     batch_size=args.batch_size, shuffle=True, #     num_workers=args.workers, pin_memory=True) # if \\'scale\\' in pretrainedmodels.pretrained_settings[args.arch][args.pretrained]: #     scale = pretrainedmodels.pretrained_settings[args.arch][args.pretrained][\\'scale\\'] # else: #     scale = 0.875 scale = 0.875 print ( \\'Images transformed from size {} to {}\\' . format ( int ( round ( max ( model . input_size ) / scale ) ) , model . input_size ) ) val_tf = pretrainedmodels . utils . TransformImage ( model , scale = scale , preserve_aspect_ratio = args . preserve_aspect_ratio ) val_loader = torch . utils . data . DataLoader ( datasets . ImageFolder ( valdir , val_tf ) , batch_size = args . batch_size , shuffle = False , num_workers = args . workers , pin_memory = True ) # define loss function (criterion) and optimizer criterion = nn . CrossEntropyLoss ( ) . cuda ( ) optimizer = torch . optim . SGD ( model . parameters ( ) , args . lr , momentum = args . momentum , weight_decay = args . weight_decay ) model = torch . nn . DataParallel ( model ) . cuda ( ) if args . evaluate : validate ( val_loader , model , criterion ) return for epoch in range ( args . start_epoch , args . epochs ) : adjust_learning_rate ( optimizer , epoch ) # train for one epoch train ( train_loader , model , criterion , optimizer , epoch ) # evaluate on validation set prec1 = validate ( val_loader , model , criterion ) # remember best prec@1 and save checkpoint is_best = prec1 > best_prec1 best_prec1 = max ( prec1 , best_prec1 ) save_checkpoint ( { \\'epoch\\' : epoch + 1 , \\'arch\\' : args . arch , \\'state_dict\\' : model . state_dict ( ) , \\'best_prec1\\' : best_prec1 , } , is_best )',\n",
       "  'score': '81.67842',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_7049',\n",
       "  'title': None,\n",
       "  'text': 'def presnet ( block , n_layers , name , pre = False , * * kwargs ) : model = PResNet ( block , n_layers , * * kwargs ) #if pre: model.load_state_dict(model_zoo.load_url(model_urls[name])) if pre : model . load_state_dict ( torch . load ( model_urls [ name ] ) ) return model',\n",
       "  'score': '81.67412',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_256798',\n",
       "  'title': None,\n",
       "  'text': 'def create ( input_width , input_height , input_channels = 1 ) : def instantiate ( * * _ ) : return DoubleNatureCnn ( input_width = input_width , input_height = input_height , input_channels = input_channels ) return ModelFactory . generic ( instantiate )',\n",
       "  'score': '81.6682',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_14123',\n",
       "  'title': None,\n",
       "  'text': 'def learn ( env , policy_fn , * , timesteps_per_actorbatch , # timesteps per actor per update clip_param , entcoeff , # clipping parameter epsilon, entropy coeff optim_epochs , optim_stepsize , optim_batchsize , # optimization hypers gamma , lam , # advantage estimation max_timesteps = 0 , max_episodes = 0 , max_iters = 0 , max_seconds = 0 , # time constraint callback = None , # you can do anything in the callback, since it takes locals(), globals() adam_epsilon = 1e-5 , schedule = \\'constant\\' # annealing for stepsize parameters (epsilon and adam) ) : # Setup losses and stuff # ---------------------------------------- ob_space = env . observation_space ac_space = env . action_space pi = policy_fn ( \"pi\" , ob_space , ac_space ) # Construct network for new policy oldpi = policy_fn ( \"oldpi\" , ob_space , ac_space ) # Network for old policy atarg = tf . placeholder ( dtype = tf . float32 , shape = [ None ] ) # Target advantage function (if applicable) ret = tf . placeholder ( dtype = tf . float32 , shape = [ None ] ) # Empirical return lrmult = tf . placeholder ( name = \\'lrmult\\' , dtype = tf . float32 , shape = [ ] ) # learning rate multiplier, updated with schedule ob = U . get_placeholder_cached ( name = \"ob\" ) ac = pi . pdtype . sample_placeholder ( [ None ] ) kloldnew = oldpi . pd . kl ( pi . pd ) ent = pi . pd . entropy ( ) meankl = tf . reduce_mean ( kloldnew ) meanent = tf . reduce_mean ( ent ) pol_entpen = ( - entcoeff ) * meanent ratio = tf . exp ( pi . pd . logp ( ac ) - oldpi . pd . logp ( ac ) ) # pnew / pold surr1 = ratio * atarg # surrogate from conservative policy iteration surr2 = tf . clip_by_value ( ratio , 1.0 - clip_param , 1.0 + clip_param ) * atarg # pol_surr = - tf . reduce_mean ( tf . minimum ( surr1 , surr2 ) ) # PPO\\'s pessimistic surrogate (L^CLIP) vf_loss = tf . reduce_mean ( tf . square ( pi . vpred - ret ) ) total_loss = pol_surr + pol_entpen + vf_loss losses = [ pol_surr , pol_entpen , vf_loss , meankl , meanent ] loss_names = [ \"pol_surr\" , \"pol_entpen\" , \"vf_loss\" , \"kl\" , \"ent\" ] var_list = pi . get_trainable_variables ( ) lossandgrad = U . function ( [ ob , ac , atarg , ret , lrmult ] , losses + [ U . flatgrad ( total_loss , var_list ) ] ) adam = MpiAdam ( var_list , epsilon = adam_epsilon ) assign_old_eq_new = U . function ( [ ] , [ ] , updates = [ tf . assign ( oldv , newv ) for ( oldv , newv ) in zipsame ( oldpi . get_variables ( ) , pi . get_variables ( ) ) ] ) compute_losses = U . function ( [ ob , ac , atarg , ret , lrmult ] , losses ) U . initialize ( ) adam . sync ( ) # Prepare for rollouts # ---------------------------------------- seg_gen = traj_segment_generator ( pi , env , timesteps_per_actorbatch , stochastic = True ) episodes_so_far = 0 timesteps_so_far = 0 iters_so_far = 0 tstart = time . time ( ) lenbuffer = deque ( maxlen = 100 ) # rolling buffer for episode lengths rewbuffer = deque ( maxlen = 100 ) # rolling buffer for episode rewards assert sum ( [ max_iters > 0 , max_timesteps > 0 , max_episodes > 0 , max_seconds > 0 ] ) == 1 , \"Only one time constraint permitted\" while True : if callback : callback ( locals ( ) , globals ( ) ) if max_timesteps and timesteps_so_far >= max_timesteps : break elif max_episodes and episodes_so_far >= max_episodes : break elif max_iters and iters_so_far >= max_iters : break elif max_seconds and time . time ( ) - tstart >= max_seconds : break if schedule == \\'constant\\' : cur_lrmult = 1.0 elif schedule == \\'linear\\' : cur_lrmult = max ( 1.0 - float ( timesteps_so_far ) / max_timesteps , 0 ) else : raise NotImplementedError logger . log ( \"********** Iteration %i ************\" % iters_so_far ) seg = seg_gen . __next__ ( ) add_vtarg_and_adv ( seg , gamma , lam ) # ob, ac, atarg, ret, td1ret = map(np.concatenate, (obs, acs, atargs, rets, td1rets)) ob , ac , atarg , tdlamret = seg [ \"ob\" ] , seg [ \"ac\" ] , seg [ \"adv\" ] , seg [ \"tdlamret\" ] vpredbefore = seg [ \"vpred\" ] # predicted value function before udpate atarg = ( atarg - atarg . mean ( ) ) / atarg . std ( ) # standardized advantage function estimate d = Dataset ( dict ( ob = ob , ac = ac , atarg = atarg , vtarg = tdlamret ) , deterministic = pi . recurrent ) optim_batchsize = optim_batchsize or ob . shape [ 0 ] if hasattr ( pi , \"ob_rms\" ) : pi . ob_rms . update ( ob ) # update running mean/std for policy assign_old_eq_new ( ) # set old parameter values to new parameter values logger . log ( \"Optimizing...\" ) logger . log ( fmt_row ( 13 , loss_names ) ) # Here we do a bunch of optimization epochs over the data for _ in range ( optim_epochs ) : losses = [ ] # list of tuples, each of which gives the loss for a minibatch for batch in d . iterate_once ( optim_batchsize ) : * newlosses , g = lossandgrad ( batch [ \"ob\" ] , batch [ \"ac\" ] , batch [ \"atarg\" ] , batch [ \"vtarg\" ] , cur_lrmult ) adam . update ( g , optim_stepsize * cur_lrmult ) losses . append ( newlosses ) logger . log ( fmt_row ( 13 , np . mean ( losses , axis = 0 ) ) ) logger . log ( \"Evaluating losses...\" ) losses = [ ] for batch in d . iterate_once ( optim_batchsize ) : newlosses = compute_losses ( batch [ \"ob\" ] , batch [ \"ac\" ] , batch [ \"atarg\" ] , batch [ \"vtarg\" ] , cur_lrmult ) losses . append ( newlosses ) meanlosses , _ , _ = mpi_moments ( losses , axis = 0 ) logger . log ( fmt_row ( 13 , meanlosses ) ) for ( lossval , name ) in zipsame ( meanlosses , loss_names ) : logger . record_tabular ( \"loss_\" + name , lossval ) logger . record_tabular ( \"ev_tdlam_before\" , explained_variance ( vpredbefore , tdlamret ) ) lrlocal = ( seg [ \"ep_lens\" ] , seg [ \"ep_rets\" ] ) # local values listoflrpairs = MPI . COMM_WORLD . allgather ( lrlocal ) # list of tuples lens , rews = map ( flatten_lists , zip ( * listoflrpairs ) ) lenbuffer . extend ( lens ) rewbuffer . extend ( rews ) logger . record_tabular ( \"EpLenMean\" , np . mean ( lenbuffer ) ) logger . record_tabular ( \"EpRewMean\" , np . mean ( rewbuffer ) ) logger . record_tabular ( \"EpThisIter\" , len ( lens ) ) episodes_so_far += len ( lens ) timesteps_so_far += sum ( lens ) iters_so_far += 1 logger . record_tabular ( \"EpisodesSoFar\" , episodes_so_far ) logger . record_tabular ( \"TimestepsSoFar\" , timesteps_so_far ) logger . record_tabular ( \"TimeElapsed\" , time . time ( ) - tstart ) if MPI . COMM_WORLD . Get_rank ( ) == 0 : logger . dump_tabular ( ) return pi',\n",
       "  'score': '81.667274',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_502012',\n",
       "  'title': None,\n",
       "  'text': 'def sample ( self , bqm , chain_strength = 1.0 , chain_break_fraction = True , * * parameters ) : # solve the problem on the child system child = self . child # apply the embedding to the given problem to map it to the child sampler __ , target_edgelist , target_adjacency = child . structure # add self-loops to edgelist to handle singleton variables source_edgelist = list ( bqm . quadratic ) + [ ( v , v ) for v in bqm . linear ] # get the embedding embedding = minorminer . find_embedding ( source_edgelist , target_edgelist ) if bqm and not embedding : raise ValueError ( \"no embedding found\" ) bqm_embedded = embed_bqm ( bqm , embedding , target_adjacency , chain_strength = chain_strength , smear_vartype = dimod . SPIN ) if \\'initial_state\\' in parameters : parameters [ \\'initial_state\\' ] = _embed_state ( embedding , parameters [ \\'initial_state\\' ] ) response = child . sample ( bqm_embedded , * * parameters ) return unembed_sampleset ( response , embedding , source_bqm = bqm , chain_break_fraction = chain_break_fraction )',\n",
       "  'score': '81.60599',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_42610',\n",
       "  'title': None,\n",
       "  'text': 'def parse_args ( ) : parser = argparse . ArgumentParser ( \"Run an already learned DQN model.\" ) # Environment parser . add_argument ( \"--env\" , type = str , required = True , help = \"name of the game\" ) parser . add_argument ( \"--model-dir\" , type = str , default = None , help = \"load model from this directory. \" ) parser . add_argument ( \"--video\" , type = str , default = None , help = \"Path to mp4 file where the \\\\\\n                        video of first episode will be recorded.\" ) boolean_flag ( parser , \"stochastic\" , default = True , help = \"whether or not to use stochastic \\\\\\n                 actions according to models eps value\" ) boolean_flag ( parser , \"dueling\" , default = False , help = \"whether or not to use dueling model\" ) # V: Attack Arguments# parser . add_argument ( \"--model-dir2\" , type = str , default = None , help = \"load adversarial model from \\\\\\n                        this directory (blackbox attacks). \" ) parser . add_argument ( \"--attack\" , type = str , default = None , help = \"Method to attack the model.\" ) boolean_flag ( parser , \"noisy\" , default = False , help = \"whether or not to NoisyNetwork\" ) boolean_flag ( parser , \"noisy2\" , default = False , help = \"whether or not to NoisyNetwork\" ) boolean_flag ( parser , \"blackbox\" , default = False , help = \"whether or not to NoisyNetwork\" ) return parser . parse_args ( )',\n",
       "  'score': '81.60332',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_125930',\n",
       "  'title': None,\n",
       "  'text': 'def compile ( self , session = None ) : if not self . num_data == self . X . shape [ 0 ] : self . num_data = self . X . shape [ 0 ] self . q_mu = Parameter ( np . zeros ( ( self . num_data , self . num_latent ) ) ) self . q_sqrt = Parameter ( np . eye ( self . num_data ) [ : , : , None ] * np . ones ( ( 1 , 1 , self . num_latent ) ) ) return super ( VGP , self ) . compile ( session = session )',\n",
       "  'score': '81.598175',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_103315',\n",
       "  'title': None,\n",
       "  'text': 'def main ( env , args ) : model = Policy ( ) optimizer = optim . Adam ( model . parameters ( ) , lr = 1e-2 ) eps = np . finfo ( np . float32 ) . eps . item ( ) timesteps = list ( range ( 10000 ) ) def run_single_timestep ( engine , timestep ) : observation = engine . state . observation action = select_action ( model , observation ) engine . state . observation , reward , done , _ = env . step ( action ) if args . render : env . render ( ) model . rewards . append ( reward ) if done : engine . terminate_epoch ( ) engine . state . timestep = timestep trainer = Engine ( run_single_timestep ) @ trainer . on ( Events . STARTED ) def initialize ( engine ) : engine . state . running_reward = 10 @ trainer . on ( EPISODE_STARTED ) def reset_environment_state ( engine ) : engine . state . observation = env . reset ( ) @ trainer . on ( EPISODE_COMPLETED ) def update_model ( engine ) : t = engine . state . timestep engine . state . running_reward = engine . state . running_reward * 0.99 + t * 0.01 finish_episode ( model , optimizer , args . gamma , eps ) @ trainer . on ( EPISODE_COMPLETED ) def log_episode ( engine ) : i_episode = engine . state . epoch if i_episode % args . log_interval == 0 : print ( \\'Episode {}\\\\tLast length: {:5d}\\\\tAverage length: {:.2f}\\' . format ( i_episode , engine . state . timestep , engine . state . running_reward ) ) @ trainer . on ( EPISODE_COMPLETED ) def should_finish_training ( engine ) : running_reward = engine . state . running_reward if running_reward > env . spec . reward_threshold : print ( \"Solved! Running reward is now {} and \" \"the last episode runs to {} time steps!\" . format ( running_reward , engine . state . timestep ) ) engine . should_terminate = True trainer . run ( timesteps , max_epochs = args . max_episodes )',\n",
       "  'score': '81.536064',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_23791',\n",
       "  'title': None,\n",
       "  'text': 'def forward ( self , agent_qs , batch ) : return th . sum ( agent_qs , dim = 2 , keepdim = True )',\n",
       "  'score': '81.53171',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_176699',\n",
       "  'title': None,\n",
       "  'text': \"def __init__ ( self , model_name : str ) : import os os . environ [ 'TF_CPP_MIN_LOG_LEVEL' ] = '3' import tensorflow as tf self . model = load_precise_model ( model_name ) self . graph = tf . get_default_graph ( )\",\n",
       "  'score': '81.5276',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_13886',\n",
       "  'title': None,\n",
       "  'text': 'def learn ( * , network , env , total_timesteps , eval_env = None , seed = None , nsteps = 2048 , ent_coef = 0.0 , lr = 3e-4 , vf_coef = 0.5 , max_grad_norm = 0.5 , gamma = 0.99 , lam = 0.95 , log_interval = 10 , nminibatches = 4 , noptepochs = 4 , cliprange = 0.2 , save_interval = 0 , load_path = None , model_fn = None , * * network_kwargs ) : set_global_seeds ( seed ) if isinstance ( lr , float ) : lr = constfn ( lr ) else : assert callable ( lr ) if isinstance ( cliprange , float ) : cliprange = constfn ( cliprange ) else : assert callable ( cliprange ) total_timesteps = int ( total_timesteps ) policy = build_policy ( env , network , * * network_kwargs ) # Get the nb of env nenvs = env . num_envs # Get state_space and action_space ob_space = env . observation_space ac_space = env . action_space # Calculate the batch_size nbatch = nenvs * nsteps nbatch_train = nbatch // nminibatches # Instantiate the model object (that creates act_model and train_model) if model_fn is None : from baselines . ppo2 . model import Model model_fn = Model model = model_fn ( policy = policy , ob_space = ob_space , ac_space = ac_space , nbatch_act = nenvs , nbatch_train = nbatch_train , nsteps = nsteps , ent_coef = ent_coef , vf_coef = vf_coef , max_grad_norm = max_grad_norm ) if load_path is not None : model . load ( load_path ) # Instantiate the runner object runner = Runner ( env = env , model = model , nsteps = nsteps , gamma = gamma , lam = lam ) if eval_env is not None : eval_runner = Runner ( env = eval_env , model = model , nsteps = nsteps , gamma = gamma , lam = lam ) epinfobuf = deque ( maxlen = 100 ) if eval_env is not None : eval_epinfobuf = deque ( maxlen = 100 ) # Start total timer tfirststart = time . perf_counter ( ) nupdates = total_timesteps // nbatch for update in range ( 1 , nupdates + 1 ) : assert nbatch % nminibatches == 0 # Start timer tstart = time . perf_counter ( ) frac = 1.0 - ( update - 1.0 ) / nupdates # Calculate the learning rate lrnow = lr ( frac ) # Calculate the cliprange cliprangenow = cliprange ( frac ) # Get minibatch obs , returns , masks , actions , values , neglogpacs , states , epinfos = runner . run ( ) #pylint: disable=E0632 if eval_env is not None : eval_obs , eval_returns , eval_masks , eval_actions , eval_values , eval_neglogpacs , eval_states , eval_epinfos = eval_runner . run ( ) #pylint: disable=E0632 epinfobuf . extend ( epinfos ) if eval_env is not None : eval_epinfobuf . extend ( eval_epinfos ) # Here what we\\'re going to do is for each minibatch calculate the loss and append it. mblossvals = [ ] if states is None : # nonrecurrent version # Index of each element of batch_size # Create the indices array inds = np . arange ( nbatch ) for _ in range ( noptepochs ) : # Randomize the indexes np . random . shuffle ( inds ) # 0 to batch_size with batch_train_size step for start in range ( 0 , nbatch , nbatch_train ) : end = start + nbatch_train mbinds = inds [ start : end ] slices = ( arr [ mbinds ] for arr in ( obs , returns , masks , actions , values , neglogpacs ) ) mblossvals . append ( model . train ( lrnow , cliprangenow , * slices ) ) else : # recurrent version assert nenvs % nminibatches == 0 envsperbatch = nenvs // nminibatches envinds = np . arange ( nenvs ) flatinds = np . arange ( nenvs * nsteps ) . reshape ( nenvs , nsteps ) for _ in range ( noptepochs ) : np . random . shuffle ( envinds ) for start in range ( 0 , nenvs , envsperbatch ) : end = start + envsperbatch mbenvinds = envinds [ start : end ] mbflatinds = flatinds [ mbenvinds ] . ravel ( ) slices = ( arr [ mbflatinds ] for arr in ( obs , returns , masks , actions , values , neglogpacs ) ) mbstates = states [ mbenvinds ] mblossvals . append ( model . train ( lrnow , cliprangenow , * slices , mbstates ) ) # Feedforward --> get losses --> update lossvals = np . mean ( mblossvals , axis = 0 ) # End timer tnow = time . perf_counter ( ) # Calculate the fps (frame per second) fps = int ( nbatch / ( tnow - tstart ) ) if update % log_interval == 0 or update == 1 : # Calculates if value function is a good predicator of the returns (ev > 1) # or if it\\'s just worse than predicting nothing (ev =< 0) ev = explained_variance ( values , returns ) logger . logkv ( \"serial_timesteps\" , update * nsteps ) logger . logkv ( \"nupdates\" , update ) logger . logkv ( \"total_timesteps\" , update * nbatch ) logger . logkv ( \"fps\" , fps ) logger . logkv ( \"explained_variance\" , float ( ev ) ) logger . logkv ( \\'eprewmean\\' , safemean ( [ epinfo [ \\'r\\' ] for epinfo in epinfobuf ] ) ) logger . logkv ( \\'eplenmean\\' , safemean ( [ epinfo [ \\'l\\' ] for epinfo in epinfobuf ] ) ) if eval_env is not None : logger . logkv ( \\'eval_eprewmean\\' , safemean ( [ epinfo [ \\'r\\' ] for epinfo in eval_epinfobuf ] ) ) logger . logkv ( \\'eval_eplenmean\\' , safemean ( [ epinfo [ \\'l\\' ] for epinfo in eval_epinfobuf ] ) ) logger . logkv ( \\'time_elapsed\\' , tnow - tfirststart ) for ( lossval , lossname ) in zip ( lossvals , model . loss_names ) : logger . logkv ( lossname , lossval ) if MPI is None or MPI . COMM_WORLD . Get_rank ( ) == 0 : logger . dumpkvs ( ) if save_interval and ( update % save_interval == 0 or update == 1 ) and logger . get_dir ( ) and ( MPI is None or MPI . COMM_WORLD . Get_rank ( ) == 0 ) : checkdir = osp . join ( logger . get_dir ( ) , \\'checkpoints\\' ) os . makedirs ( checkdir , exist_ok = True ) savepath = osp . join ( checkdir , \\'%.5i\\' % update ) print ( \\'Saving to\\' , savepath ) model . save ( savepath ) return model',\n",
       "  'score': '81.52028',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_15927',\n",
       "  'title': None,\n",
       "  'text': 'def graph_attention ( q , k , v , bias , dropout_rate = 0.0 , image_shapes = None , name = None , make_image_summary = True , save_weights_to = None , dropout_broadcast_dims = None , adjacency_matrix = None , num_edge_types = 5 ) : with tf . variable_scope ( name , default_name = \"dot_product_attention\" , values = [ q , k , v ] ) as scope : # [batch, num_heads, query_length, memory_length] logits = tf . matmul ( q , k , transpose_b = True ) if adjacency_matrix is not None : key_head_depth = common_layers . shape_list ( q ) [ - 1 ] adjacency_vectors = make_edge_vectors ( adjacency_matrix , num_edge_types , key_head_depth , name = name ) # transposing q to be [batch, length_q, heads, depth_k] # to allow for matmul with [batch, length_q, length_q, depth_k] q_t = tf . transpose ( q , [ 0 , 2 , 1 , 3 ] ) adj_logits = tf . matmul ( q_t , adjacency_vectors , transpose_b = True ) logits += tf . transpose ( adj_logits , [ 0 , 2 , 1 , 3 ] ) # [batch, depth, num_nodes, num_nodes] if bias is not None : logits += bias weights = tf . nn . softmax ( logits , name = \"attention_weights\" ) if save_weights_to is not None : save_weights_to [ scope . name ] = weights # dropping out the attention links for each of the heads weights = common_layers . dropout_with_broadcast_dims ( weights , 1.0 - dropout_rate , broadcast_dims = dropout_broadcast_dims ) if common_layers . should_generate_summaries ( ) and make_image_summary : common_attention . attention_image_summary ( weights , image_shapes ) return tf . matmul ( weights , v )',\n",
       "  'score': '81.514626',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_666897',\n",
       "  'title': None,\n",
       "  'text': 'def create_tf_model ( self , input_tensor , training = False ) : with tf . variable_scope ( \"model\" ) : output_tensors , debug_tensors = self . __call__ ( input_tensor , training , for_tensorflow = True , for_keras = False ) output_tensors . update ( debug_tensors ) return output_tensors',\n",
       "  'score': '81.512375',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_349716',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , actions ) : super ( SparseQ , self ) . __init__ ( ) self . model = SparseNet ( inputSize = ( 4 , 84 , 84 ) , outChannels = [ 16 , 32 ] , kernelSize = [ 8 , 4 ] , stride = [ 4 , 2 ] , n = 256 , outputSize = actions , c_k = [ 160 , 320 ] , k = 40 , weightSparsity = 0.3 , useBatchNorm = True , useSoftmax = False )',\n",
       "  'score': '81.50705',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_159236',\n",
       "  'title': None,\n",
       "  'text': 'def disassemble ( qobj ) : run_config = qobj . config . to_dict ( ) user_qobj_header = qobj . header . to_dict ( ) circuits = _experiments_to_circuits ( qobj ) return circuits , run_config , user_qobj_header',\n",
       "  'score': '81.48281',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_256954',\n",
       "  'title': None,\n",
       "  'text': 'def create ( backbone : ModelFactory , input_block : typing . Optional [ ModelFactory ] = None , initial_std_dev = 0.4 , factorized_noise = True ) : if input_block is None : input_block = IdentityFactory ( ) return NoisyQModelFactory ( input_block = input_block , backbone = backbone , initial_std_dev = initial_std_dev , factorized_noise = factorized_noise )',\n",
       "  'score': '81.478676',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_159052',\n",
       "  'title': None,\n",
       "  'text': 'def build_model ( nr_class , width , depth , conv_depth , * * kwargs ) : with Model . define_operators ( { \"|\" : concatenate , \">>\" : chain , \"**\" : clone } ) : embed = ( HashEmbed ( width , 5000 , column = 1 ) | StaticVectors ( \"spacy_pretrained_vectors\" , width , column = 5 ) | HashEmbed ( width // 2 , 750 , column = 2 ) | HashEmbed ( width // 2 , 750 , column = 3 ) | HashEmbed ( width // 2 , 750 , column = 4 ) ) >> LN ( Maxout ( width ) ) sent2vec = ( flatten_add_lengths >> with_getitem ( 0 , embed >> Residual ( ExtractWindow ( nW = 1 ) >> LN ( Maxout ( width ) ) ) ** conv_depth , ) >> ParametricAttention ( width ) >> Pooling ( sum_pool ) >> Residual ( LN ( Maxout ( width ) ) ) ** depth ) model = ( foreach ( sent2vec , drop_factor = 2.0 ) >> flatten_add_lengths # This block would allow the model to learn some cross-sentence # features. It\\'s not useful on this problem. It might make more # sense to use a BiLSTM here, following Liang et al (2016). # >> with_getitem(0, #    Residual(ExtractWindow(nW=1) >> LN(Maxout(width))) ** conv_depth # ) >> ParametricAttention ( width , hard = False ) >> Pooling ( sum_pool ) >> Residual ( LN ( Maxout ( width ) ) ) ** depth >> Softmax ( nr_class ) ) model . lsuv = False return model',\n",
       "  'score': '81.47647',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_176577',\n",
       "  'title': None,\n",
       "  'text': \"def load_precise_model ( model_name : str ) -> Any : if not model_name . endswith ( '.net' ) : print ( 'Warning: Unknown model type, ' , model_name ) inject_params ( model_name ) return load_keras ( ) . models . load_model ( model_name )\",\n",
       "  'score': '81.4664',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_1023191',\n",
       "  'title': None,\n",
       "  'text': \"def explore_maze ( ) : # simplified version of the reinforcement learning tutorial example structure = [ list ( '!!!!!!!!!!' ) , list ( '! !  ! ! !' ) , list ( '! !! ! ! !' ) , list ( '!    !   !' ) , list ( '! !!!!!! !' ) , list ( '! ! !    !' ) , list ( '! ! !!!! !' ) , list ( '!        !' ) , list ( '! !!!!!  !' ) , list ( '!   !    !' ) , list ( '!!!!!!!!!!' ) , ] structure = np . array ( [ [ ord ( c ) - ord ( ' ' ) for c in row ] for row in structure ] ) shape = np . array ( structure . shape ) environment = Maze ( structure , tuple ( shape - 2 ) ) controller = ActionValueTable ( shape . prod ( ) , 4 ) controller . initialize ( 1. ) learner = Q ( ) agent = LearningAgent ( controller , learner ) task = MDPMazeTask ( environment ) experiment = Experiment ( task , agent ) for i in range ( 30 ) : experiment . doInteractions ( 30 ) agent . learn ( ) agent . reset ( ) controller . params . reshape ( shape . prod ( ) , 4 ) . max ( 1 ) . reshape ( * shape ) # (0, 0) is upper left and (0, N) is upper right, so flip matrix upside down to match NESW action order greedy_policy = np . argmax ( controller . params . reshape ( shape . prod ( ) , 4 ) , 1 ) greedy_policy = np . flipud ( np . array ( list ( 'NESW' ) ) [ greedy_policy ] . reshape ( shape ) ) maze = np . flipud ( np . array ( list ( ' #' ) ) [ structure ] ) print ( 'Maze map:' ) print ( '\\\\n' . join ( '' . join ( row ) for row in maze ) ) print ( 'Greedy policy:' ) print ( '\\\\n' . join ( '' . join ( row ) for row in greedy_policy ) ) assert '\\\\n' . join ( '' . join ( row ) for row in greedy_policy ) == 'NNNNN\\\\nNSNNN\\\\nNSNNN\\\\nNEENN\\\\nNNNNN'\",\n",
       "  'score': '81.46393',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_143156',\n",
       "  'title': None,\n",
       "  'text': \"def run_bell_high_level ( n_shots = 1000 ) : # Step 1. Get a device. Either a QVM or a QPU qc = get_qc ( '9q-generic-qvm' ) q = [ 4 , 5 ] # qubits # Step 2. Construct your program program = Program ( H ( q [ 0 ] ) , CNOT ( q [ 0 ] , q [ 1 ] ) ) # Step 3. Run results = qc . run_and_measure ( program , trials = n_shots ) # Bincount bitstrings ints = sum ( results [ q [ i ] ] * 2 ** i for i in range ( len ( q ) ) ) print ( 'bincounts' , np . bincount ( ints ) ) # Check parity parities = ( results [ q [ 0 ] ] + results [ q [ 1 ] ] ) % 2 print ( 'avg parity' , np . mean ( parities ) )\",\n",
       "  'score': '81.45798',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_790892',\n",
       "  'title': None,\n",
       "  'text': 'def setup ( self , * * kwargs ) : # Check for saved model clobber = self . clobber self . clobber = False if not self . load_model ( \\'nPLD\\' ) : raise Exception ( \"Can\\'t find `nPLD` model for target.\" ) self . clobber = clobber # Powell iterations self . piter = kwargs . get ( \\'piter\\' , 3 ) self . pmaxf = kwargs . get ( \\'pmaxf\\' , 300 ) self . ppert = kwargs . get ( \\'ppert\\' , 0.1 )',\n",
       "  'score': '81.44275',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_16674',\n",
       "  'title': None,\n",
       "  'text': 'def train_world_model ( env , data_dir , output_dir , hparams , world_model_steps_num , epoch ) : world_model_steps_num += world_model_step_increment ( hparams , is_initial_epoch = ( epoch == 0 ) ) model_hparams = trainer_lib . create_hparams ( hparams . generative_model_params ) model_hparams . learning_rate = model_hparams . learning_rate_constant if epoch > 0 : model_hparams . learning_rate *= hparams . learning_rate_bump if hparams . wm_policy_param_sharing : model_hparams . optimizer_zero_grads = True restarter = Restarter ( \"world_model\" , output_dir , world_model_steps_num ) if restarter . should_skip : return world_model_steps_num with restarter . training_loop ( ) : train_supervised ( problem = env , model_name = hparams . generative_model , hparams = model_hparams , data_dir = data_dir , output_dir = output_dir , train_steps = restarter . target_global_step , eval_steps = 100 , local_eval_frequency = 2000 ) return world_model_steps_num',\n",
       "  'score': '81.43502',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_13790',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , * args , * * kwargs ) : # make sure the attribute is there if CreateModel fails self . _impl = None status , impl = deepspeech . impl . CreateModel ( * args , * * kwargs ) if status != 0 : raise RuntimeError ( \"CreateModel failed with error code {}\" . format ( status ) ) self . _impl = impl',\n",
       "  'score': '81.417175',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_257197',\n",
       "  'title': None,\n",
       "  'text': \"def instantiate ( self , * * extra_args ) : input_block = self . input_block . instantiate ( ) backbone = self . backbone . instantiate ( * * extra_args ) return QDuelingModel ( input_block , backbone , extra_args [ 'action_space' ] )\",\n",
       "  'score': '81.3935',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_39078',\n",
       "  'title': None,\n",
       "  'text': \"def step ( self , exploration ) : old_s = self . _current_ob if self . rng . rand ( ) <= exploration : act = self . rng . choice ( range ( self . num_actions ) ) else : history = self . recent_state ( ) history . append ( old_s ) history = np . stack ( history , axis = - 1 ) # state_shape + (Hist,) # assume batched network history = np . expand_dims ( history , axis = 0 ) q_values = self . predictor ( history ) [ 0 ] [ 0 ] # this is the bottleneck act = np . argmax ( q_values ) self . _current_ob , reward , isOver , info = self . player . step ( act ) self . _current_game_score . feed ( reward ) self . _current_episode . append ( Experience ( old_s , act , reward , isOver ) ) if isOver : flush_experience = True if 'ale.lives' in info : # if running Atari, do something special if info [ 'ale.lives' ] != 0 : # only record score and flush experience # when a whole game is over (not when an episode is over) flush_experience = False self . player . reset ( ) if flush_experience : self . total_scores . append ( self . _current_game_score . sum ) self . _current_game_score . reset ( ) # Ensure that the whole episode of experience is continuous in the replay buffer with self . memory . writer_lock : for exp in self . _current_episode : self . memory . append ( exp ) self . _current_episode . clear ( )\",\n",
       "  'score': '81.36403',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_29200',\n",
       "  'title': None,\n",
       "  'text': 'def wideresnet50 ( pooling ) : dir_models = os . path . join ( expanduser ( \"~\" ) , \\'.torch/wideresnet\\' ) path_hkl = os . path . join ( dir_models , \\'wideresnet50.hkl\\' ) if os . path . isfile ( path_hkl ) : params = hkl . load ( path_hkl ) # convert numpy arrays to torch Variables for k , v in sorted ( params . items ( ) ) : print ( k , v . shape ) params [ k ] = Variable ( torch . from_numpy ( v ) , requires_grad = True ) else : os . system ( \\'mkdir -p \\' + dir_models ) os . system ( \\'wget {} -O {}\\' . format ( model_urls [ \\'wideresnet50\\' ] , path_hkl ) ) f = define_model ( params ) model = WideResNet ( pooling ) return model',\n",
       "  'score': '81.360985',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_518931',\n",
       "  'title': None,\n",
       "  'text': 'def modelparams_to_state ( self , modelparams ) : if modelparams . ndim == 1 : qobj = qt . Qobj ( np . tensordot ( modelparams , self . data , 1 ) , dims = [ self . dims , self . dims ] ) if self . superrep is not None : qobj . superrep = self . superrep return qobj else : return list ( map ( self . modelparams_to_state , modelparams ) )',\n",
       "  'score': '81.34526',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_256869',\n",
       "  'title': None,\n",
       "  'text': 'def create ( input_block : ModelFactory , rnn_type : str , hidden_layers : typing . List [ int ] , output_dim : int , dropout = 0.0 ) : def instantiate ( * * _ ) : return MultilayerRnnSequenceModel ( input_block . instantiate ( ) , rnn_type = rnn_type , hidden_layers = hidden_layers , output_dim = output_dim , dropout = dropout ) return ModelFactory . generic ( instantiate )',\n",
       "  'score': '81.326645',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_13972',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , env , model , nsteps = 5 , gamma = 0.99 ) : super ( ) . __init__ ( env = env , model = model , nsteps = nsteps ) self . gamma = gamma self . batch_action_shape = [ x if x is not None else - 1 for x in model . train_model . action . shape . as_list ( ) ] self . ob_dtype = model . train_model . X . dtype . as_numpy_dtype',\n",
       "  'score': '81.3101',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_43054',\n",
       "  'title': None,\n",
       "  'text': 'def parse_args ( ) : parser = argparse . ArgumentParser ( \"DQN experiments for Atari games\" ) # Environment parser . add_argument ( \"--env\" , type = str , default = \"Pong\" , help = \"name of the game\" ) parser . add_argument ( \"--seed\" , type = int , default = 42 , help = \"which seed to use\" ) # Core DQN parameters parser . add_argument ( \"--replay-buffer-size\" , type = int , default = int ( 1e6 ) , help = \"replay buffer size\" ) parser . add_argument ( \"--lr\" , type = float , default = 1e-4 , help = \"learning rate for Adam optimizer\" ) parser . add_argument ( \"--num-steps\" , type = int , default = int ( 2e8 ) , help = \"total number of steps to \\\\\\n                        run the environment for\" ) parser . add_argument ( \"--batch-size\" , type = int , default = 32 , help = \"number of transitions to optimize \\\\\\n                        at the same time\" ) parser . add_argument ( \"--learning-freq\" , type = int , default = 4 , help = \"number of iterations between \\\\\\n                        every optimization step\" ) parser . add_argument ( \"--target-update-freq\" , type = int , default = 40000 , help = \"number of iterations between \\\\\\n                        every target network update\" ) # Bells and whistles boolean_flag ( parser , \"noisy\" , default = False , help = \"whether or not to NoisyNetwork\" ) boolean_flag ( parser , \"double-q\" , default = True , help = \"whether or not to use double q learning\" ) boolean_flag ( parser , \"dueling\" , default = False , help = \"whether or not to use dueling model\" ) boolean_flag ( parser , \"prioritized\" , default = False , help = \"whether or not to use prioritized replay buffer\" ) parser . add_argument ( \"--prioritized-alpha\" , type = float , default = 0.6 , help = \"alpha parameter for prioritized replay buffer\" ) parser . add_argument ( \"--prioritized-beta0\" , type = float , default = 0.4 , help = \"initial value of beta \\\\\\n                        parameters for prioritized replay\" ) parser . add_argument ( \"--prioritized-eps\" , type = float , default = 1e-6 , help = \"eps parameter for prioritized replay buffer\" ) # Checkpointing parser . add_argument ( \"--save-dir\" , type = str , default = None , required = True , help = \"directory in which \\\\\\n                        training state and model should be saved.\" ) parser . add_argument ( \"--save-azure-container\" , type = str , default = None , help = \"It present data will saved/loaded from Azure. \\\\\\n                        Should be in format ACCOUNT_NAME:ACCOUNT_KEY:\\\\\\n                        CONTAINER\" ) parser . add_argument ( \"--save-freq\" , type = int , default = 1e6 , help = \"save model once every time this many \\\\\\n                        iterations are completed\" ) boolean_flag ( parser , \"load-on-start\" , default = True , help = \"if true and model was previously saved then training \\\\\\n                 will be resumed\" ) # V: Attack Arguments # parser . add_argument ( \"--attack\" , type = str , default = None , help = \"Method to attack the model.\" ) parser . add_argument ( \"--attack-init\" , type = int , default = 0 , help = \"Iteration no. to begin attacks\" ) parser . add_argument ( \"--attack-prob\" , type = float , default = 0.0 , help = \"Probability of attack at each step, \\\\\\n                        float in range 0 - 1.0\" ) return parser . parse_args ( )',\n",
       "  'score': '81.266624',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_313876',\n",
       "  'title': None,\n",
       "  'text': \"def __init__ ( self , model , ntemps , nwalkers , betas = None , checkpoint_interval = None , checkpoint_signal = None , loglikelihood_function = None , nprocesses = 1 , use_mpi = False ) : self . model = model # create a wrapper for calling the model if loglikelihood_function is None : loglikelihood_function = 'loglikelihood' # frustratingly, emcee_pt does not support blob data, so we have to # turn it off model_call = models . CallModel ( model , loglikelihood_function , return_all_stats = False ) # Set up the pool if nprocesses > 1 : # these are used to help paralleize over multiple cores / MPI models . _global_instance = model_call model_call = models . _call_global_model prior_call = models . _call_global_model_logprior else : prior_call = models . CallModel ( model , 'logprior' , return_all_stats = False ) pool = choose_pool ( mpi = use_mpi , processes = nprocesses ) if pool is not None : pool . count = nprocesses # construct the sampler: PTSampler needs the likelihood and prior # functions separately ndim = len ( model . variable_params ) self . _sampler = emcee . PTSampler ( ntemps , nwalkers , ndim , model_call , prior_call , pool = pool , betas = betas ) self . _nwalkers = nwalkers self . _ntemps = ntemps self . _checkpoint_interval = checkpoint_interval self . _checkpoint_signal = checkpoint_signal\",\n",
       "  'score': '81.26295',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_626745',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , q , progress , dataset , max , images , controller ) : Thread . __init__ ( self ) self . q = q self . progress = progress self . dataset = dataset self . max = max self . images = images self . controller = controller',\n",
       "  'score': '81.26088',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_256780',\n",
       "  'title': None,\n",
       "  'text': 'def create ( input_width , input_height , input_channels = 1 , output_dim = 512 ) : def instantiate ( * * _ ) : return NatureCnn ( input_width = input_width , input_height = input_height , input_channels = input_channels , output_dim = output_dim ) return ModelFactory . generic ( instantiate )',\n",
       "  'score': '81.22246',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_20330',\n",
       "  'title': None,\n",
       "  'text': \"def get_squeezenet ( version , pretrained = False , ctx = cpu ( ) , root = os . path . join ( base . data_dir ( ) , 'models' ) , * * kwargs ) : net = SqueezeNet ( version , * * kwargs ) if pretrained : from . . model_store import get_model_file net . load_parameters ( get_model_file ( 'squeezenet%s' % version , root = root ) , ctx = ctx ) return net\",\n",
       "  'score': '81.20482',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_930178',\n",
       "  'title': None,\n",
       "  'text': 'def build_model ( self , in_dim , out_dim , n_class = 1 ) : self . model = build_model ( in_dim , out_dim = out_dim , n_hidden = self . n_hidden , l1_norm = self . l1_norm , l2_norm = self . l2_norm , n_deep = self . n_deep , drop = self . drop , learning_rate = self . learning_rate , optimizer = self . optimizer , activation = self . activation , n_class = self . n_class ) self . w0 = self . model . get_weights ( ) return self',\n",
       "  'score': '81.196106',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_213493',\n",
       "  'title': None,\n",
       "  'text': 'def multihead_encdec_attention_incremental ( query_antecedent , wq , wo , k , v , mask , name = \"multihead_attention\" ) : heads , _ , kv_channels = k . shape . dims [ - 3 : ] query_dims = query_antecedent . shape . dims [ : - 1 ] with tf . variable_scope ( name , default_name = \"multihead_attention\" ) : q = mtf . einsum ( [ query_antecedent , wq ] , mtf . Shape ( query_dims + [ heads , kv_channels ] ) ) o = dot_product_attention ( q , k , v , mask ) return mtf . einsum ( [ o , wo ] , query_antecedent . shape )',\n",
       "  'score': '81.19104',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_39962',\n",
       "  'title': None,\n",
       "  'text': 'def quantize ( self ) : quantized_model = callBigDlFunc ( self . bigdl_type , \"quantize\" , self . value ) return Layer . of ( quantized_model )',\n",
       "  'score': '81.182816',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_256957',\n",
       "  'title': None,\n",
       "  'text': \"def instantiate ( self , * * extra_args ) : input_block = self . input_block . instantiate ( ) backbone = self . backbone . instantiate ( * * extra_args ) return NoisyQModel ( input_block , backbone , extra_args [ 'action_space' ] , initial_std_dev = self . initial_std_dev , factorized_noise = self . factorized_noise )\",\n",
       "  'score': '81.16745',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_290700',\n",
       "  'title': None,\n",
       "  'text': 'def __call__ ( self , queries : mx . sym . Symbol , memory : mx . sym . Symbol , memory_lengths : Optional [ mx . sym . Symbol ] = None , bias : Optional [ mx . sym . Symbol ] = None ) -> mx . sym . Symbol : # (batch, query_max_length, depth) queries = mx . sym . FullyConnected ( data = queries , weight = self . w_q2h , no_bias = True , num_hidden = self . depth , flatten = False , name = \"%sq_transform\" % self . prefix ) # (batch, memory_max_length, depth) keys = mx . sym . FullyConnected ( data = memory , weight = self . w_k2h , no_bias = True , num_hidden = self . depth , flatten = False , name = \"%sk_transform\" % self . prefix ) # (batch, memory_max_length, depth) values = mx . sym . FullyConnected ( data = memory , weight = self . w_v2h , no_bias = True , num_hidden = self . depth , flatten = False , name = \"%sv_transform\" % self . prefix ) return self . _attend ( queries , keys , values , bias = bias , lengths = memory_lengths )',\n",
       "  'score': '81.13518',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_1315',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , d_model , d_inner , dropout , pre_lnorm = False ) : super ( PositionwiseFF , self ) . __init__ ( ) self . d_model = d_model self . d_inner = d_inner self . dropout = dropout self . CoreNet = nn . Sequential ( nn . Linear ( d_model , d_inner ) , nn . ReLU ( inplace = True ) , nn . Dropout ( dropout ) , nn . Linear ( d_inner , d_model ) , nn . Dropout ( dropout ) , ) self . layer_norm = LayerNorm ( d_model ) self . pre_lnorm = pre_lnorm',\n",
       "  'score': '81.1259',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_14064',\n",
       "  'title': None,\n",
       "  'text': \"def __init__ ( self , env , observations , latent , estimate_q = False , vf_latent = None , sess = None , * * tensors ) : self . X = observations self . state = tf . constant ( [ ] ) self . initial_state = None self . __dict__ . update ( tensors ) vf_latent = vf_latent if vf_latent is not None else latent vf_latent = tf . layers . flatten ( vf_latent ) latent = tf . layers . flatten ( latent ) # Based on the action space, will select what probability distribution type self . pdtype = make_pdtype ( env . action_space ) self . pd , self . pi = self . pdtype . pdfromlatent ( latent , init_scale = 0.01 ) # Take an action self . action = self . pd . sample ( ) # Calculate the neg log of our probability self . neglogp = self . pd . neglogp ( self . action ) self . sess = sess or tf . get_default_session ( ) if estimate_q : assert isinstance ( env . action_space , gym . spaces . Discrete ) self . q = fc ( vf_latent , 'q' , env . action_space . n ) self . vf = self . q else : self . vf = fc ( vf_latent , 'vf' , 1 ) self . vf = self . vf [ : , 0 ]\",\n",
       "  'score': '81.11301',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_839371',\n",
       "  'title': None,\n",
       "  'text': \"def bdpg_chains2 ( self : Experiment , logdir = None , env = 1 , heads = 3 , n = 50 , bootstrap = False , sr = 50000 ) : from tensorflow . contrib import layers import gym from gym import spaces from gym import wrappers import numpy as np from tensorflow . contrib . framework import arg_scope def gym_make ( id ) -> gym . Env : return gym . make ( id ) chi . set_loglevel ( 'debug' ) if env == 0 : import gym_mix from chi . rl . wrappers import PenalizeAction env = gym_mix . envs . ChainEnv ( n ) env = PenalizeAction ( env , .001 , 1 ) elif env == 1 : # env = gym.make('Pendulum-v0') env = gym . make ( 'MountainCarContinuous-v0' ) if bootstrap : class Noise ( Wrapper ) : def __init__ ( self , env ) : super ( ) . __init__ ( env ) self . n = 3 self . observation_space = gym . spaces . Box ( np . concatenate ( ( self . observation_space . low , np . full ( [ self . n ] , - 1 ) ) ) , np . concatenate ( ( self . observation_space . high , np . full ( [ self . n ] , 1 ) ) ) ) def _reset ( self ) : s = super ( ) . _reset ( ) self . noise = np . random . uniform ( - 1 , 1 , [ self . n ] ) s = np . concatenate ( [ s , self . noise ] ) return s def _step ( self , action ) : s , r , d , i = super ( ) . _step ( action ) s = np . concatenate ( [ s , self . noise ] ) return s , r , d , i env = Noise ( env ) print_env ( env ) def pp ( x ) : # v = get_local_variable('noise', [x.shape[0], 100], initializer=tf.random_normal_initializer) # y = tf.concat(x, v) return x def ac ( x ) : with tf . name_scope ( 'actor_head' ) : x = layers . fully_connected ( x , 50 , biases_initializer = layers . xavier_initializer ( ) ) x = layers . fully_connected ( x , 50 , biases_initializer = layers . xavier_initializer ( ) ) # a = layers.fully_connected(x, env.action_space.shape[0], None, weights_initializer=tf.random_normal_initializer(0, 1e-4)) a = layers . fully_connected ( x , env . action_space . shape [ 0 ] , None ) return a def cr ( x , a ) : with tf . name_scope ( 'critic_head' ) : x = layers . fully_connected ( x , 50 , biases_initializer = layers . xavier_initializer ( ) ) x = tf . concat ( [ x , a ] , axis = 1 ) x = layers . fully_connected ( x , 50 , biases_initializer = layers . xavier_initializer ( ) ) # q = layers.fully_connected(x, 1, None, weights_initializer=tf.random_normal_initializer(0, 1e-4)) q = layers . fully_connected ( x , 1 , None ) return tf . squeeze ( q , 1 ) if bootstrap : agent = DdpgAgent ( env , ac , cr , replay_start = sr , noise = lambda a : a ) else : agent = DdpgAgent ( env , ac , cr , replay_start = sr ) threshold = getattr ( getattr ( env , 'spec' , None ) , 'reward_threshold' , None ) for ep in range ( 100000 ) : R , info = agent . play_episode ( ) if ep % 20 == 0 : head = info . get ( 'head' ) print ( f'Return of episode {ep} after timestep {agent.t}: {R} (head = {head}, threshold = {threshold})' ) if ep % 100 == 0 and bootstrap : pass\",\n",
       "  'score': '81.11094',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_14988',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , env_batch_size , * args , * * kwargs ) : super ( BatchDQNAgent , self ) . __init__ ( * args , * * kwargs ) self . env_batch_size = env_batch_size obs_size = dqn_agent . NATURE_DQN_OBSERVATION_SHAPE state_shape = [ self . env_batch_size , obs_size [ 0 ] , obs_size [ 1 ] , dqn_agent . NATURE_DQN_STACK_SIZE ] self . state_batch = np . zeros ( state_shape ) self . state = None # assure it will be not used self . _observation = None # assure it will be not used self . reset_current_rollouts ( )',\n",
       "  'score': '81.10599',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_518731',\n",
       "  'title': None,\n",
       "  'text': \"def score ( self , outcomes , modelparams , expparams , return_L = False ) : if len ( modelparams . shape ) == 1 : modelparams = modelparams [ : , np . newaxis ] # compute likelihood at central point L0 = self . likelihood ( outcomes , modelparams , expparams ) # allocate space for the score q = np . empty ( [ self . n_modelparams , outcomes . shape [ 0 ] , modelparams . shape [ 0 ] , expparams . shape [ 0 ] ] ) h_perturb = np . empty ( modelparams . shape ) # just loop over the model parameter as there usually won't be so many # of them that vectorizing would be worth the effort. for mp_idx in range ( self . n_modelparams ) : h_perturb [ : ] = np . zeros ( modelparams . shape ) h_perturb [ : , mp_idx ] = self . h [ mp_idx ] # use the chain rule since taking the numerical derivative of a  # logarithm is unstable q [ mp_idx , : ] = ( self . likelihood ( outcomes , modelparams + h_perturb , expparams ) - self . likelihood ( outcomes , modelparams - h_perturb , expparams ) ) / ( 2 * self . h [ mp_idx ] * L0 ) if return_L : return q , L0 else : return q\",\n",
       "  'score': '81.09181',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_1081836',\n",
       "  'title': None,\n",
       "  'text': 'def build_training_model ( self ) : self . encoder_input = self . encoder . encoder_input self . encoder_embed = self . encoder . encoder_embed self . decoder_input = self . decoder . decoder_input self . decoder_embed = self . decoder . decoder_embed self . decoder_output = self . decoder . decoder_output if self . config [ \\'hierarchical\\' ] : pass # do something else : if self . num_gpus <= 1 : print ( \"[INFO] training with 1 GPU...\" ) self . training_model = Model ( [ self . encoder_input , self . decoder_input ] , self . decoder_output ) else : print ( \"[INFO] training with {} GPUs...\" . format ( self . num_gpus ) ) # we\\'ll store a copy of the model on *every* GPU and then combine # the results from the gradient updates on the CPU with tf . device ( \"/cpu:0\" ) : model = Model ( [ self . encoder_input , self . decoder_input ] , self . decoder_output ) # make the model parallel self . training_model = multi_gpu_model ( model , gpus = self . num_gpus )',\n",
       "  'score': '81.07272',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_434855',\n",
       "  'title': None,\n",
       "  'text': 'def build_model ( ) : x = Input ( ( 28 * 28 , ) , name = \"x\" ) hidden_dim = 512 h = x h = Dense ( hidden_dim ) ( h ) h = BatchNormalization ( ) ( h ) h = LeakyReLU ( 0.2 ) ( h ) h = Dropout ( 0.5 ) ( h ) h = Dense ( hidden_dim / 2 ) ( h ) h = BatchNormalization ( ) ( h ) h = LeakyReLU ( 0.2 ) ( h ) h = Dropout ( 0.5 ) ( h ) h = Dense ( 10 ) ( h ) h = Activation ( \\'softmax\\' ) ( h ) m = Model ( x , h ) m . compile ( \\'adam\\' , \\'categorical_crossentropy\\' , metrics = [ \\'accuracy\\' ] ) return m',\n",
       "  'score': '81.063354',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_839411',\n",
       "  'title': None,\n",
       "  'text': \"def dqn_atari ( logdir , env = 'Pong' , memory_size = 100000 ) : import numpy as np import gym import tensorflow as tf from gym import wrappers from tensorflow . contrib import layers from tensorflow . contrib . framework import arg_scope from chi . util import in_collections chi . set_loglevel ( 'debug' ) log_top ( logdir + '/logs/top' ) log_nvidia_smi ( logdir + '/logs/nvidia-smi' ) env += 'NoFrameskip-v3' env = gym . make ( env ) env = chi . rl . wrappers . AtariWrapper ( env ) env = chi . rl . wrappers . StackFrames ( env , 4 ) env = wrappers . SkipWrapper ( 4 ) ( env ) test = 10 train = 40 env = monitor = wrappers . Monitor ( env , logdir + '/monitor' , video_callable = lambda i : i % ( test + train ) == 0 or i % ( test + train ) == train ) print_env ( env ) @ chi . model ( tracker = tf . train . ExponentialMovingAverage ( 1 - .0005 ) , # TODO: replace with original weight freeze optimizer = tf . train . RMSPropOptimizer ( .00025 , .95 , .95 , .01 ) ) def q_network ( x ) : x /= 255 x = layers . conv2d ( x , 32 , 8 , 4 ) x = layers . conv2d ( x , 64 , 4 , 2 ) x = layers . conv2d ( x , 64 , 3 , 1 ) x = layers . flatten ( x ) x = layers . fully_connected ( x , 512 ) x = layers . fully_connected ( x , env . action_space . n , activation_fn = None ) x = tf . identity ( x , name = 'Q' ) return x memory = chi . rl . ReplayMemory ( memory_size , 32 ) agent = DqnAgent ( env , q_network , memory ) from time import time step = monitor . get_total_steps ( ) t = time ( ) for ep in range ( 100000 ) : for _ in range ( train ) : agent . play_episode ( ) for _ in range ( test ) : agent . play_episode ( test = True ) ar = np . mean ( monitor . get_episode_rewards ( ) [ - ( train + test ) : - test ] ) at = np . mean ( monitor . get_episode_rewards ( ) [ - test : ] ) ds = monitor . get_total_steps ( ) - step step = monitor . get_total_steps ( ) dt = time ( ) - t t = time ( ) print ( f'av. test return {at}, av. train return {ar}, av. fps {ds/dt}' )\",\n",
       "  'score': '81.0614',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_125933',\n",
       "  'title': None,\n",
       "  'text': 'def compile ( self , session = None ) : if not self . num_data == self . X . shape [ 0 ] : self . num_data = self . X . shape [ 0 ] self . q_alpha = Parameter ( np . zeros ( ( self . num_data , self . num_latent ) ) ) self . q_lambda = Parameter ( np . ones ( ( self . num_data , self . num_latent ) ) , transforms . positive ) return super ( VGP_opper_archambeau , self ) . compile ( session = session )',\n",
       "  'score': '81.05255',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_349854',\n",
       "  'title': None,\n",
       "  'text': \"def train ( self , params ) : # self.net = buildNetwork(params['encoding_num'] * params['num_lags'], #                         params['num_cells'], #                         params['encoding_num'], #                         bias=True, #                         outputbias=True) ds = SupervisedDataSet ( params [ 'encoding_num' ] * params [ 'num_lags' ] , params [ 'encoding_num' ] ) history = self . window ( self . history , params [ 'learning_window' ] ) n = params [ 'encoding_num' ] for i in xrange ( params [ 'num_lags' ] , len ( history ) ) : targets = numpy . zeros ( ( 1 , n ) ) targets [ 0 , : ] = self . encoder . encode ( history [ i ] ) features = numpy . zeros ( ( 1 , n * params [ 'num_lags' ] ) ) for lags in xrange ( params [ 'num_lags' ] ) : features [ 0 , lags * n : ( lags + 1 ) * n ] = self . encoder . encode ( history [ i - ( lags + 1 ) ] ) ds . addSample ( features , targets ) trainer = BackpropTrainer ( self . net , dataset = ds , verbose = params [ 'verbosity' ] > 0 ) if len ( history ) > 1 : trainer . trainEpochs ( params [ 'num_epochs' ] )\",\n",
       "  'score': '81.041405',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_29247',\n",
       "  'title': None,\n",
       "  'text': \"def alexnet ( num_classes = 1000 , pretrained = 'imagenet' ) : # https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py model = models . alexnet ( pretrained = False ) if pretrained is not None : settings = pretrained_settings [ 'alexnet' ] [ pretrained ] model = load_pretrained ( model , num_classes , settings ) model = modify_alexnet ( model ) return model\",\n",
       "  'score': '81.02626',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_839912',\n",
       "  'title': None,\n",
       "  'text': \"def ddpg_car ( self : Experiment , logdir = None , env = 3 ) : from tensorflow . contrib import layers import gym from gym import spaces from gym import wrappers import numpy as np from tensorflow . contrib . framework import arg_scope from chi . rl import ReplayMemory if env == 0 : import gym_mix env = gym . make ( 'ContinuousCopyRand-v0' ) elif env == 1 : env = gym . make ( 'Pendulum-v0' ) class P ( gym . Wrapper ) : def _step ( self , a ) : observation , reward , done , info = self . env . step ( a ) # observation = observation * np.array([1, 1, 1 / 8]) reward = reward - .01 * a ** 2 reward *= .1 return observation , reward , done , info env = P ( env ) elif env == 2 : env = gym . make ( 'MountainCarContinuous-v0' ) env = wrappers . Monitor ( env , logdir + '/monitor' ) elif env == 3 : import rlunity # print(rlunity.__file__) env = gym . make ( 'UnityCar-v0' ) env = wrappers . Monitor ( env , logdir + '/monitor' , video_callable = None ) env = wrappers . SkipWrapper ( 1 ) ( env ) env = PenalizeAction ( env ) print_env ( env ) m = ReplayMemory ( 100000 ) @ model ( optimizer = tf . train . AdamOptimizer ( .00005 ) , tracker = tf . train . ExponentialMovingAverage ( 1 - 0.0005 ) ) def pp ( x ) : # x = layers.batch_norm(x, trainable=False) print ( x . shape ) # x = tf.Print(x, [x], summarize=20) x = tf . concat ( [ tf . maximum ( x , 0 ) , - tf . minimum ( x , 0 ) ] , 1 ) # x = tf.reshape(o, [tf.shape(o)[0], 32*32*3]) x = layers . fully_connected ( x , 300 ) x = layers . fully_connected ( x , 300 ) return x @ model ( optimizer = tf . train . AdamOptimizer ( 0.0001 ) , tracker = tf . train . ExponentialMovingAverage ( 1 - 0.001 ) ) def actor ( x , noise = False ) : # x = layers.fully_connected(x, 50, biases_initializer=layers.xavier_initializer()) x = layers . fully_connected ( x , 300 , biases_initializer = layers . xavier_initializer ( ) ) x = layers . fully_connected ( x , 300 , biases_initializer = layers . xavier_initializer ( ) ) a = layers . fully_connected ( x , env . action_space . shape [ 0 ] , None , weights_initializer = tf . random_normal_initializer ( 0 , 1e-4 ) ) return a @ model ( optimizer = tf . train . AdamOptimizer ( .001 ) , tracker = tf . train . ExponentialMovingAverage ( 1 - 0.001 ) ) def critic ( x , a ) : x = layers . fully_connected ( x , 300 , biases_initializer = layers . xavier_initializer ( ) ) x = tf . concat ( [ x , a ] , axis = 1 ) x = layers . fully_connected ( x , 300 , biases_initializer = layers . xavier_initializer ( ) ) x = layers . fully_connected ( x , 300 , biases_initializer = layers . xavier_initializer ( ) ) q = layers . fully_connected ( x , 1 , None , weights_initializer = tf . random_normal_initializer ( 0 , 1e-4 ) ) return tf . squeeze ( q , 1 ) # @chi.function # def plot(): #   obsp = env.observation_space #   h = obsp.high #   l = obsp.low #   x, y = tf.meshgrid(tf.linspace(l[0], h[0], 100), tf.linspace(l[1], h[1], 100)) #   x = tf.reshape(x, [-1]) #   y = tf.reshape(y, [-1]) #   inp = tf.stack(x, y, axis=1) # #   s = pp(inp) #   a0 = actor(s) agent = DdpgAgent ( env , actor , critic , pp , m , training_repeats = 5 ) for ep in range ( 100000 ) : ret , _ = agent . play_episode ( ) if ep % 100 == 0 : print ( f'Episode {ep}: R={ret}, t={agent.t})' ) getattr ( getattr ( env , 'unwrapped' , env ) , 'report' , lambda : None ) ( )\",\n",
       "  'score': '81.018394',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_47789',\n",
       "  'title': None,\n",
       "  'text': 'def compute_batch_q_values ( self , state_batch ) : batch = self . process_state_batch ( state_batch ) q_values = self . model . predict_on_batch ( batch ) assert q_values . shape == ( len ( state_batch ) , self . nb_actions ) return q_values',\n",
       "  'score': '80.96343',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_13846',\n",
       "  'title': None,\n",
       "  'text': 'def _mlp ( hiddens , input_ , num_actions , scope , reuse = False , layer_norm = False ) : with tf . variable_scope ( scope , reuse = reuse ) : out = input_ for hidden in hiddens : out = layers . fully_connected ( out , num_outputs = hidden , activation_fn = None ) if layer_norm : out = layers . layer_norm ( out , center = True , scale = True ) out = tf . nn . relu ( out ) q_out = layers . fully_connected ( out , num_outputs = num_actions , activation_fn = None ) return q_out',\n",
       "  'score': '80.93681',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_983903',\n",
       "  'title': None,\n",
       "  'text': 'def m ( self , model , pre_save = None , post_save = None , seed = None , quantity = None ) : make = partial ( self . make , model = model , pre_save = pre_save , post_save = post_save , seed = seed , quantity = quantity ) def fn ( * * kwargs ) : return make ( fields = kwargs ) return fn',\n",
       "  'score': '80.9235',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_544133',\n",
       "  'title': None,\n",
       "  'text': \"def load_model ( self , path ) : with open ( path ) as model : for line in model : data = loads ( line ) if data [ 0 ] == 'Q1' : _ , x , word , p = data self . q1 [ x , word ] = p elif data [ 0 ] == 'Q2' : _ , x , y1 , y2 , p = data self . q2 [ x , y1 , y2 ] = p elif data [ 0 ] == 'WORDS' : self . well_known_words = data [ 1 ] self . __build_caches ( )\",\n",
       "  'score': '80.92027',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_14224',\n",
       "  'title': None,\n",
       "  'text': 'def learn ( env , policy_func , reward_giver , expert_dataset , rank , pretrained , pretrained_weight , * , g_step , d_step , entcoeff , save_per_iter , ckpt_dir , log_dir , timesteps_per_batch , task_name , gamma , lam , max_kl , cg_iters , cg_damping = 1e-2 , vf_stepsize = 3e-4 , d_stepsize = 3e-4 , vf_iters = 3 , max_timesteps = 0 , max_episodes = 0 , max_iters = 0 , callback = None ) : nworkers = MPI . COMM_WORLD . Get_size ( ) rank = MPI . COMM_WORLD . Get_rank ( ) np . set_printoptions ( precision = 3 ) # Setup losses and stuff # ---------------------------------------- ob_space = env . observation_space ac_space = env . action_space pi = policy_func ( \"pi\" , ob_space , ac_space , reuse = ( pretrained_weight != None ) ) oldpi = policy_func ( \"oldpi\" , ob_space , ac_space ) atarg = tf . placeholder ( dtype = tf . float32 , shape = [ None ] ) # Target advantage function (if applicable) ret = tf . placeholder ( dtype = tf . float32 , shape = [ None ] ) # Empirical return ob = U . get_placeholder_cached ( name = \"ob\" ) ac = pi . pdtype . sample_placeholder ( [ None ] ) kloldnew = oldpi . pd . kl ( pi . pd ) ent = pi . pd . entropy ( ) meankl = tf . reduce_mean ( kloldnew ) meanent = tf . reduce_mean ( ent ) entbonus = entcoeff * meanent vferr = tf . reduce_mean ( tf . square ( pi . vpred - ret ) ) ratio = tf . exp ( pi . pd . logp ( ac ) - oldpi . pd . logp ( ac ) ) # advantage * pnew / pold surrgain = tf . reduce_mean ( ratio * atarg ) optimgain = surrgain + entbonus losses = [ optimgain , meankl , entbonus , surrgain , meanent ] loss_names = [ \"optimgain\" , \"meankl\" , \"entloss\" , \"surrgain\" , \"entropy\" ] dist = meankl all_var_list = pi . get_trainable_variables ( ) var_list = [ v for v in all_var_list if v . name . startswith ( \"pi/pol\" ) or v . name . startswith ( \"pi/logstd\" ) ] vf_var_list = [ v for v in all_var_list if v . name . startswith ( \"pi/vff\" ) ] assert len ( var_list ) == len ( vf_var_list ) + 1 d_adam = MpiAdam ( reward_giver . get_trainable_variables ( ) ) vfadam = MpiAdam ( vf_var_list ) get_flat = U . GetFlat ( var_list ) set_from_flat = U . SetFromFlat ( var_list ) klgrads = tf . gradients ( dist , var_list ) flat_tangent = tf . placeholder ( dtype = tf . float32 , shape = [ None ] , name = \"flat_tan\" ) shapes = [ var . get_shape ( ) . as_list ( ) for var in var_list ] start = 0 tangents = [ ] for shape in shapes : sz = U . intprod ( shape ) tangents . append ( tf . reshape ( flat_tangent [ start : start + sz ] , shape ) ) start += sz gvp = tf . add_n ( [ tf . reduce_sum ( g * tangent ) for ( g , tangent ) in zipsame ( klgrads , tangents ) ] ) # pylint: disable=E1111 fvp = U . flatgrad ( gvp , var_list ) assign_old_eq_new = U . function ( [ ] , [ ] , updates = [ tf . assign ( oldv , newv ) for ( oldv , newv ) in zipsame ( oldpi . get_variables ( ) , pi . get_variables ( ) ) ] ) compute_losses = U . function ( [ ob , ac , atarg ] , losses ) compute_lossandgrad = U . function ( [ ob , ac , atarg ] , losses + [ U . flatgrad ( optimgain , var_list ) ] ) compute_fvp = U . function ( [ flat_tangent , ob , ac , atarg ] , fvp ) compute_vflossandgrad = U . function ( [ ob , ret ] , U . flatgrad ( vferr , vf_var_list ) ) @ contextmanager def timed ( msg ) : if rank == 0 : print ( colorize ( msg , color = \\'magenta\\' ) ) tstart = time . time ( ) yield print ( colorize ( \"done in %.3f seconds\" % ( time . time ( ) - tstart ) , color = \\'magenta\\' ) ) else : yield def allmean ( x ) : assert isinstance ( x , np . ndarray ) out = np . empty_like ( x ) MPI . COMM_WORLD . Allreduce ( x , out , op = MPI . SUM ) out /= nworkers return out U . initialize ( ) th_init = get_flat ( ) MPI . COMM_WORLD . Bcast ( th_init , root = 0 ) set_from_flat ( th_init ) d_adam . sync ( ) vfadam . sync ( ) if rank == 0 : print ( \"Init param sum\" , th_init . sum ( ) , flush = True ) # Prepare for rollouts # ---------------------------------------- seg_gen = traj_segment_generator ( pi , env , reward_giver , timesteps_per_batch , stochastic = True ) episodes_so_far = 0 timesteps_so_far = 0 iters_so_far = 0 tstart = time . time ( ) lenbuffer = deque ( maxlen = 40 ) # rolling buffer for episode lengths rewbuffer = deque ( maxlen = 40 ) # rolling buffer for episode rewards true_rewbuffer = deque ( maxlen = 40 ) assert sum ( [ max_iters > 0 , max_timesteps > 0 , max_episodes > 0 ] ) == 1 g_loss_stats = stats ( loss_names ) d_loss_stats = stats ( reward_giver . loss_name ) ep_stats = stats ( [ \"True_rewards\" , \"Rewards\" , \"Episode_length\" ] ) # if provide pretrained weight if pretrained_weight is not None : U . load_state ( pretrained_weight , var_list = pi . get_variables ( ) ) while True : if callback : callback ( locals ( ) , globals ( ) ) if max_timesteps and timesteps_so_far >= max_timesteps : break elif max_episodes and episodes_so_far >= max_episodes : break elif max_iters and iters_so_far >= max_iters : break # Save model if rank == 0 and iters_so_far % save_per_iter == 0 and ckpt_dir is not None : fname = os . path . join ( ckpt_dir , task_name ) os . makedirs ( os . path . dirname ( fname ) , exist_ok = True ) saver = tf . train . Saver ( ) saver . save ( tf . get_default_session ( ) , fname ) logger . log ( \"********** Iteration %i ************\" % iters_so_far ) def fisher_vector_product ( p ) : return allmean ( compute_fvp ( p , * fvpargs ) ) + cg_damping * p # ------------------ Update G ------------------ logger . log ( \"Optimizing Policy...\" ) for _ in range ( g_step ) : with timed ( \"sampling\" ) : seg = seg_gen . __next__ ( ) add_vtarg_and_adv ( seg , gamma , lam ) # ob, ac, atarg, ret, td1ret = map(np.concatenate, (obs, acs, atargs, rets, td1rets)) ob , ac , atarg , tdlamret = seg [ \"ob\" ] , seg [ \"ac\" ] , seg [ \"adv\" ] , seg [ \"tdlamret\" ] vpredbefore = seg [ \"vpred\" ] # predicted value function before udpate atarg = ( atarg - atarg . mean ( ) ) / atarg . std ( ) # standardized advantage function estimate if hasattr ( pi , \"ob_rms\" ) : pi . ob_rms . update ( ob ) # update running mean/std for policy args = seg [ \"ob\" ] , seg [ \"ac\" ] , atarg fvpargs = [ arr [ : : 5 ] for arr in args ] assign_old_eq_new ( ) # set old parameter values to new parameter values with timed ( \"computegrad\" ) : * lossbefore , g = compute_lossandgrad ( * args ) lossbefore = allmean ( np . array ( lossbefore ) ) g = allmean ( g ) if np . allclose ( g , 0 ) : logger . log ( \"Got zero gradient. not updating\" ) else : with timed ( \"cg\" ) : stepdir = cg ( fisher_vector_product , g , cg_iters = cg_iters , verbose = rank == 0 ) assert np . isfinite ( stepdir ) . all ( ) shs = .5 * stepdir . dot ( fisher_vector_product ( stepdir ) ) lm = np . sqrt ( shs / max_kl ) # logger.log(\"lagrange multiplier:\", lm, \"gnorm:\", np.linalg.norm(g)) fullstep = stepdir / lm expectedimprove = g . dot ( fullstep ) surrbefore = lossbefore [ 0 ] stepsize = 1.0 thbefore = get_flat ( ) for _ in range ( 10 ) : thnew = thbefore + fullstep * stepsize set_from_flat ( thnew ) meanlosses = surr , kl ,  * _ = allmean ( np . array ( compute_losses ( * args ) ) ) improve = surr - surrbefore logger . log ( \"Expected: %.3f Actual: %.3f\" % ( expectedimprove , improve ) ) if not np . isfinite ( meanlosses ) . all ( ) : logger . log ( \"Got non-finite value of losses -- bad!\" ) elif kl > max_kl * 1.5 : logger . log ( \"violated KL constraint. shrinking step.\" ) elif improve < 0 : logger . log ( \"surrogate didn\\'t improve. shrinking step.\" ) else : logger . log ( \"Stepsize OK!\" ) break stepsize *= .5 else : logger . log ( \"couldn\\'t compute a good step\" ) set_from_flat ( thbefore ) if nworkers > 1 and iters_so_far % 20 == 0 : paramsums = MPI . COMM_WORLD . allgather ( ( thnew . sum ( ) , vfadam . getflat ( ) . sum ( ) ) ) # list of tuples assert all ( np . allclose ( ps , paramsums [ 0 ] ) for ps in paramsums [ 1 : ] ) with timed ( \"vf\" ) : for _ in range ( vf_iters ) : for ( mbob , mbret ) in dataset . iterbatches ( ( seg [ \"ob\" ] , seg [ \"tdlamret\" ] ) , include_final_partial_batch = False , batch_size = 128 ) : if hasattr ( pi , \"ob_rms\" ) : pi . ob_rms . update ( mbob ) # update running mean/std for policy g = allmean ( compute_vflossandgrad ( mbob , mbret ) ) vfadam . update ( g , vf_stepsize ) g_losses = meanlosses for ( lossname , lossval ) in zip ( loss_names , meanlosses ) : logger . record_tabular ( lossname , lossval ) logger . record_tabular ( \"ev_tdlam_before\" , explained_variance ( vpredbefore , tdlamret ) ) # ------------------ Update D ------------------ logger . log ( \"Optimizing Discriminator...\" ) logger . log ( fmt_row ( 13 , reward_giver . loss_name ) ) ob_expert , ac_expert = expert_dataset . get_next_batch ( len ( ob ) ) batch_size = len ( ob ) // d_step d_losses = [ ] # list of tuples, each of which gives the loss for a minibatch for ob_batch , ac_batch in dataset . iterbatches ( ( ob , ac ) , include_final_partial_batch = False , batch_size = batch_size ) : ob_expert , ac_expert = expert_dataset . get_next_batch ( len ( ob_batch ) ) # update running mean/std for reward_giver if hasattr ( reward_giver , \"obs_rms\" ) : reward_giver . obs_rms . update ( np . concatenate ( ( ob_batch , ob_expert ) , 0 ) )  * newlosses , g = reward_giver . lossandgrad ( ob_batch , ac_batch , ob_expert , ac_expert ) d_adam . update ( allmean ( g ) , d_stepsize ) d_losses . append ( newlosses ) logger . log ( fmt_row ( 13 , np . mean ( d_losses , axis = 0 ) ) ) lrlocal = ( seg [ \"ep_lens\" ] , seg [ \"ep_rets\" ] , seg [ \"ep_true_rets\" ] ) # local values listoflrpairs = MPI . COMM_WORLD . allgather ( lrlocal ) # list of tuples lens , rews , true_rets = map ( flatten_lists , zip ( * listoflrpairs ) ) true_rewbuffer . extend ( true_rets ) lenbuffer . extend ( lens ) rewbuffer . extend ( rews ) logger . record_tabular ( \"EpLenMean\" , np . mean ( lenbuffer ) ) logger . record_tabular ( \"EpRewMean\" , np . mean ( rewbuffer ) ) logger . record_tabular ( \"EpTrueRewMean\" , np . mean ( true_rewbuffer ) ) logger . record_tabular ( \"EpThisIter\" , len ( lens ) ) episodes_so_far += len ( lens ) timesteps_so_far += sum ( lens ) iters_so_far += 1 logger . record_tabular ( \"EpisodesSoFar\" , episodes_so_far ) logger . record_tabular ( \"TimestepsSoFar\" , timesteps_so_far ) logger . record_tabular ( \"TimeElapsed\" , time . time ( ) - tstart ) if rank == 0 : logger . dump_tabular ( )',\n",
       "  'score': '80.90381',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_368767',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , greedy_q_learning , init_state_key ) : if isinstance ( greedy_q_learning , GreedyQLearning ) : self . __greedy_q_learning = greedy_q_learning else : raise TypeError ( ) self . __init_state_key = init_state_key',\n",
       "  'score': '80.891495',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_107854',\n",
       "  'title': None,\n",
       "  'text': 'def model ( x , is_train = True , reuse = False ) : with tf . variable_scope ( \"quan_cnn\" , reuse = reuse ) : net = tl . layers . InputLayer ( x , name = \\'input\\' ) net = tl . layers . QuanConv2dWithBN ( net , 32 , ( 5 , 5 ) , ( 1 , 1 ) , padding = \\'SAME\\' , act = tf . nn . relu , is_train = is_train , name = \\'qcbnb1\\' ) net = tl . layers . MaxPool2d ( net , ( 2 , 2 ) , ( 2 , 2 ) , padding = \\'SAME\\' , name = \\'pool1\\' ) net = tl . layers . QuanConv2dWithBN ( net , 64 , ( 5 , 5 ) , ( 1 , 1 ) , padding = \\'SAME\\' , act = tf . nn . relu , is_train = is_train , name = \\'qcbn2\\' ) net = tl . layers . MaxPool2d ( net , ( 2 , 2 ) , ( 2 , 2 ) , padding = \\'SAME\\' , name = \\'pool2\\' ) net = tl . layers . FlattenLayer ( net ) # net = tl.layers.DropoutLayer(net, 0.8, True, is_train, name=\\'drop1\\') net = tl . layers . QuanDenseLayerWithBN ( net , 256 , is_train = is_train , act = tf . nn . relu , name = \\'qdbn\\' ) # net = tl.layers.DropoutLayer(net, 0.8, True, is_train, name=\\'drop2\\') net = tl . layers . QuanDenseLayer ( net , 10 , name = \\'qdbn_out\\' ) return net',\n",
       "  'score': '80.85702',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_229551',\n",
       "  'title': None,\n",
       "  'text': 'def pnqp ( H , q , lower , upper , x_init = None , n_iter = 20 ) : GAMMA = 0.1 n_batch , n , _ = H . size ( ) pnqp_I = 1e-11 * torch . eye ( n ) . type_as ( H ) . expand_as ( H ) def obj ( x ) : return 0.5 * util . bquad ( x , H ) + util . bdot ( q , x ) if x_init is None : if n == 1 : x_init = - ( 1. / H . squeeze ( 2 ) ) * q else : H_lu = H . btrifact ( ) x_init = - q . btrisolve ( * H_lu ) # Clamped in the x assignment. else : x_init = x_init . clone ( ) # Don\\'t over-write the original x_init. x = util . eclamp ( x_init , lower , upper ) # Active examples in the batch. J = torch . ones ( n_batch ) . type_as ( x ) . byte ( ) for i in range ( n_iter ) : g = util . bmv ( H , x ) + q Ic = ( ( x == lower ) & ( g > 0 ) ) | ( ( x == upper ) & ( g < 0 ) ) If = 1 - Ic if If . is_cuda : Hff_I = util . bger ( If . float ( ) , If . float ( ) ) . type_as ( If ) not_Hff_I = 1 - Hff_I Hfc_I = util . bger ( If . float ( ) , Ic . float ( ) ) . type_as ( If ) else : Hff_I = util . bger ( If , If ) not_Hff_I = 1 - Hff_I Hfc_I = util . bger ( If , Ic ) g_ = g . clone ( ) g_ [ Ic ] = 0. H_ = H . clone ( ) H_ [ not_Hff_I ] = 0.0 H_ += pnqp_I if n == 1 : dx = - ( 1. / H_ . squeeze ( 2 ) ) * g_ else : H_lu_ = H_ . btrifact ( ) dx = - g_ . btrisolve ( * H_lu_ ) J = torch . norm ( dx , 2 , 1 ) >= 1e-4 m = J . sum ( ) . item ( ) # Number of active examples in the batch. if m == 0 : return x , H_ if n == 1 else H_lu_ , If , i alpha = torch . ones ( n_batch ) . type_as ( x ) decay = 0.1 max_armijo = GAMMA count = 0 while max_armijo <= GAMMA and count < 10 : # Crude way of making sure too much time isn\\'t being spent # doing the line search. # assert count < 10 maybe_x = util . eclamp ( x + torch . diag ( alpha ) . mm ( dx ) , lower , upper ) armijos = ( GAMMA + 1e-6 ) * torch . ones ( n_batch ) . type_as ( x ) armijos [ J ] = ( obj ( x ) - obj ( maybe_x ) ) [ J ] / util . bdot ( g , x - maybe_x ) [ J ] I = armijos <= GAMMA alpha [ I ] *= decay max_armijo = torch . max ( armijos ) count += 1 x = maybe_x # TODO: Maybe change this to a warning. print ( \"[WARNING] pnqp warning: Did not converge\" ) return x , H_ if n == 1 else H_lu_ , If , i',\n",
       "  'score': '80.84584',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_21992',\n",
       "  'title': None,\n",
       "  'text': \"def inception_v3 ( pretrained = False , ctx = cpu ( ) , root = os . path . join ( base . data_dir ( ) , 'models' ) , * * kwargs ) : net = Inception3 ( * * kwargs ) if pretrained : from . . model_store import get_model_file net . load_parameters ( get_model_file ( 'inceptionv3' , root = root ) , ctx = ctx ) return net\",\n",
       "  'score': '80.81865',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_16185',\n",
       "  'title': None,\n",
       "  'text': 'def masked_local_attention_1d ( q , k , v , block_length = 128 , make_image_summary = False , dropout_rate = 0. , name = None ) : with tf . variable_scope ( name , default_name = \"local_attention_1d\" , values = [ q , k , v ] ) : batch , heads , length , depth_k = common_layers . shape_list ( q ) depth_v = common_layers . shape_list ( v ) [ - 1 ] if isinstance ( block_length , tf . Tensor ) : const = tf . contrib . util . constant_value ( block_length ) if const is not None : block_length = int ( const ) # If (length < 2 * block_length), then we use only one block. if isinstance ( length , int ) and isinstance ( block_length , int ) : block_length = length if length < block_length * 2 else block_length else : block_length = tf . where ( tf . less ( length , block_length * 2 ) , length , block_length ) # Pad query, key, value to ensure multiple of block length. original_length = length padding_size = tf . mod ( - length , block_length ) length += padding_size padding = [ [ 0 , 0 ] , [ 0 , 0 ] , [ 0 , padding_size ] , [ 0 , 0 ] ] q = tf . pad ( q , padding ) k = tf . pad ( k , padding ) v = tf . pad ( v , padding ) if isinstance ( length , int ) and isinstance ( block_length , int ) : num_blocks = length // block_length else : num_blocks = tf . div ( length , block_length ) # Compute attention for the first query block. first_q = tf . slice ( q , [ 0 , 0 , 0 , 0 ] , [ - 1 , - 1 , block_length , - 1 ] ) first_k = tf . slice ( k , [ 0 , 0 , 0 , 0 ] , [ - 1 , - 1 , block_length , - 1 ] ) first_v = tf . slice ( v , [ 0 , 0 , 0 , 0 ] , [ - 1 , - 1 , block_length , - 1 ] ) first_output = dot_product_attention ( first_q , first_k , first_v , attention_bias_lower_triangle ( block_length ) , dropout_rate = dropout_rate , make_image_summary = make_image_summary , name = \"first_block\" ) # Compute attention for all subsequent query blocks. q = tf . reshape ( q , [ batch , heads , num_blocks , block_length , depth_k ] ) k = tf . reshape ( k , [ batch , heads , num_blocks , block_length , depth_k ] ) v = tf . reshape ( v , [ batch , heads , num_blocks , block_length , depth_v ] ) local_k = _make_local_block ( k , depth_k , batch , heads , num_blocks , block_length ) local_v = _make_local_block ( v , depth_v , batch , heads , num_blocks , block_length ) tail_q = tf . slice ( q , [ 0 , 0 , 1 , 0 , 0 ] , [ - 1 , - 1 , - 1 , - 1 , - 1 ] ) tail_q = tf . reshape ( tail_q , [ batch , heads , num_blocks - 1 , block_length , depth_k ] ) local_length = common_layers . shape_list ( local_k ) [ 3 ] # make sure source_pos <= target_pos good_part = common_layers . ones_matrix_band_part ( block_length , local_length , - 1 , block_length , out_shape = [ 1 , 1 , 1 , block_length , local_length ] ) bias = ( 1.0 - good_part ) * - 1e9 # TODO(noam): figure out how to show a summary for the remaining blocks. # The naive way currently causes errors due to empty tensors. # output: [batch, heads, num_blocks-1, block_length, depth_v] tail_output = dot_product_attention ( tail_q , local_k , local_v , bias , dropout_rate = dropout_rate , make_image_summary = False , name = \"tail_block\" ) tail_output = tf . reshape ( tail_output , [ batch , heads , ( num_blocks - 1 ) * block_length , depth_v ] ) output = tf . concat ( [ first_output , tail_output ] , axis = 2 ) # Remove the padding if introduced. output = tf . slice ( output , [ 0 , 0 , 0 , 0 ] , [ - 1 , - 1 , original_length , - 1 ] ) output = tf . reshape ( output , [ batch , heads , original_length , depth_v ] ) return output',\n",
       "  'score': '80.79071',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_22335',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , model , data , session = None , batch_size = 50 , local_smoothing = 0 ) : # try and import keras and tensorflow global tf , keras if tf is None : import tensorflow as tf if LooseVersion ( tf . __version__ ) < LooseVersion ( \"1.4.0\" ) : warnings . warn ( \"Your TensorFlow version is older than 1.4.0 and not supported.\" ) if keras is None : try : import keras if LooseVersion ( keras . __version__ ) < LooseVersion ( \"2.1.0\" ) : warnings . warn ( \"Your Keras version is older than 2.1.0 and not supported.\" ) except : pass # determine the model inputs and outputs if str ( type ( model ) ) . endswith ( \"keras.engine.sequential.Sequential\\'>\" ) : self . model_inputs = model . inputs self . model_output = model . layers [ - 1 ] . output elif str ( type ( model ) ) . endswith ( \"keras.models.Sequential\\'>\" ) : self . model_inputs = model . inputs self . model_output = model . layers [ - 1 ] . output elif str ( type ( model ) ) . endswith ( \"keras.engine.training.Model\\'>\" ) : self . model_inputs = model . inputs self . model_output = model . layers [ - 1 ] . output elif str ( type ( model ) ) . endswith ( \"tuple\\'>\" ) : self . model_inputs = model [ 0 ] self . model_output = model [ 1 ] else : assert False , str ( type ( model ) ) + \" is not currently a supported model type!\" assert type ( self . model_output ) != list , \"The model output to be explained must be a single tensor!\" assert len ( self . model_output . shape ) < 3 , \"The model output must be a vector or a single value!\" self . multi_output = True if len ( self . model_output . shape ) == 1 : self . multi_output = False # check if we have multiple inputs self . multi_input = True if type ( self . model_inputs ) != list : self . multi_input = False self . model_inputs = [ self . model_inputs ] if type ( data ) != list : data = [ data ] self . data = data self . _num_vinputs = { } self . batch_size = batch_size self . local_smoothing = local_smoothing # if we are not given a session find a default session if session is None : # if keras is installed and already has a session then use it if keras is not None and keras . backend . tensorflow_backend . _SESSION is not None : session = keras . backend . get_session ( ) else : session = tf . keras . backend . get_session ( ) self . session = tf . get_default_session ( ) if session is None else session # see if there is a keras operation we need to save self . keras_phase_placeholder = None for op in self . session . graph . get_operations ( ) : if \\'keras_learning_phase\\' in op . name : self . keras_phase_placeholder = op . outputs [ 0 ] # save the expected output of the model #self.expected_value = self.run(self.model_output, self.model_inputs, self.data).mean(0) if not self . multi_output : self . gradients = [ None ] else : self . gradients = [ None for i in range ( self . model_output . shape [ 1 ] ) ]',\n",
       "  'score': '80.75711',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_31220',\n",
       "  'title': None,\n",
       "  'text': \"def build_model2 ( layers ) : d = 0.2 model = Sequential ( ) model . add ( LSTM ( 128 , input_shape = ( layers [ 1 ] , layers [ 0 ] ) , return_sequences = True ) ) model . add ( Dropout ( d ) ) model . add ( LSTM ( 64 , input_shape = ( layers [ 1 ] , layers [ 0 ] ) , return_sequences = False ) ) model . add ( Dropout ( d ) ) model . add ( Dense ( 16 , init = 'uniform' , activation = 'relu' ) ) model . add ( Dense ( 1 , init = 'uniform' , activation = 'relu' ) ) model . compile ( loss = 'mse' , optimizer = 'adam' , metrics = [ 'accuracy' ] ) return model\",\n",
       "  'score': '80.73126',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_934267',\n",
       "  'title': None,\n",
       "  'text': \"def best_model ( self ) : if hasattr ( self , 'halloffame' ) : model = self . _params [ 'specification' ] ( * self . parse_individual ( self . halloffame [ 0 ] ) ) model . pack_new_sequences ( self . _params [ 'sequence' ] ) return model else : raise NameError ( 'No best model found, have you ran the optimiser?' )\",\n",
       "  'score': '80.71712',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_5940',\n",
       "  'title': None,\n",
       "  'text': 'def create_body ( arch : Callable , pretrained : bool = True , cut : Optional [ Union [ int , Callable ] ] = None ) : model = arch ( pretrained ) cut = ifnone ( cut , cnn_config ( arch ) [ \\'cut\\' ] ) if cut is None : ll = list ( enumerate ( model . children ( ) ) ) cut = next ( i for i , o in reversed ( ll ) if has_pool_type ( o ) ) if isinstance ( cut , int ) : return nn . Sequential ( * list ( model . children ( ) ) [ : cut ] ) elif isinstance ( cut , Callable ) : return cut ( model ) else : raise NamedError ( \"cut must be either integer or a function\" )',\n",
       "  'score': '80.68739',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_441069',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , model = None ) : super ( NodeAffinity , self ) . __init__ ( ) self . _preferred_during_scheduling_ignored_during_execution = [ ] self . _required_during_scheduling_ignored_during_execution = None if model is not None : self . _build_with_model ( model )',\n",
       "  'score': '80.6862',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_159828',\n",
       "  'title': None,\n",
       "  'text': 'def assemble ( experiments , backend = None , qobj_id = None , qobj_header = None , # common run options shots = 1024 , memory = False , max_credits = None , seed_simulator = None , default_qubit_los = None , default_meas_los = None , # schedule run options schedule_los = None , meas_level = 2 , meas_return = \\'avg\\' , memory_slots = None , memory_slot_size = 100 , rep_time = None , parameter_binds = None , config = None , seed = None , # deprecated * * run_config ) : # deprecation matter if config : warnings . warn ( \\'config is not used anymore. Set all configs in \\' \\'run_config.\\' , DeprecationWarning ) run_config = run_config or config if seed : warnings . warn ( \\'seed is deprecated in favor of seed_simulator.\\' , DeprecationWarning ) seed_simulator = seed_simulator or seed # Get RunConfig(s) that will be inserted in Qobj to configure the run experiments = experiments if isinstance ( experiments , list ) else [ experiments ] qobj_id , qobj_header , run_config = _parse_run_args ( backend , qobj_id , qobj_header , shots , memory , max_credits , seed_simulator , default_qubit_los , default_meas_los , schedule_los , meas_level , meas_return , memory_slots , memory_slot_size , rep_time , parameter_binds , * * run_config ) # assemble either circuits or schedules if all ( isinstance ( exp , QuantumCircuit ) for exp in experiments ) : # If circuits are parameterized, bind parameters and remove from run_config bound_experiments , run_config = _expand_parameters ( circuits = experiments , run_config = run_config ) return assemble_circuits ( circuits = bound_experiments , qobj_id = qobj_id , qobj_header = qobj_header , run_config = run_config ) elif all ( isinstance ( exp , Schedule ) for exp in experiments ) : return assemble_schedules ( schedules = experiments , qobj_id = qobj_id , qobj_header = qobj_header , run_config = run_config ) else : raise QiskitError ( \"bad input to assemble() function; \" \"must be either circuits or schedules\" )',\n",
       "  'score': '80.66508',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_1081837',\n",
       "  'title': None,\n",
       "  'text': \"def build_inference_model ( self ) : # grab some important hyperparameters hidden_dim = self . config [ 'encoding-layer-width' ] recurrent_unit = self . config [ 'recurrent-unit-type' ] # build the encoder model self . encoder_model = Model ( self . encoder . encoder_input , self . encoder . encoder_hidden_state ) decoder_hidden_state_input = None decoder_hidden_state_output = None decoder_output = None # build the decoder model if recurrent_unit == 'lstm' : decoder_hidden_state_input_h = Input ( shape = ( hidden_dim , ) ) decoder_hidden_state_input_c = Input ( shape = ( hidden_dim , ) ) decoder_hidden_state_input = [ decoder_hidden_state_input_h , decoder_hidden_state_input_c ] # take in the regular inputs, condition on the hidden state _ , decoder_state_h , decoder_state_c = self . decoder . decoder_rnn ( self . decoder_embed , initial_state = decoder_hidden_state_input ) decoder_hidden_state_output = [ decoder_state_h , decoder_state_c ] elif recurrent_unit == 'gru' : decoder_hidden_state_input = [ Input ( shape = ( hidden_dim , ) ) ] # take in the regular inputs, condition on the hidden state decoder_output , hidden_state = self . decoder . decoder_rnn ( self . decoder_embed , initial_state = decoder_hidden_state_input ) decoder_hidden_state_output = [ hidden_state ] else : raise Exception ( 'Invalid recurrent unit type: {}' . format ( recurrent_unit ) ) decoder_output = self . decoder . decoder_dense ( decoder_output ) self . decoder_model = Model ( [ self . decoder_input ] + decoder_hidden_state_input , [ decoder_output ] + decoder_hidden_state_output )\",\n",
       "  'score': '80.650696',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_510896',\n",
       "  'title': None,\n",
       "  'text': 'def build_model ( self ) : # Direct passing model parameters can be used return linear_model . BayesianRidge ( normalize = True , verbose = True , compute_score = True )',\n",
       "  'score': '80.61861',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_115959',\n",
       "  'title': None,\n",
       "  'text': 'def build_graph ( model = None , args = None , input_names = None , transforms = \"default\" , framework_transforms = \"default\" ) : # Initialize an empty graph g = Graph ( ) # Detect framwork framework = detect_framework ( model ) if framework == \"torch\" : from . pytorch_builder import import_graph , FRAMEWORK_TRANSFORMS assert args is not None , \"Argument args must be provided for Pytorch models.\" import_graph ( g , model , args ) elif framework == \"tensorflow\" : from . tf_builder import import_graph , FRAMEWORK_TRANSFORMS import_graph ( g , model ) else : raise ValueError ( \"`model` input param must be a PyTorch, TensorFlow, or Keras-with-TensorFlow-backend model.\" ) # Apply Transforms if framework_transforms : if framework_transforms == \"default\" : framework_transforms = FRAMEWORK_TRANSFORMS for t in framework_transforms : g = t . apply ( g ) if transforms : if transforms == \"default\" : from . transforms import SIMPLICITY_TRANSFORMS transforms = SIMPLICITY_TRANSFORMS for t in transforms : g = t . apply ( g ) return g',\n",
       "  'score': '80.612976',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_29307',\n",
       "  'title': None,\n",
       "  'text': \"def polynet ( num_classes = 1000 , pretrained = 'imagenet' ) : if pretrained : settings = pretrained_settings [ 'polynet' ] [ pretrained ] assert num_classes == settings [ 'num_classes' ] , 'num_classes should be {}, but is {}' . format ( settings [ 'num_classes' ] , num_classes ) model = PolyNet ( num_classes = num_classes ) model . load_state_dict ( model_zoo . load_url ( settings [ 'url' ] ) ) model . input_space = settings [ 'input_space' ] model . input_size = settings [ 'input_size' ] model . input_range = settings [ 'input_range' ] model . mean = settings [ 'mean' ] model . std = settings [ 'std' ] else : model = PolyNet ( num_classes = num_classes ) return model\",\n",
       "  'score': '80.60756',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_214948',\n",
       "  'title': None,\n",
       "  'text': 'def plot_acquisition ( self , filename = None ) : if self . model . model is None : from copy import deepcopy model_to_plot = deepcopy ( self . model ) if self . normalize_Y : Y = normalize ( self . Y , self . normalization_type ) else : Y = self . Y model_to_plot . updateModel ( self . X , Y , self . X , Y ) else : model_to_plot = self . model return plot_acquisition ( self . acquisition . space . get_bounds ( ) , model_to_plot . model . X . shape [ 1 ] , model_to_plot . model , model_to_plot . model . X , model_to_plot . model . Y , self . acquisition . acquisition_function , self . suggest_next_locations ( ) , filename )',\n",
       "  'score': '80.59819',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_16183',\n",
       "  'title': None,\n",
       "  'text': 'def masked_within_block_local_attention_1d ( q , k , v , block_length = 64 , name = None ) : with tf . variable_scope ( name , default_name = \"within_local_attention_1d\" , values = [ q , k , v ] ) : batch , heads , length , depth_k = common_layers . shape_list ( q ) depth_v = common_layers . shape_list ( v ) [ - 1 ] if isinstance ( block_length , tf . Tensor ) : const = tf . contrib . util . constant_value ( block_length ) if const is not None : block_length = int ( const ) # Pad query, key, value to ensure multiple of block length. original_length = length padding_size = tf . mod ( - length , block_length ) length += padding_size padding = [ [ 0 , 0 ] , [ 0 , 0 ] , [ 0 , padding_size ] , [ 0 , 0 ] ] q = tf . pad ( q , padding ) k = tf . pad ( k , padding ) v = tf . pad ( v , padding ) # Compute attention for all subsequent query blocks. num_blocks = tf . div ( length , block_length ) q = tf . reshape ( q , [ batch , heads , num_blocks , block_length , depth_k ] ) k = tf . reshape ( k , [ batch , heads , num_blocks , block_length , depth_k ] ) v = tf . reshape ( v , [ batch , heads , num_blocks , block_length , depth_v ] ) # [batch, heads, num_blocks, block_length, block_length] attention = tf . matmul ( q , k , transpose_b = True ) attention += tf . reshape ( attention_bias_lower_triangle ( block_length ) , [ 1 , 1 , 1 , block_length , block_length ] ) attention = tf . nn . softmax ( attention ) # [batch, heads, num_blocks, block_length, depth_v] output = tf . matmul ( attention , v ) output = tf . reshape ( output , [ batch , heads , - 1 , depth_v ] ) # Remove the padding if introduced. output = tf . slice ( output , [ 0 , 0 , 0 , 0 ] , [ - 1 , - 1 , original_length , - 1 ] ) output . set_shape ( [ None if isinstance ( dim , tf . Tensor ) else dim for dim in ( batch , heads , length , depth_v ) ] ) return output',\n",
       "  'score': '80.582886',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_978173',\n",
       "  'title': None,\n",
       "  'text': \"def q_y_ax ( self ) : with tf . variable_scope ( 'q_y_ax' ) : x_to_qy_l = Layers ( self . x_l ) x_to_qy_l . fc ( 500 , activation_fn = None ) q_y_ax_l = Layers ( x_to_qy_l . get_output ( ) + self . samples [ 'qa_l' ] ) q_y_ax_l . fc ( 500 ) . fc ( 500 ) qy_l = MultinomialLayer ( q_y_ax_l . get_output ( ) , num_latent = 100 , eq_samples = 10 ) self . samples [ 'qy_l' ] = qy_l . compute_samples ( ) with tf . variable_scope ( 'q_y_ax' , reuse = True ) : x_to_qy_u = Layers ( self . x_u_rep ) x_to_qy_u . fc ( 500 , activation_fn = None ) q_y_ax_u = Layers ( x_to_qy_u . get_output ( ) + self . samples [ 'qa_u' ] ) q_y_ax_u . fc ( 500 ) . fc ( 500 ) qy_u = MultinomialLayer ( q_y_ax_u . get_output ( ) , num_latent = 100 , eq_samples = 10 ) self . samples [ 'qy_u' ] = qy_u . compute_samples ( ) return qy_l , qy_u\",\n",
       "  'score': '80.56664',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_14206',\n",
       "  'title': None,\n",
       "  'text': 'def main ( ) : env = gym . make ( \"CartPole-v0\" ) act = deepq . learn ( env , network = \\'mlp\\' , lr = 1e-3 , total_timesteps = 100000 , buffer_size = 50000 , exploration_fraction = 0.1 , exploration_final_eps = 0.02 , print_freq = 10 , callback = callback ) print ( \"Saving model to cartpole_model.pkl\" ) act . save ( \"cartpole_model.pkl\" )',\n",
       "  'score': '80.55713',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_210197',\n",
       "  'title': None,\n",
       "  'text': 'def run ( self , verbose = False ) : self . prepare ( ) for x in self : if verbose : print ( self . _q_count )',\n",
       "  'score': '80.53608',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_256735',\n",
       "  'title': None,\n",
       "  'text': 'def create ( input_width , input_height , input_channels = 1 , output_dim = 512 , initial_std_dev = 0.4 , factorized_noise = True ) : def instantiate ( * * _ ) : return DoubleNoisyNatureCnn ( input_width = input_width , input_height = input_height , input_channels = input_channels , output_dim = output_dim , initial_std_dev = initial_std_dev , factorized_noise = factorized_noise ) return ModelFactory . generic ( instantiate )',\n",
       "  'score': '80.526474',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_257022',\n",
       "  'title': None,\n",
       "  'text': \"def run ( self ) : device = self . model_config . torch_device ( ) env = self . env_factory . instantiate_single ( preset = 'record' , seed = self . model_config . seed ) model = self . model_factory . instantiate ( action_space = env . action_space ) . to ( device ) training_info = TrainingInfo ( start_epoch_idx = self . storage . last_epoch_idx ( ) , run_name = self . model_config . run_name ) model_state , hidden_state = self . storage . load ( training_info ) model . load_state_dict ( model_state ) model . eval ( ) for i in range ( self . takes ) : self . record_take ( model , env , device , take_number = i + 1 )\",\n",
       "  'score': '80.52033',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_256762',\n",
       "  'title': None,\n",
       "  'text': 'def __init__ ( self , input_block : BackboneModel , backbone : LinearBackboneModel , action_space : gym . Space ) : super ( ) . __init__ ( ) self . action_space = action_space self . input_block = input_block self . backbone = backbone self . q_head = QHead ( input_dim = backbone . output_dim , action_space = action_space )',\n",
       "  'score': '80.49728',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_39897',\n",
       "  'title': None,\n",
       "  'text': 'def build_model ( class_num ) : model = Sequential ( ) model . add ( Reshape ( [ 1 , 28 , 28 ] ) ) model . add ( SpatialConvolution ( 1 , 6 , 5 , 5 ) ) model . add ( Tanh ( ) ) model . add ( SpatialMaxPooling ( 2 , 2 , 2 , 2 ) ) model . add ( SpatialConvolution ( 6 , 12 , 5 , 5 ) ) model . add ( Tanh ( ) ) model . add ( SpatialMaxPooling ( 2 , 2 , 2 , 2 ) ) model . add ( Reshape ( [ 12 * 4 * 4 ] ) ) model . add ( Linear ( 12 * 4 * 4 , 100 ) ) model . add ( Tanh ( ) ) model . add ( Linear ( 100 , class_num ) ) model . add ( LogSoftMax ( ) ) return model',\n",
       "  'score': '80.4907',\n",
       "  'has_answer': True},\n",
       " {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_23956',\n",
       "  'title': None,\n",
       "  'text': 'def _env_runner ( base_env , extra_batch_callback , policies , policy_mapping_fn , unroll_length , horizon , preprocessors , obs_filters , clip_rewards , clip_actions , pack , callbacks , tf_sess , perf_stats , soft_horizon ) : try : if not horizon : horizon = ( base_env . get_unwrapped ( ) [ 0 ] . spec . max_episode_steps ) except Exception : logger . debug ( \"no episode horizon specified, assuming inf\" ) if not horizon : horizon = float ( \"inf\" ) # Pool of batch builders, which can be shared across episodes to pack # trajectory data. batch_builder_pool = [ ] def get_batch_builder ( ) : if batch_builder_pool : return batch_builder_pool . pop ( ) else : return MultiAgentSampleBatchBuilder ( policies , clip_rewards , callbacks . get ( \"on_postprocess_traj\" ) ) def new_episode ( ) : episode = MultiAgentEpisode ( policies , policy_mapping_fn , get_batch_builder , extra_batch_callback ) if callbacks . get ( \"on_episode_start\" ) : callbacks [ \"on_episode_start\" ] ( { \"env\" : base_env , \"policy\" : policies , \"episode\" : episode , } ) return episode active_episodes = defaultdict ( new_episode ) while True : perf_stats . iters += 1 t0 = time . time ( ) # Get observations from all ready agents unfiltered_obs , rewards , dones , infos , off_policy_actions = base_env . poll ( ) perf_stats . env_wait_time += time . time ( ) - t0 if log_once ( \"env_returns\" ) : logger . info ( \"Raw obs from env: {}\" . format ( summarize ( unfiltered_obs ) ) ) logger . info ( \"Info return from env: {}\" . format ( summarize ( infos ) ) ) # Process observations and prepare for policy evaluation t1 = time . time ( ) active_envs , to_eval , outputs = _process_observations ( base_env , policies , batch_builder_pool , active_episodes , unfiltered_obs , rewards , dones , infos , off_policy_actions , horizon , preprocessors , obs_filters , unroll_length , pack , callbacks , soft_horizon ) perf_stats . processing_time += time . time ( ) - t1 for o in outputs : yield o # Do batched policy eval t2 = time . time ( ) eval_results = _do_policy_eval ( tf_sess , to_eval , policies , active_episodes ) perf_stats . inference_time += time . time ( ) - t2 # Process results and update episode state t3 = time . time ( ) actions_to_send = _process_policy_eval_results ( to_eval , eval_results , active_episodes , active_envs , off_policy_actions , policies , clip_actions ) perf_stats . processing_time += time . time ( ) - t3 # Return computed actions to ready envs. We also send to envs that have # taken off-policy actions; those envs are free to ignore the action. t4 = time . time ( ) base_env . send_actions ( actions_to_send ) perf_stats . env_wait_time += time . time ( ) - t4',\n",
       "  'score': '80.48529',\n",
       "  'has_answer': True}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Find the closest xterm-256 approximation to the given RGB value.',\n",
       " 'answers': 'def rgb_to_short(rgb, mapping):\\n    \"\"\"Find the closest xterm-256 approximation to the given RGB value.\"\"\"\\n    # Thanks to Micah Elliott (http://MicahElliott.com) for colortrans.py\\n    rgb = rgb.lstrip(\\'#\\') if rgb.startswith(\\'#\\') else rgb\\n    incs = (0x00, 0x5f, 0x87, 0xaf, 0xd7, 0xff)\\n    # Break 6-char RGB code into 3 integer vals.\\n    parts = [int(h, 16) for h in re.split(r\\'(..)(..)(..)\\', rgb)[1:4]]\\n    res = []\\n    for part in parts:\\n        i = 0\\n        while i < len(incs)-1:\\n            s, b = incs[i], incs[i+1]  # smaller, bigger\\n            if s <= part <= b:\\n                s1 = abs(s - part)\\n                b1 = abs(b - part)\\n                if s1 < b1: closest = s\\n                else: closest = b\\n                res.append(closest)\\n                break\\n            i += 1\\n    res = \\'\\'.join([ (\\'%02.x\\' % i) for i in res ])\\n    equiv = mapping[res]\\n    return equiv, res',\n",
       " 'ctxs': [{'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_623279',\n",
       "   'title': None,\n",
       "   'text': 'def rgb_to_xterm ( r , g , b ) : if r < 5 and g < 5 and b < 5 : return 16 best_match = 0 smallest_distance = 10000000000 for c in range ( 16 , 256 ) : d = ( COLOR_TABLE [ c ] [ 0 ] - r ) ** 2 + ( COLOR_TABLE [ c ] [ 1 ] - g ) ** 2 + ( COLOR_TABLE [ c ] [ 2 ] - b ) ** 2 if d < smallest_distance : smallest_distance = d best_match = c return best_match',\n",
       "   'score': '88.41523',\n",
       "   'has_answer': True},\n",
       "  {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_453505',\n",
       "   'title': None,\n",
       "   'text': 'def _closest_color ( self , r , g , b ) : distance = 257 * 257 * 3 # \"infinity\" (>distance from #000000 to #ffffff) match = 0 for i in range ( 0 , 254 ) : values = self . xterm_colors [ i ] rd = r - values [ 0 ] gd = g - values [ 1 ] bd = b - values [ 2 ] d = rd * rd + gd * gd + bd * bd if d < distance : match = i distance = d return match',\n",
       "   'score': '87.88971',\n",
       "   'has_answer': True},\n",
       "  {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_955791',\n",
       "   'title': None,\n",
       "   'text': \"def find_nearest_color_hexstr ( hexdigits , color_table = None , method = 'euclid' ) : triplet = [ ] try : if len ( hexdigits ) == 3 : for digit in hexdigits : digit = int ( digit , 16 ) triplet . append ( ( digit * 16 ) + digit ) elif len ( hexdigits ) == 6 : triplet . extend ( int ( hexdigits [ i : i + 2 ] , 16 ) for i in ( 0 , 2 , 4 ) ) else : raise ValueError ( 'wrong length: %r' % hexdigits ) except ValueError : return None return find_nearest_color_index ( * triplet , color_table = color_table , method = method )\",\n",
       "   'score': '86.835846',\n",
       "   'has_answer': True},\n",
       "  {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_387671',\n",
       "   'title': None,\n",
       "   'text': 'def __missing__ ( self , value ) : r , g , b = value # Find closest color. # (Thanks to Pygments for this!) distance = 257 * 257 * 3 # \"infinity\" (>distance from #000000 to #ffffff) match = 0 for i , ( r2 , g2 , b2 ) in enumerate ( self . colors ) : d = ( r - r2 ) ** 2 + ( g - g2 ) ** 2 + ( b - b2 ) ** 2 if d < distance : match = i distance = d # Turn color name into code. self [ value ] = match return match',\n",
       "   'score': '84.70274',\n",
       "   'has_answer': True},\n",
       "  {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_638209',\n",
       "   'title': None,\n",
       "   'text': \"def rgb2termhex ( r : int , g : int , b : int ) -> str : incs = [ 0x00 , 0x5f , 0x87 , 0xaf , 0xd7 , 0xff ] res = [ ] parts = r , g , b for part in parts : if ( part < 0 ) or ( part > 255 ) : raise ValueError ( 'Expecting 0-255 for RGB code, got: {!r}' . format ( parts ) ) i = 0 while i < len ( incs ) - 1 : s , b = incs [ i ] , incs [ i + 1 ] # smaller, bigger if s <= part <= b : s1 = abs ( s - part ) b1 = abs ( b - part ) if s1 < b1 : closest = s else : closest = b res . append ( closest ) break i += 1 # Convert back into nearest hex value. return rgb2hex ( * res )\",\n",
       "   'score': '84.54327',\n",
       "   'has_answer': True},\n",
       "  {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_407540',\n",
       "   'title': None,\n",
       "   'text': 'def get_component ( self , colour , tolerance = 0 , default = None ) : if not ( 0 <= tolerance <= np . sqrt ( 195075 ) ) : raise LegendError ( \\'Tolerance must be between 0 and 441.67\\' ) for decor in self . __list : if colour . lower ( ) == decor . colour : return decor . component # If we\\'re here, we didn\\'t find one yet. r1 , g1 , b1 = utils . hex_to_rgb ( colour ) # Start with a best match of black. best_match = \\'#000000\\' best_match_dist = np . sqrt ( r1 ** 2. + g1 ** 2. + b1 ** 2. ) # Now compare to each colour in the legend. for decor in self . __list : r2 , g2 , b2 = decor . rgb distance = np . sqrt ( ( r2 - r1 ) ** 2. + ( g2 - g1 ) ** 2. + ( b2 - b1 ) ** 2. ) if distance < best_match_dist : best_match = decor . component best_match_dist = distance best_match_colour = decor . colour if best_match_dist <= tolerance : return best_match else : with warnings . catch_warnings ( ) : warnings . simplefilter ( \"always\" ) w = \"No match found for {0} \" . format ( colour . lower ( ) ) w += \"with tolerance of {0}. Best match is \" . format ( tolerance ) w += \"{0}, {1}\" . format ( best_match . summary ( ) , best_match_colour ) w += \", d={0}\" . format ( best_match_dist ) warnings . warn ( w ) return default',\n",
       "   'score': '84.34134',\n",
       "   'has_answer': True},\n",
       "  {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_453506',\n",
       "   'title': None,\n",
       "   'text': 'def _color_index ( self , color ) : index = self . best_match . get ( color , None ) if color in ansicolors : # strip the `#ansi` part and look up code index = color self . best_match [ color ] = index if index is None : try : rgb = int ( str ( color ) , 16 ) except ValueError : rgb = 0 r = ( rgb >> 16 ) & 0xff g = ( rgb >> 8 ) & 0xff b = rgb & 0xff index = self . _closest_color ( r , g , b ) self . best_match [ color ] = index return index',\n",
       "   'score': '84.1149',\n",
       "   'has_answer': True},\n",
       "  {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_638165',\n",
       "   'title': None,\n",
       "   'text': 'def hex ( self , value , text = None , back = None , style = None , rgb_mode = False ) : if rgb_mode : try : colrval = hex2rgb ( value , allow_short = True ) except ValueError : raise InvalidColr ( value ) else : try : colrval = hex2term ( value , allow_short = True ) except ValueError : raise InvalidColr ( value ) return self . chained ( text = text , fore = colrval , back = back , style = style )',\n",
       "   'score': '82.91644',\n",
       "   'has_answer': True},\n",
       "  {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_623278',\n",
       "   'title': None,\n",
       "   'text': 'def xterm_to_rgb ( xcolor ) : assert 0 <= xcolor <= 255 if xcolor < 16 : # basic colors return BASIC16 [ xcolor ] elif 16 <= xcolor <= 231 : # color cube xcolor -= 16 return ( CUBE_STEPS [ xcolor // 36 % 6 ] , CUBE_STEPS [ xcolor // 6 % 6 ] , CUBE_STEPS [ xcolor % 6 ] ) elif 232 <= xcolor <= 255 : # gray tone c = 8 + ( xcolor - 232 ) * 0x0A return ( c , c , c )',\n",
       "   'score': '82.78706',\n",
       "   'has_answer': True},\n",
       "  {'id': '/mnt/new_volume/database/python_dedupe_definitions_v2-001.pkl_333094',\n",
       "   'title': None,\n",
       "   'text': 'def _color_from_rgb ( rgb ) : # Returns the index of a color matching the 888 RGB color \\'rgb\\'. The # returned color might be an ~exact match or an approximation, depending on # terminal capabilities. # Calculates the Euclidean distance between two RGB colors def dist ( r1 , r2 ) : return sum ( ( x - y ) ** 2 for x , y in zip ( r1 , r2 ) ) if curses . COLORS >= 256 : # Assume we\\'re dealing with xterm\\'s 256-color extension if curses . can_change_color ( ) : # Best case -- the terminal supports changing palette entries via # curses.init_color(). Initialize an unused palette entry and # return it. return _alloc_rgb ( rgb ) # Second best case -- pick between the xterm 256-color extension colors # Closest 6-cube \"color\" color c6 = _rgb_to_6cube ( rgb ) # Closest gray color gray = _rgb_to_gray ( rgb ) if dist ( rgb , _6cube_to_rgb ( c6 ) ) < dist ( rgb , _gray_to_rgb ( gray ) ) : # Use the \"color\" color from the 6x6x6 color palette. Calculate the # color number from the 6-cube index triplet. return 16 + 36 * c6 [ 0 ] + 6 * c6 [ 1 ] + c6 [ 2 ] # Use the color from the gray palette return 232 + gray # Terminal not in xterm 256-color mode. This is probably the best we can # do, or is it? Submit patches. :) min_dist = float ( \\'inf\\' ) best = - 1 for color in range ( curses . COLORS ) : # ncurses uses the range 0..1000. Scale that down to 0..255. d = dist ( rgb , tuple ( int ( round ( 255 * c / 1000 ) ) for c in curses . color_content ( color ) ) ) if d < min_dist : min_dist = d best = color return best',\n",
       "   'score': '82.614685',\n",
       "   'has_answer': True}]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"/mnt/new_volume/codesearchnet/codesearchnet_train_filtered_full.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(dir, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ret",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
