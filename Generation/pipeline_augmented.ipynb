{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell loads the dataset\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "top_k = 40\n",
    "instances_num = 1000\n",
    "text_prompt_all = [] # list of instances_num code prompts\n",
    "codes_ground_truth_all = [] # list of instances_num code strings\n",
    "scores_raw_all = np.zeros((instances_num, top_k))\n",
    "codes_retrieved_all = [] # list of instances_num lists, each list has top_k items\n",
    "row = 0\n",
    "with open(\"/home/ubuntu/fancy_retriever/output_ret/train1000_codexglue.json\") as f:\n",
    "        data = json.load(f)\n",
    "        for i in range(len(data)):\n",
    "            text_prompt_all.append(data[i][\"question\"])\n",
    "            codes_ground_truth_all.append(data[i][\"answers\"])\n",
    "            scores_raw_all[row] = np.array([data[i][\"ctxs\"][j][\"score\"] for j in range(top_k)])\n",
    "            codes_retrieved_all.append([data[i][\"ctxs\"][j][\"text\"] for j in range(top_k)])\n",
    "            row += 1\n",
    "\n",
    "# data has 1000 items, \n",
    "# for each item \"data[i]\" i in [0, 999], there are associated \"question\" (code prompt, str), \"answers\" (ground truth code, str), and a \"ctxs\"\n",
    "# each \"ctxs\" is a list of dictionaries \"data[i][\"ctxs\"][j]\" j in [0, 199], each dictionary has 5 keys:\n",
    "# ['id' (path, str), 'title' (str), 'text' (retrieved code, str), 'score' (similarity, float), 'has_answer' (bool)]\n",
    "\n",
    "# check for json file loading\n",
    "# print(0 in scores_raw_all) # False\n",
    "# for i in range(len(codes_retrieved_all)): assert len(codes_retrieved_all[i]) == top_k\n",
    "assert row == len(text_prompt_all) == len(codes_ground_truth_all) == len(codes_retrieved_all) == instances_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text prompt classification into easy, medium, hard\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define variables that should persist between runs\n",
    "if 'tokenizer' not in globals() or 'model' not in globals():\n",
    "    print(\"initializing tokenizer and mode...l\")\n",
    "    # model_name = \"meta-llama/Llama-3.1-8B-Instruct\" # Llama\n",
    "    # model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\" # DSR1qwen\n",
    "    model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\" # Qwen1M: top5; Qwen1Mlong: top20\n",
    "    # model_name = \"ministral/Ministral-3b-instruct\" # mst3b\n",
    "    hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\") # or insert your own huggingface access token here\n",
    "    if hf_token is None:\n",
    "        raise ValueError(\"Please set your HUGGINGFACE_HUB_TOKEN environment variable.\")\n",
    "\n",
    "    # Load tokenizer and model only once\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "# Now iterate over your data\n",
    "results = []\n",
    "\n",
    "instances_num = 200\n",
    "for i in tqdm(range(instances_num)):\n",
    "    text_prompt = text_prompt_all[i]\n",
    "    augmented_prompt = f\"Given the problem '{text_prompt}', please classify this given problem into easy, medium, or hard problem.\\n\"\n",
    "    augmented_prompt += \"When you are doing classification, you should consider that 60% of problems are easy, 35% of problems are medium, and 10% of problems are hard.\\n\"\n",
    "    augmented_prompt += \"You may refer to the following examples and their classification levels:\\n\"\n",
    "    augmented_prompt += \"easy problem: Determine color-category mapping. If color_column was specified, then map the category names to color values. Otherwise, use the palettable colors to automatically generate a set of colors for the group values.\\n\"\n",
    "    augmented_prompt += \"easy problem: Return a df with predictions and confidence interval.\\n\"\n",
    "    augmented_prompt += \"medium problem: convert fasta to phylip because RAxML is ridiculous.\\n\"\n",
    "    augmented_prompt += \"medium problem: scale table based on the column with the largest sum.\\n\"\n",
    "    augmented_prompt += \"hard problem: log processors that structlog executes before final rendering.\\n\"\n",
    "    augmented_prompt += \"easy problem: Call method to perform any setup.\\n\"\n",
    "    augmented_prompt += \"easy problem: get top hits after sorting by column number.\\n\"\n",
    "    augmented_prompt += \"medium problem: the final log processor that structlog requires to render.\\n\"\n",
    "    augmented_prompt += \"medium problem: execute jobs in processes using N threads.\\n\"\n",
    "    augmented_prompt += \"Your answer should only be the classification made for the given problem. For the given proble, you should only generate one word, which is either 'easy', or 'medium', or 'hard'.\\n\"\n",
    "    ori_prompt = augmented_prompt\n",
    "    \n",
    "    # Tokenize with truncation\n",
    "    # tokenized = tokenizer(augmented_prompt, return_tensors=\"pt\", truncation=True, max_length=max_context_length)\n",
    "    tokenized = tokenizer(augmented_prompt, return_tensors=\"pt\")\n",
    "    input_length = tokenized.input_ids.size(1)\n",
    "\n",
    "    # Set up the generation pipeline\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    output = generator(\n",
    "        augmented_prompt,\n",
    "        max_new_tokens=16, # Set a reasonable generation limit\n",
    "        temperature=0.1,             # Lower temperature for less randomness\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1,    # Generate one sequence\n",
    "    )\n",
    "\n",
    "    generated_text = output[0][\"generated_text\"]\n",
    "    generated_text = generated_text.split(ori_prompt)[1]\n",
    "    # print(\"Generated Text:\\n\", generated_text)\n",
    "    if \"easy\" in generated_text: \n",
    "        dynamic_k = 5\n",
    "    elif \"medium\" in generated_text: \n",
    "        dynamic_k = 20\n",
    "    elif \"hard\" in generated_text: \n",
    "        dynamic_k = 40\n",
    "    results.append({\n",
    "        \"text_prompt\": text_prompt,\n",
    "        \"classification\": generated_text,\n",
    "        \"top_k\": dynamic_k\n",
    "    })\n",
    "\n",
    "# Save the first 200 pairs in a JSON file.\n",
    "with open(\"/home/ubuntu/fancy_retriever/Generation/classification_Qwen1M.json\", \"w\") as f: json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the classification json file\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "acc = 0\n",
    "ground_truth_k = np.load(\"/home/ubuntu/fancy_retriever/Generation/converge.npy\")\n",
    "with open(\"/home/ubuntu/fancy_retriever/Generation/classification_Qwen1M_new.json\") as f:\n",
    "        data = json.load(f)\n",
    "        for i in range(len(data)):\n",
    "            dynamic_k = data[i][\"top_k\"]\n",
    "            if ground_truth_k[i] <= 1 and dynamic_k == 5:\n",
    "                acc += 1\n",
    "            elif ground_truth_k[i] >= 2 and ground_truth_k[i] <= 3 and dynamic_k == 20:\n",
    "                acc += 1\n",
    "            elif ground_truth_k[i] == 4 and dynamic_k == 40:\n",
    "                acc += 1\n",
    "print(\"lite LLM text prompt classification accuracy: \", acc/instances_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCODE_G code generation re-implementation augmented with dynamic top_k retrieved examples\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Determine whether to use a dynamic amount of retrieved code instances\n",
    "Dynamic = True\n",
    "if Dynamic:\n",
    "    # Load the classification results\n",
    "    with open(\"/home/ubuntu/fancy_retriever/Generation/classification_Qwen1M.json\") as f:\n",
    "        data = json.load(f)\n",
    "        # Extract the top_k values from the classification results\n",
    "        top_k_all = [item[\"top_k\"] for item in data]\n",
    "\n",
    "# Define variables that should persist between runs\n",
    "if 'tokenizer' not in globals() or 'model' not in globals():\n",
    "    # model_name = \"meta-llama/Llama-3.1-8B-Instruct\" # Llama\n",
    "    # model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\" # DSR1qwen\n",
    "    model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\" # Qwen1M: top5; Qwen1Mlong: top20\n",
    "    hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\") # or insert your own huggingface access token here\n",
    "    if hf_token is None:\n",
    "        raise ValueError(\"Please set your HUGGINGFACE_HUB_TOKEN environment variable.\")\n",
    "\n",
    "    # Load tokenizer and model only once\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Use the additional_special_tokens attribute to check if our special token is already added.\n",
    "    special_token = \"<|retrieved|>\"\n",
    "    if special_token not in tokenizer.additional_special_tokens:\n",
    "        # Record the original size and then add exactly one token.\n",
    "        original_vocab_size = len(tokenizer)\n",
    "        tokenizer.add_special_tokens({\"additional_special_tokens\": [special_token]})\n",
    "        model.resize_token_embeddings(original_vocab_size + 1)\n",
    "\n",
    "# Now iterate over your data\n",
    "results = []\n",
    "max_context_length = 4096\n",
    "\n",
    "instances_num = 200\n",
    "for i in tqdm(range(instances_num)):\n",
    "    text_prompt = text_prompt_all[i]       # Your i-th natural language prompt (str)\n",
    "    retrieved_examples = codes_retrieved_all[i]   # The i-th list of code strings\n",
    "    \n",
    "    delimiter = f\"\\n{special_token}\\n\"\n",
    "    \n",
    "    # augmented_prompt = \"\"\"\"\n",
    "    # def is_prime(n):\n",
    "    # if n <= 1:\n",
    "    #     return False\n",
    "    # for i in range(2, int(n**0.5) + 1):\n",
    "    # \"\"\"\n",
    "    # augmented_prompt = \"Write a Python function that checks if a number is prime.\"\n",
    "\n",
    "    augmented_prompt = f\"Write a python function that {text_prompt} based on the following examples:\\n\"\n",
    "    if Dynamic: # only use the dynamic_k retrieved examples\n",
    "        dynamic_k = top_k_all[i]\n",
    "        retrieved_examples = retrieved_examples[:dynamic_k]\n",
    "    # Concatenate all retrieved examples using the delimiter.\n",
    "    augmented_prompt += delimiter.join(retrieved_examples)\n",
    "    # Concatenate\n",
    "    augmented_prompt += f\"\"\"\\n{special_token}\n",
    "    #Requirements:\n",
    "    1. Please do not repeat the above code snippets\n",
    "    2. Please directly generated the code without any explanation or documnetation\n",
    "    3. Please do not add any comments\n",
    "    4. Please do not add any additional text\n",
    "    5. Please do not import any libraries\n",
    "    Here is your turn.\\n\n",
    "    \"\"\"\n",
    "    \n",
    "    ori_prompt = augmented_prompt\n",
    "\n",
    "    augmented_prompt += \"```python \\n\"\n",
    "    \n",
    "    # Tokenize with truncation\n",
    "    # tokenized = tokenizer(augmented_prompt, return_tensors=\"pt\", truncation=True, max_length=max_context_length)\n",
    "    tokenized = tokenizer(augmented_prompt, return_tensors=\"pt\")\n",
    "    input_length = tokenized.input_ids.size(1)\n",
    "    # print(f\"Input Length: {input_length}\")\n",
    "    allowed_new_tokens = max_context_length - input_length\n",
    "    if allowed_new_tokens < 1:\n",
    "        allowed_new_tokens = 1  # safeguard\n",
    "\n",
    "    # Set up the generation pipeline\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    output = generator(\n",
    "        augmented_prompt,\n",
    "        max_new_tokens=256, # Set a reasonable generation limit\n",
    "        temperature=0.1,             # Lower temperature for less randomness\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1,    # Generate one sequence\n",
    "    )\n",
    "\n",
    "    generated_text = output[0][\"generated_text\"]\n",
    "    generated_text = generated_text.split(ori_prompt)[1]\n",
    "    # print(\"Generated Text:\\n\", generated_text)\n",
    "    results.append({\n",
    "        \"text_prompt\": text_prompt,\n",
    "        \"ground_truth_code\": codes_ground_truth_all[i],\n",
    "        \"generated_code\": generated_text\n",
    "    })\n",
    "\n",
    "# Save the first 200 pairs in a JSON file.\n",
    "with open(\"/home/ubuntu/fancy_retriever/Generation/cxg200_Qwen1M_dynamic.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCODE_G code generation re-implementation augmented with top_k retrieved examples\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Determine whether to use a dynamic amount of retrieved code instances\n",
    "Dynamic = False\n",
    "if Dynamic:\n",
    "    # Load the classification results\n",
    "    with open(\"/home/ubuntu/fancy_retriever/Generation/classification_Qwen1M.json\") as f:\n",
    "        data = json.load(f)\n",
    "        # Extract the top_k values from the classification results\n",
    "        top_k_all = [item[\"top_k\"] for item in data]\n",
    "\n",
    "# Define variables that should persist between runs\n",
    "if 'tokenizer' not in globals() or 'model' not in globals():\n",
    "    # model_name = \"meta-llama/Llama-3.1-8B-Instruct\" # Llama\n",
    "    # model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\" # DSR1qwen\n",
    "    model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\" # Qwen1M: top5; Qwen1Mlong: top20\n",
    "    hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\") # or insert your own huggingface access token here\n",
    "    if hf_token is None:\n",
    "        raise ValueError(\"Please set your HUGGINGFACE_HUB_TOKEN environment variable.\")\n",
    "\n",
    "    # Load tokenizer and model only once\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Use the additional_special_tokens attribute to check if our special token is already added.\n",
    "    special_token = \"<|retrieved|>\"\n",
    "    if special_token not in tokenizer.additional_special_tokens:\n",
    "        # Record the original size and then add exactly one token.\n",
    "        original_vocab_size = len(tokenizer)\n",
    "        tokenizer.add_special_tokens({\"additional_special_tokens\": [special_token]})\n",
    "        model.resize_token_embeddings(original_vocab_size + 1)\n",
    "\n",
    "# Now iterate over your data\n",
    "results = []\n",
    "max_context_length = 4096\n",
    "\n",
    "instances_num = 200\n",
    "for i in tqdm(range(instances_num)):\n",
    "    text_prompt = text_prompt_all[i]       # Your i-th natural language prompt (str)\n",
    "    retrieved_examples = codes_retrieved_all[i]   # The i-th list of code strings\n",
    "    \n",
    "    delimiter = f\"\\n{special_token}\\n\"\n",
    "    \n",
    "    # augmented_prompt = \"\"\"\"\n",
    "    # def is_prime(n):\n",
    "    # if n <= 1:\n",
    "    #     return False\n",
    "    # for i in range(2, int(n**0.5) + 1):\n",
    "    # \"\"\"\n",
    "    # augmented_prompt = \"Write a Python function that checks if a number is prime.\"\n",
    "\n",
    "    augmented_prompt = f\"Write a python function that {text_prompt} based on the following examples:\\n\"\n",
    "    if Dynamic: # only use the dynamic_k retrieved examples\n",
    "        dynamic_k = top_k_all[i]\n",
    "        retrieved_examples = retrieved_examples[:dynamic_k]\n",
    "    # Concatenate all retrieved examples using the delimiter.\n",
    "    augmented_prompt += delimiter.join(retrieved_examples)\n",
    "    # Concatenate\n",
    "    augmented_prompt += f\"\"\"\\n{special_token}\n",
    "    #Requirements:\n",
    "    1. Please do not repeat the above code snippets\n",
    "    2. Please directly generated the code without any explanation or documnetation\n",
    "    3. Please do not add any comments\n",
    "    4. Please do not add any additional text\n",
    "    5. Please do not import any libraries\n",
    "    Here is your turn.\\n\n",
    "    \"\"\"\n",
    "    \n",
    "    ori_prompt = augmented_prompt\n",
    "\n",
    "    augmented_prompt += \"```python \\n\"\n",
    "    \n",
    "    # Tokenize with truncation\n",
    "    # tokenized = tokenizer(augmented_prompt, return_tensors=\"pt\", truncation=True, max_length=max_context_length)\n",
    "    tokenized = tokenizer(augmented_prompt, return_tensors=\"pt\")\n",
    "    input_length = tokenized.input_ids.size(1)\n",
    "    # print(f\"Input Length: {input_length}\")\n",
    "    allowed_new_tokens = max_context_length - input_length\n",
    "    if allowed_new_tokens < 1:\n",
    "        allowed_new_tokens = 1  # safeguard\n",
    "\n",
    "    # Set up the generation pipeline\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    output = generator(\n",
    "        augmented_prompt,\n",
    "        max_new_tokens=256, # Set a reasonable generation limit\n",
    "        temperature=0.1,             # Lower temperature for less randomness\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1,    # Generate one sequence\n",
    "    )\n",
    "\n",
    "    generated_text = output[0][\"generated_text\"]\n",
    "    generated_text = generated_text.split(ori_prompt)[1]\n",
    "    # print(\"Generated Text:\\n\", generated_text)\n",
    "    results.append({\n",
    "        \"text_prompt\": text_prompt,\n",
    "        \"ground_truth_code\": codes_ground_truth_all[i],\n",
    "        \"generated_code\": generated_text\n",
    "    })\n",
    "\n",
    "# Save the first 200 pairs in a JSON file.\n",
    "with open(\"/home/ubuntu/fancy_retriever/Generation/cxg200_Qwen1M_top40.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCODE_G code generation re-implementation augmented with top_0 retrieved examples\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define variables that should persist between runs\n",
    "if 'tokenizer' not in globals() or 'model' not in globals():\n",
    "    # model_name = \"meta-llama/Llama-3.1-8B-Instruct\" # Llama\n",
    "    # model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\" # DSR1qwen\n",
    "    model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\" # Qwen1M: top5; Qwen1Mlong: top20\n",
    "    hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\") # or insert your own huggingface access token here\n",
    "    if hf_token is None:\n",
    "        raise ValueError(\"Please set your HUGGINGFACE_HUB_TOKEN environment variable.\")\n",
    "\n",
    "    # Load tokenizer and model only once\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Use the additional_special_tokens attribute to check if our special token is already added.\n",
    "    special_token = \"<|retrieved|>\"\n",
    "    if special_token not in tokenizer.additional_special_tokens:\n",
    "        # Record the original size and then add exactly one token.\n",
    "        original_vocab_size = len(tokenizer)\n",
    "        tokenizer.add_special_tokens({\"additional_special_tokens\": [special_token]})\n",
    "        model.resize_token_embeddings(original_vocab_size + 1)\n",
    "\n",
    "# Now iterate over your data\n",
    "results = []\n",
    "max_context_length = 4096\n",
    "\n",
    "instances_num = 200\n",
    "for i in tqdm(range(instances_num)):\n",
    "    text_prompt = text_prompt_all[i]       # Your i-th natural language prompt (str)\n",
    "    retrieved_examples = codes_retrieved_all[i]   # The i-th list of code strings\n",
    "    \n",
    "    delimiter = f\"\\n{special_token}\\n\"\n",
    "    \n",
    "    # augmented_prompt = \"\"\"\"\n",
    "    # def is_prime(n):\n",
    "    # if n <= 1:\n",
    "    #     return False\n",
    "    # for i in range(2, int(n**0.5) + 1):\n",
    "    # \"\"\"\n",
    "    # augmented_prompt = \"Write a Python function that checks if a number is prime.\"\n",
    "    \"\"\"\n",
    "    augmented_prompt = f\"Write a python function that {text_prompt} based on the following examples:\\n\"\n",
    "    # Concatenate all retrieved examples using the delimiter.\n",
    "    augmented_prompt += delimiter.join(retrieved_examples)\n",
    "    augmented_prompt += f\\n{special_token}\n",
    "    #Requirements:\n",
    "    1. Please do not repeat the above code snippets\n",
    "    2. Please directly generated the code without any explanation or documnetation\n",
    "    3. Please do not add any comments\n",
    "    4. Please do not add any additional text\n",
    "    5. Please do not import any libraries\n",
    "    Here is your turn.\\n\n",
    "    \"\"\"\n",
    "    augmented_prompt = f\"Write a python function that {text_prompt}\\n\"\n",
    "    ori_prompt = augmented_prompt\n",
    "\n",
    "    augmented_prompt += \"```python \\n\"\n",
    "    \n",
    "    # Tokenize with truncation\n",
    "    # tokenized = tokenizer(augmented_prompt, return_tensors=\"pt\", truncation=True, max_length=max_context_length)\n",
    "    tokenized = tokenizer(augmented_prompt, return_tensors=\"pt\")\n",
    "    input_length = tokenized.input_ids.size(1)\n",
    "    # print(f\"Input Length: {input_length}\")\n",
    "    allowed_new_tokens = max_context_length - input_length\n",
    "    if allowed_new_tokens < 1:\n",
    "        allowed_new_tokens = 1  # safeguard\n",
    "\n",
    "    # Set up the generation pipeline\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    output = generator(\n",
    "        augmented_prompt,\n",
    "        max_new_tokens=256, # Set a reasonable generation limit\n",
    "        temperature=0.1,             # Lower temperature for less randomness\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1,    # Generate one sequence\n",
    "    )\n",
    "\n",
    "    generated_text = output[0][\"generated_text\"]\n",
    "    generated_text = generated_text.split(ori_prompt)[1]\n",
    "    # print(\"Generated Text:\\n\", generated_text)\n",
    "    results.append({\n",
    "        \"text_prompt\": text_prompt,\n",
    "        \"ground_truth_code\": codes_ground_truth_all[i],\n",
    "        \"generated_code\": generated_text\n",
    "    })\n",
    "\n",
    "# Save the first 200 pairs in a JSON file.\n",
    "with open(\"/home/ubuntu/fancy_retriever/Generation/cxg200_Qwen1M_top0.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeLlama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
