{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell loads the Code_RAG_Bench dataset (crb)\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "top_k = 5\n",
    "instances_num = 500\n",
    "text_prompt_all = [] # list of instances_num code prompts\n",
    "codes_ground_truth_all = [] # list of instances_num code strings\n",
    "scores_raw_all = np.zeros((instances_num, top_k))\n",
    "codes_retrieved_all = [] # list of instances_num lists, each list has top_k items\n",
    "row = 0\n",
    "with open(\"/home/ubuntu/fancy_retriever/output_ret/coderagbench_500.json\") as f:\n",
    "        data = json.load(f)\n",
    "        for i in range(len(data)):\n",
    "            text_prompt_all.append(data[i][\"question\"])\n",
    "            codes_ground_truth_all.append(data[i][\"answers\"])\n",
    "            scores_raw_all[row] = np.array([data[i][\"ctxs\"][j][\"score\"] for j in range(top_k)])\n",
    "            codes_retrieved_all.append([data[i][\"ctxs\"][j][\"text\"] for j in range(top_k)])\n",
    "            row += 1\n",
    "\n",
    "# data has 1000 items, \n",
    "# for each item \"data[i]\" i in [0, 999], there are associated \"question\" (code prompt, str), \"answers\" (ground truth code, str), and a \"ctxs\"\n",
    "# each \"ctxs\" is a list of dictionaries \"data[i][\"ctxs\"][j]\" j in [0, 199], each dictionary has 5 keys:\n",
    "# ['id' (path, str), 'title' (str), 'text' (retrieved code, str), 'score' (similarity, float), 'has_answer' (bool)]\n",
    "\n",
    "# check for json file loading\n",
    "# print(0 in scores_raw_all) # False\n",
    "# for i in range(len(codes_retrieved_all)): assert len(codes_retrieved_all[i]) == top_k\n",
    "assert row == len(text_prompt_all) == len(codes_ground_truth_all) == len(codes_retrieved_all) == instances_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell loads the Code_Search_Net subset from the CodeXGLUE dataset (csn)\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "top_k = 5\n",
    "instances_num = 23107\n",
    "text_prompt_all = [] # list of instances_num code prompts\n",
    "codes_ground_truth_all = [] # list of instances_num code strings\n",
    "scores_raw_all = np.zeros((instances_num, top_k))\n",
    "codes_retrieved_all = [] # list of instances_num lists, each list has top_k items\n",
    "row = 0\n",
    "for file in range(1, 25):\n",
    "    with open(\"/home/ubuntu/fancy_retriever/output_ret/retrieval_chunk_\" + str(file) + \"_top200_result.json\") as f:\n",
    "        data = json.load(f)\n",
    "        for i in range(len(data)):\n",
    "            text_prompt_all.append(data[i][\"question\"])\n",
    "            codes_ground_truth_all.append(data[i][\"answers\"])\n",
    "            scores_raw_all[row] = np.array([data[i][\"ctxs\"][j][\"score\"] for j in range(top_k)])\n",
    "            codes_retrieved_all.append([data[i][\"ctxs\"][j][\"text\"] for j in range(top_k)])\n",
    "            row += 1\n",
    "\n",
    "# data has 1000 items, \n",
    "# for each item \"data[i]\" i in [0, 999], there are associated \"question\" (code prompt, str), \"answers\" (ground truth code, str), and a \"ctxs\"\n",
    "# each \"ctxs\" is a list of dictionaries \"data[i][\"ctxs\"][j]\" j in [0, 199], each dictionary has 5 keys:\n",
    "# ['id' (path, str), 'title' (str), 'text' (retrieved code, str), 'score' (similarity, float), 'has_answer' (bool)]\n",
    "\n",
    "# check for json file loading\n",
    "# print(0 in scores_raw_all) # False\n",
    "# for i in range(len(codes_retrieved_all)): assert len(codes_retrieved_all[i]) == top_k\n",
    "assert row == len(text_prompt_all) == len(codes_ground_truth_all) == len(codes_retrieved_all) == instances_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCODE_G code generation re-implementation\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define variables that should persist between runs\n",
    "if 'tokenizer' not in globals() or 'model' not in globals():\n",
    "    model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\") # or insert your own huggingface access token here\n",
    "    if hf_token is None:\n",
    "        raise ValueError(\"Please set your HUGGINGFACE_HUB_TOKEN environment variable.\")\n",
    "\n",
    "    # Load tokenizer and model only once\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=hf_token,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Use the additional_special_tokens attribute to check if our special token is already added.\n",
    "    special_token = \"<|retrieved|>\"\n",
    "    if special_token not in tokenizer.additional_special_tokens:\n",
    "        # Record the original size and then add exactly one token.\n",
    "        original_vocab_size = len(tokenizer)\n",
    "        tokenizer.add_special_tokens({\"additional_special_tokens\": [special_token]})\n",
    "        model.resize_token_embeddings(original_vocab_size + 1)\n",
    "\n",
    "# Now iterate over your data\n",
    "results = []\n",
    "max_context_length = 4096\n",
    "\n",
    "for i in tqdm(range(instances_num)):\n",
    "    text_prompt = text_prompt_all[i]       # Your i-th natural language prompt (str)\n",
    "    retrieved_examples = codes_retrieved_all[i]   # The i-th list of code strings\n",
    "    \n",
    "    delimiter = f\"\\n{special_token}\\n\"\n",
    "    # augmented_prompt = \"\"\"\"\n",
    "    # def is_prime(n):\n",
    "    # if n <= 1:\n",
    "    #     return False\n",
    "    # for i in range(2, int(n**0.5) + 1):\n",
    "    # \"\"\"\n",
    "    # augmented_prompt = \"Write a Python function that checks if a number is prime.\"\n",
    "    augmented_prompt = f\"Write a python function that {text_prompt} based on the following examples:\\n\"\n",
    "    # Concatenate all retrieved examples using the delimiter.\n",
    "    augmented_prompt += delimiter.join(retrieved_examples)\n",
    "    augmented_prompt += f\"\"\"\\n{special_token}\n",
    "    #Requirements:\n",
    "    1. Please do not repeat the above code snippets\n",
    "    2. Please directly generated the code without any explanation or documnetation\n",
    "    3. Please do not add any comments\n",
    "    4. Please do not add any additional text\n",
    "    5. Please do not import any libraries\n",
    "    Here is your turn.\\n\n",
    "    \"\"\"\n",
    "\n",
    "    ori_prompt = augmented_prompt\n",
    "\n",
    "    augmented_prompt += \"```python \\n\"\n",
    "    \n",
    "    # Tokenize with truncation\n",
    "    tokenized = tokenizer(augmented_prompt, return_tensors=\"pt\", truncation=True, max_length=max_context_length)\n",
    "    input_length = tokenized.input_ids.size(1)\n",
    "    # print(f\"Input Length: {input_length}\")\n",
    "    allowed_new_tokens = max_context_length - input_length\n",
    "    if allowed_new_tokens < 1:\n",
    "        allowed_new_tokens = 1  # safeguard\n",
    "\n",
    "    # Set up the generation pipeline\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    output = generator(\n",
    "        augmented_prompt,\n",
    "        max_new_tokens=256, # Set a reasonable generation limit\n",
    "        temperature=0.1,             # Lower temperature for less randomness\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1,    # Generate one sequence\n",
    "    )\n",
    "\n",
    "    generated_text = output[0][\"generated_text\"]\n",
    "    generated_text = generated_text.split(ori_prompt)[1]\n",
    "    # print(\"Generated Text:\\n\", generated_text)\n",
    "    results.append({\n",
    "        \"text_prompt\": text_prompt,\n",
    "        \"generated_text\": generated_text\n",
    "    })\n",
    "\n",
    "# Save the first 100 pairs in a JSON file.\n",
    "with open(\"/home/ubuntu/fancy_retriever/Generation/csn_Llama31_8B_Instruct.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeLlama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
