{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, deque\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute group level code generation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM(codes_generated, codes_ground_truth):\n",
    "    \"\"\"\n",
    "    Compute exact match rate for the given instances.\n",
    "    \"\"\"\n",
    "    if codes_generated == codes_ground_truth:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def BLEU_corpus(codes_generated, codes_ground_truth):\n",
    "    \"\"\"\n",
    "    Compute corpus-level BLEU score for the given instances.\n",
    "    \"\"\"\n",
    "    all_candidates = []  # each will be a tokenized candidate code snippet\n",
    "    all_references = []  # each will be a list containing one tokenized reference code snippet\n",
    "    # Tokenize; this is a simplistic approach!\n",
    "    candidate_tokens = codes_generated.split()\n",
    "    reference_tokens = codes_ground_truth.split()\n",
    "    all_candidates.append(candidate_tokens)\n",
    "    # NLTKâ€™s corpus_bleu expects each reference to be a list-of-lists of tokens\n",
    "    # here we only have one reference (the ground truth)\n",
    "    all_references.append([reference_tokens])\n",
    "    # Calculate corpus-level BLEU\n",
    "    bleu_score = corpus_bleu(all_references, all_candidates, weights=(0.5, 0.5, 0, 0))\n",
    "    return bleu_score # [0, 1]\n",
    "\n",
    "def BLEU_sentence(codes_generated, codes_ground_truth):\n",
    "    \"\"\"\n",
    "    Compute sentence-level BLEU score for the given instances.\n",
    "    \"\"\"\n",
    "    def tokenize_code(code_str):\n",
    "        tokens = re.findall(r\"[A-Za-z0-9_]+|\\S\", code_str)\n",
    "        return tokens\n",
    "    smooth_fn = SmoothingFunction().method4  # method4 is a common smoothing method\n",
    "    \n",
    "    # Tokenize; this is a simplistic approach!\n",
    "    candidate_tokens = tokenize_code(codes_generated)\n",
    "    reference_tokens = tokenize_code(codes_ground_truth)\n",
    "    # In NLTK's BLEU, reference is a list-of-lists\n",
    "    # here we only have one reference (the ground truth)\n",
    "    references = [reference_tokens]\n",
    "    # Calculate *sentence-level* BLEU with 1 reference\n",
    "    score = sentence_bleu(\n",
    "        references,               # list of references\n",
    "        candidate_tokens,         # candidate tokens\n",
    "        smoothing_function=smooth_fn  # method4 is a common smoothing method\n",
    "    )\n",
    "    return score # [0, 1]\n",
    "\n",
    "##############################################################################\n",
    "# 1) Simple Code Tokenizer\n",
    "##############################################################################\n",
    "def tokenize_code(code_str):\n",
    "    \"\"\"\n",
    "    Splits code on boundaries between alphanumeric/underscore and non-alphanumeric.\n",
    "    E.g., \"def foo(x): return x+1\" -> [\"def\", \"foo\", \"(\", \"x\", \")\", \":\", \"return\", \"x\", \"+\", \"1\"]\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r\"[A-Za-z0-9_]+|\\S\", code_str)\n",
    "    return tokens\n",
    "\n",
    "##############################################################################\n",
    "# 2) N-Gram BLEU (token-based), approximate from-scratch\n",
    "##############################################################################\n",
    "def ngram_counts(tokens, n):\n",
    "    \"\"\"\n",
    "    Return a Counter of all n-grams of length `n` in `tokens`.\n",
    "    Example: tokens = ['def','foo','(',')']\n",
    "             n=2 -> Counter({('def','foo'):1, ('foo','('):1, ('(',')'):1})\n",
    "    \"\"\"\n",
    "    counts = Counter()\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[i:i+n])\n",
    "        counts[ngram] += 1\n",
    "    return counts\n",
    "\n",
    "def clipped_ngram_count(candidate_tokens, reference_tokens, n):\n",
    "    \"\"\"\n",
    "    Compute clipped count = sum of min(candidate_count(ngram), reference_count(ngram)) over all n-grams.\n",
    "    \"\"\"\n",
    "    cand_counter = ngram_counts(candidate_tokens, n)\n",
    "    ref_counter  = ngram_counts(reference_tokens, n)\n",
    "    clipped_sum = 0\n",
    "    for ngram, cand_val in cand_counter.items():\n",
    "        clipped_sum += min(cand_val, ref_counter.get(ngram, 0))\n",
    "    return clipped_sum\n",
    "\n",
    "def sentence_bleu_self(candidate_tokens, reference_tokens, max_n=4, smooth_eps=1e-14):\n",
    "    \"\"\"\n",
    "    Approximate sentence-level BLEU (no brevity penalty in this simplified version),\n",
    "    with basic smoothing to avoid zero counts.\n",
    "    \"\"\"\n",
    "    # Precision for each n-gram\n",
    "    precisions = []\n",
    "    for n in range(1, max_n+1):\n",
    "        numerator = clipped_ngram_count(candidate_tokens, reference_tokens, n)\n",
    "        denominator = max(len(candidate_tokens) - n + 1, 1)\n",
    "        # Add smoothing to avoid 0\n",
    "        prec_n = (numerator + smooth_eps) / (denominator + smooth_eps)\n",
    "        precisions.append(prec_n)\n",
    "\n",
    "    # Geometric mean of precisions\n",
    "    # BLEU = exp( (1/N) * sum(log p_n) ), N=4 typically\n",
    "    if any(p == 0 for p in precisions):\n",
    "        bleu = 0.0\n",
    "    else:\n",
    "        avg_log = sum(math.log(p) for p in precisions) / len(precisions)\n",
    "        bleu = math.exp(avg_log)\n",
    "\n",
    "    # Very naive length penalty if you want (optional).\n",
    "    # Let's skip or do a minimal ratio:\n",
    "    ratio = len(candidate_tokens) / (len(reference_tokens) + smooth_eps)\n",
    "    if ratio < 1.0:\n",
    "        # Simple brevity penalty\n",
    "        bleu *= math.exp(1 - 1/ratio)\n",
    "\n",
    "    return bleu\n",
    "\n",
    "##############################################################################\n",
    "# 3) Weighted N-Gram BLEU: example weighting by token type\n",
    "##############################################################################\n",
    "# A minimal set of Python keywords\n",
    "PY_KEYWORDS = {\n",
    "    \"def\", \"class\", \"return\", \"for\", \"while\", \"if\", \"else\", \"elif\", \"try\", \"except\",\n",
    "    \"with\", \"import\", \"as\", \"from\", \"pass\", \"break\", \"continue\", \"lambda\", \"yield\",\n",
    "    \"raise\", \"in\", \"and\", \"or\", \"not\", \"is\", \"global\", \"del\", \"assert\"\n",
    "}\n",
    "\n",
    "def token_type(token):\n",
    "    \"\"\"\n",
    "    Rough classification:\n",
    "    - \"KEY\" for Python keywords\n",
    "    - \"ID\" for variable/function identifiers (alphanumeric + underscore, not keyword)\n",
    "    - \"NUM\" if purely numeric\n",
    "    - \"OP\" for punctuation/operators\n",
    "    - \"STR\" if token starts/ends with quotes (naive check)\n",
    "    \"\"\"\n",
    "    if token in PY_KEYWORDS:\n",
    "        return \"KEY\"\n",
    "    # naive string check\n",
    "    if (token.startswith(\"'\") and token.endswith(\"'\")) or (token.startswith('\"') and token.endswith('\"')):\n",
    "        return \"STR\"\n",
    "    # numeric check\n",
    "    if token.isdigit():\n",
    "        return \"NUM\"\n",
    "    # check if alphanumeric+_ (identifier-like)\n",
    "    if re.match(r\"^[A-Za-z0-9_]+$\", token):\n",
    "        return \"ID\"\n",
    "    # otherwise operator/punctuation\n",
    "    return \"OP\"\n",
    "\n",
    "def weighted_ngram_bleu(candidate_tokens, reference_tokens, max_n=4, smooth_eps=1e-14):\n",
    "    \"\"\"\n",
    "    Weighted BLEU: if an n-gram is \"key\" or \"id\" or \"num\", we can weight it differently.\n",
    "    For simplicity, we do: KEY=1.2, ID=1.0, NUM=1.0, OP=0.8, STR=1.0\n",
    "    Then we do a clipped count but accumulate those weights.\n",
    "    \"\"\"\n",
    "    # weights for each token-type\n",
    "    weight_map = {\"KEY\":1.2, \"ID\":1.0, \"NUM\":1.0, \"OP\":0.8, \"STR\":1.0}\n",
    "\n",
    "    # We'll define a function to get the \"type-weight product\" for an n-gram\n",
    "    def ngram_weight(ngram):\n",
    "        # average or product? Let's do average of token-type weights\n",
    "        # so the n-gram weight is an average of the token-type weights.\n",
    "        tw_sum = 0.0\n",
    "        for tok in ngram:\n",
    "            tw_sum += weight_map.get(token_type(tok), 1.0)\n",
    "        return tw_sum / len(ngram)\n",
    "\n",
    "    # Count n-grams in candidate and reference\n",
    "    cand_counts = ngram_counts(candidate_tokens, 1)  # for denominator\n",
    "    precisions = []\n",
    "\n",
    "    for n in range(1, max_n+1):\n",
    "        cand_counter = ngram_counts(candidate_tokens, n)\n",
    "        ref_counter  = ngram_counts(reference_tokens, n)\n",
    "        # Weighted clipped sum\n",
    "        weighted_clip_sum = 0.0\n",
    "        total_candidate_ngrams = 0.0\n",
    "\n",
    "        for ngram, cand_val in cand_counter.items():\n",
    "            # how many times does it appear in cand vs ref?\n",
    "            clip_amount = min(cand_val, ref_counter.get(ngram, 0))\n",
    "            if clip_amount > 0:\n",
    "                weighted_clip_sum += clip_amount * ngram_weight(ngram)\n",
    "            total_candidate_ngrams += cand_val * ngram_weight(ngram)\n",
    "\n",
    "        if total_candidate_ngrams < smooth_eps:\n",
    "            prec_n = 0.0\n",
    "        else:\n",
    "            prec_n = (weighted_clip_sum + smooth_eps) / (total_candidate_ngrams + smooth_eps)\n",
    "        precisions.append(prec_n)\n",
    "\n",
    "    # geometric mean\n",
    "    if any(p == 0 for p in precisions):\n",
    "        wbleu = 0.0\n",
    "    else:\n",
    "        avg_log = sum(math.log(p) for p in precisions) / len(precisions)\n",
    "        wbleu = math.exp(avg_log)\n",
    "\n",
    "    # naive brevity penalty\n",
    "    ratio = len(candidate_tokens) / (len(reference_tokens) + smooth_eps)\n",
    "    if ratio < 1.0:\n",
    "        wbleu *= math.exp(1 - 1/ratio)\n",
    "\n",
    "    return wbleu\n",
    "\n",
    "##############################################################################\n",
    "# 4) Syntax (AST) Match: parse Python code + BFS of node types, do n-gram similarity\n",
    "##############################################################################\n",
    "def ast_node_bfs_types(root):\n",
    "    \"\"\"\n",
    "    BFS traversal of AST node types (as strings).\n",
    "    Returns a list of node type names, e.g. [\"Module\", \"FunctionDef\", \"arguments\", \"arg\", ...].\n",
    "    \"\"\"\n",
    "    queue = deque([root])\n",
    "    types = []\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        types.append(type(node).__name__)\n",
    "        for child in ast.iter_child_nodes(node):\n",
    "            queue.append(child)\n",
    "    return types\n",
    "\n",
    "def syntax_match_score(candidate_code, reference_code, smooth_eps=1e-14):\n",
    "    \"\"\"\n",
    "    Parse code into Python AST. Then BFS node types. Then measure 1-gram to 4-gram overlap \n",
    "    (similar to BLEU). Return a float in [0..1].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cand_root = ast.parse(candidate_code)\n",
    "        ref_root  = ast.parse(reference_code)\n",
    "    except Exception:\n",
    "        # If parse fails, return 0\n",
    "        return 0.0\n",
    "\n",
    "    cand_types = ast_node_bfs_types(cand_root)\n",
    "    ref_types  = ast_node_bfs_types(ref_root)\n",
    "\n",
    "    # We'll just reuse `sentence_bleu` logic on these \"type tokens\"\n",
    "    return sentence_bleu(cand_types, ref_types, max_n=4, smooth_eps=smooth_eps)\n",
    "\n",
    "##############################################################################\n",
    "# 5) Data-Flow Match (naive variable \"def\" lines)\n",
    "##############################################################################\n",
    "class DefUseVisitor(ast.NodeVisitor):\n",
    "    \"\"\"\n",
    "    Collect line info of variable definitions in Python code.\n",
    "    e.g. for i in range(5): i is \"defined\" or assigned. \n",
    "    We'll store them as (varname, first_def_line).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.defs = {}\n",
    "\n",
    "    def visit_Name(self, node):\n",
    "        # If this name is being assigned, store line number if not present\n",
    "        if isinstance(node.ctx, ast.Store):\n",
    "            var = node.id\n",
    "            if var not in self.defs:\n",
    "                self.defs[var] = node.lineno\n",
    "        self.generic_visit(node)\n",
    "\n",
    "def dataflow_match_score(candidate_code, reference_code):\n",
    "    \"\"\"\n",
    "    We do a naive approach: gather (var, line_of_def) for each code. \n",
    "    Then measure the overlap. Score ~ overlap / union in [0..1].\n",
    "    \"\"\"\n",
    "    def get_var_defs(code):\n",
    "        try:\n",
    "            root = ast.parse(code)\n",
    "        except:\n",
    "            return set()\n",
    "        v = DefUseVisitor()\n",
    "        v.visit(root)\n",
    "        # Return set of (var, line)\n",
    "        return set(v.defs.items())\n",
    "\n",
    "    cand_defs = get_var_defs(candidate_code)\n",
    "    ref_defs  = get_var_defs(reference_code)\n",
    "\n",
    "    if len(cand_defs) == 0 and len(ref_defs) == 0:\n",
    "        return 1.0  # trivially no variables in both\n",
    "    overlap = len(cand_defs.intersection(ref_defs))\n",
    "    union  = len(cand_defs.union(ref_defs))\n",
    "    return overlap / (union + 1e-14)\n",
    "\n",
    "##############################################################################\n",
    "# 6) Combine the 4 sub-scores into a final CodeBLEU\n",
    "##############################################################################\n",
    "def calc_codebleu(candidate_code, reference_code, alpha=0.25, beta=0.25, gamma=0.25, theta=0.25):\n",
    "    \"\"\"\n",
    "    Returns a dict with sub-scores + final code_bleu in [0..1].\n",
    "    \"\"\"\n",
    "    # 1) Tokenize\n",
    "    cand_tokens = tokenize_code(candidate_code)\n",
    "    ref_tokens  = tokenize_code(reference_code)\n",
    "    # 2) n-gram BLEU\n",
    "    ngram_score = sentence_bleu_self(cand_tokens, ref_tokens, max_n=4)\n",
    "    # 3) Weighted n-gram BLEU\n",
    "    wngram_score = weighted_ngram_bleu(cand_tokens, ref_tokens, max_n=4)\n",
    "    # 4) Syntax (AST) match\n",
    "    syntax_score = syntax_match_score(candidate_code, reference_code)\n",
    "    # 5) Data-flow\n",
    "    dataflow_score = dataflow_match_score(candidate_code, reference_code)\n",
    "    # Weighted sum\n",
    "    codebleu = (alpha*ngram_score \n",
    "               + beta*wngram_score\n",
    "               + gamma*syntax_score\n",
    "               + theta*dataflow_score)\n",
    "    return {\n",
    "        \"ngram_match_score\": ngram_score,\n",
    "        \"weighted_ngram_match_score\": wngram_score,\n",
    "        \"syntax_match_score\": syntax_score,\n",
    "        \"dataflow_match_score\": dataflow_score,\n",
    "        \"code_bleu\": codebleu\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "# Group level corpus BLEU score\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_candidates = []  # each will be a tokenized candidate code snippet\n",
    "all_references = []  # each will be a list containing one tokenized reference code snippet\n",
    "for i in tqdm(range(instances_num), desc=\"BLEU scoring...\"):\n",
    "    # Find index of highest raw score\n",
    "    best_index = np.argmax(scores_raw_all[i])\n",
    "\n",
    "    candidate_str = codes_generated_all[i]\n",
    "    reference_str = codes_ground_truth_all[i]\n",
    "\n",
    "    # Tokenize; this is a simplistic approach!\n",
    "    candidate_tokens = candidate_str.split()\n",
    "    reference_tokens = reference_str.split()\n",
    "\n",
    "    all_candidates.append(candidate_tokens)\n",
    "    # NLTK corpus_bleu expects each reference to be a list-of-lists of tokens\n",
    "    all_references.append([reference_tokens])\n",
    "\n",
    "# Calculate corpus-level BLEU\n",
    "bleu_score = corpus_bleu(all_references, all_candidates)\n",
    "print(\"Corpus-level BLEU score:\", bleu_score)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_all = [0, 5, 10, 20, 40]\n",
    "EM_k_all = np.zeros((len(k_all), 200))\n",
    "BLEU_corpus_k_all = np.zeros((len(k_all), 200))\n",
    "BLEU_sentence_k_all = np.zeros((len(k_all), 200))\n",
    "CodeBLEU_k_all = np.zeros((len(k_all), 200))\n",
    "for k in range(len(k_all)):\n",
    "    # This cell loads the generated code from SCODE_G with the codexglue dataset\n",
    "    top_k = 1\n",
    "    instances_num = 200\n",
    "    scores_raw_all = np.zeros((instances_num, top_k))\n",
    "    codes_generated_all = [] # list of instances_num lists, each list has top_k items\n",
    "    codes_ground_truth_all = [] # list of instances_num code strings\n",
    "    row = 0\n",
    "    with open(\"/Users/dunhan/Desktop/AdvancedNLP/retriever/fancy_retriever/eval/cxg200_Qwen1M_top\" + str(k_all[k]) + \".json\") as f:\n",
    "            data = json.load(f)\n",
    "            for i in range(len(data)):\n",
    "                codes_ground_truth_all.append(data[i][\"ground_truth_code\"])\n",
    "                scores_raw_all[row] = np.array([100.0])\n",
    "                codes_generated_all.append(data[i][\"generated_code\"])\n",
    "                row += 1\n",
    "                if row == instances_num:\n",
    "                    break\n",
    "    EM_all = []\n",
    "    BLEU_corpus_all = []\n",
    "    BLEU_sentence_all = []\n",
    "    CodeBLEU_all = []\n",
    "    for i in range(instances_num):\n",
    "        EM_all.append(EM(codes_generated_all[i], codes_ground_truth_all[i]))\n",
    "        BLEU_corpus_all.append(BLEU_corpus(codes_generated_all[i], codes_ground_truth_all[i]))\n",
    "        BLEU_sentence_all.append(BLEU_sentence(codes_generated_all[i], codes_ground_truth_all[i]))\n",
    "        CodeBLEU_all.append(calc_codebleu(codes_generated_all[i], codes_ground_truth_all[i])[\"code_bleu\"])\n",
    "    EM_k_all[k, :] = np.array(EM_all)\n",
    "    BLEU_corpus_k_all[k, :] = np.array(BLEU_corpus_all)\n",
    "    BLEU_sentence_k_all[k, :] = np.array(BLEU_sentence_all)\n",
    "    CodeBLEU_k_all[k, :] = np.array(CodeBLEU_all)\n",
    "    print(\"EM: \", 100 * np.mean(EM_all))\n",
    "    print(\"BLEU_corpus: \", 100 * np.mean(BLEU_corpus_all))\n",
    "    print(\"BLEU_sentence: \", 100 * np.mean(BLEU_sentence_all))\n",
    "    print(\"CodeBLEU: \", 100 * np.mean(CodeBLEU_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converge = np.zeros((200))\n",
    "for i in range(200):\n",
    "    for a in range(5):\n",
    "        if BLEU_corpus_k_all[a, i] >= 0.98 * BLEU_corpus_k_all[:, i].max():\n",
    "            break\n",
    "    for b in range(5):\n",
    "        if BLEU_sentence_k_all[b, i] >= 0.98 * BLEU_sentence_k_all[:, i].max():\n",
    "            break\n",
    "    for c in range(5):\n",
    "        if CodeBLEU_k_all[c, i] >= 0.98 * CodeBLEU_k_all[:, i].max():\n",
    "            break\n",
    "    converge[i] = np.round(np.mean([a, b, c]))\n",
    "print(\"converge in top_0:\", np.sum(converge == 0)/2, \"%\")\n",
    "print(\"converge in top_5:\", np.sum(converge == 1)/2, \"%\")\n",
    "print(\"converge in top_10:\", np.sum(converge == 2)/2, \"%\")\n",
    "print(\"converge in top_20:\", np.sum(converge == 3)/2, \"%\")\n",
    "print(\"converge in top_40:\", np.sum(converge == 4)/2, \"%\")\n",
    "np.save(\"/Users/dunhan/Desktop/AdvancedNLP/retriever/fancy_retriever/eval/converge.npy\", converge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_all = []\n",
    "for i in range(200):\n",
    "    with open(\"/Users/dunhan/Desktop/AdvancedNLP/retriever/fancy_retriever/eval/cxg200_Qwen1M_top0.json\") as f:\n",
    "        data = json.load(f)\n",
    "        for i in range(len(data)):\n",
    "            prompt_all.append(data[i][\"text_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"easy questions:\")\n",
    "for i in range(200):\n",
    "    if converge[i] <= 1: # converge in top_0 or top_5\n",
    "        print(i)\n",
    "        print(prompt_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"medium questions:\")\n",
    "for i in range(200):\n",
    "    if converge[i] == 2 or converge[i] == 3: # converge in top_10 or top_20\n",
    "        print(i)\n",
    "        print(prompt_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hard questions:\")\n",
    "for i in range(200):\n",
    "    if converge[i] == 4: # converge in top_40\n",
    "        print(i)\n",
    "        print(prompt_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for visualization\n",
    "x_labels = ['top0', 'top5', 'top10', 'top20', 'top40']\n",
    "x = range(len(x_labels))\n",
    "\n",
    "# Scores for each attribute\n",
    "EM = [] # add your EM scores here\n",
    "Corpus_BLEU = [] # add your Corpus_BLEU scores here\n",
    "Sentence_BLEU = [] # add your Sentence_BLEU scores here\n",
    "CodeBLEU = [] # add your CodeBLEU scores here\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, EM, label='EM', color='blue', marker='o')\n",
    "plt.plot(x, Corpus_BLEU, label='Corpus_BLEU', color='green', marker='s')\n",
    "plt.plot(x, Sentence_BLEU, label='Sentence_BLEU', color='red', marker='^')\n",
    "plt.plot(x, CodeBLEU, label='CodeBLEU', color='orange', marker='d')\n",
    "# Formatting\n",
    "plt.xticks(x, x_labels)\n",
    "plt.xlabel('Top-k Retrievals')\n",
    "plt.ylabel('Score')\n",
    "plt.title('SCODE_G Top-200 on CodeXGLUE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeLlama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
