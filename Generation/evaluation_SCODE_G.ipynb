{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell loads the Code_Search_Net subset from the CodeXGLUE dataset (csn) from SCODE_R\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "top_k = 1\n",
    "instances_num = 23107\n",
    "scores_raw_all = np.zeros((instances_num, top_k))\n",
    "codes_retrieved_all = [] # list of instances_num lists, each list has top_k items\n",
    "codes_ground_truth_all = [] # list of instances_num code strings\n",
    "row = 0\n",
    "for file in range(1, 25):\n",
    "    with open(\"/home/ubuntu/fancy_retriever/output_ret/retrieval_chunk_\" + str(file) + \"_top200_result.json\") as f:\n",
    "        data = json.load(f)\n",
    "        for i in range(len(data)):\n",
    "            codes_ground_truth_all.append(data[i][\"answers\"])\n",
    "            scores_raw_all[row] = np.array([data[i][\"ctxs\"][j][\"score\"] for j in range(top_k)])\n",
    "            # codes_retrieved_all.append([data[i][\"ctxs\"][j][\"text\"] for j in range(top_k)])\n",
    "            row += 1\n",
    "            if row == instances_num:\n",
    "                break\n",
    "row = 0\n",
    "with open(\"/home/ubuntu/fancy_retriever/Generation/csn100_Llama31_8B_Instruct.json\") as f:\n",
    "        data = json.load(f)\n",
    "        for i in range(len(data)):\n",
    "            # for SCODE_G evaluation, \"code_retrieved_all\" comes from the code generation model\n",
    "            codes_retrieved_all.append([data[i][\"generated_text\"]])\n",
    "            row += 1\n",
    "            if row == instances_num:\n",
    "                break\n",
    "# data has 1000 items, \n",
    "# for each item \"data[i]\" i in [0, 999], there are associated \"question\" (code prompt, str), \"answers\" (ground truth code, str), and a \"ctxs\"\n",
    "# each \"ctxs\" is a list of dictionaries \"data[i][\"ctxs\"][j]\" j in [0, 199], each dictionary has 5 keys:\n",
    "# ['id' (path, str), 'title' (str), 'text' (retrieved code, str), 'score' (similarity, float), 'has_answer' (bool)]\n",
    "\n",
    "# check for json file loading\n",
    "# print(0 in scores_raw_all) # False\n",
    "# for i in range(len(codes_retrieved_all)): assert len(codes_retrieved_all[i]) == top_k\n",
    "# assert row == len(codes_retrieved_all) == len(codes_ground_truth_all) == instances_num\n",
    "scores_raw_all = scores_raw_all[:100]\n",
    "codes_retrieved_all = codes_retrieved_all[:100]\n",
    "codes_ground_truth_all = codes_ground_truth_all[:100]\n",
    "instances_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell loads the Code_Search_Net subset from the CodeXGLUE dataset (csn) from SCODE_R\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "top_k = 1\n",
    "instances_num = 23107\n",
    "scores_raw_all = np.zeros((instances_num, top_k))\n",
    "codes_retrieved_all = [] # list of instances_num lists, each list has top_k items\n",
    "codes_ground_truth_all = [] # list of instances_num code strings\n",
    "row = 0\n",
    "for file in range(1, 25):\n",
    "    with open(\"/home/ubuntu/fancy_retriever/output_ret/retrieval_chunk_\" + str(file) + \"_top200_result.json\") as f:\n",
    "        data = json.load(f)\n",
    "        for i in range(len(data)):\n",
    "            codes_ground_truth_all.append(data[i][\"answers\"])\n",
    "            scores_raw_all[row] = np.array([data[i][\"ctxs\"][j][\"score\"] for j in range(top_k)])\n",
    "            # codes_retrieved_all.append([data[i][\"ctxs\"][j][\"text\"] for j in range(top_k)])\n",
    "            row += 1\n",
    "            if row == instances_num:\n",
    "                break\n",
    "row = 0\n",
    "with open(\"/home/ubuntu/fancy_retriever/Generation/csn10_Llama31_8B_Instruct.json\") as f:\n",
    "        data = json.load(f)\n",
    "        for i in range(len(data)):\n",
    "            # for SCODE_G evaluation, \"code_retrieved_all\" comes from the code generation model\n",
    "            codes_retrieved_all.append([data[i][\"generated_text\"]])\n",
    "            row += 1\n",
    "            if row == instances_num:\n",
    "                break\n",
    "# data has 1000 items, \n",
    "# for each item \"data[i]\" i in [0, 999], there are associated \"question\" (code prompt, str), \"answers\" (ground truth code, str), and a \"ctxs\"\n",
    "# each \"ctxs\" is a list of dictionaries \"data[i][\"ctxs\"][j]\" j in [0, 199], each dictionary has 5 keys:\n",
    "# ['id' (path, str), 'title' (str), 'text' (retrieved code, str), 'score' (similarity, float), 'has_answer' (bool)]\n",
    "\n",
    "# check for json file loading\n",
    "# print(0 in scores_raw_all) # False\n",
    "# for i in range(len(codes_retrieved_all)): assert len(codes_retrieved_all[i]) == top_k\n",
    "print(row, len(codes_retrieved_all), len(codes_ground_truth_all), instances_num)\n",
    "scores_raw_all = scores_raw_all[10:20]\n",
    "codes_ground_truth_all = codes_ground_truth_all[10:20]\n",
    "instances_num = len(codes_ground_truth_all)\n",
    "print(row, len(codes_retrieved_all), len(codes_ground_truth_all), instances_num)\n",
    "\n",
    "import re\n",
    "def extract_python_code_block(text):\n",
    "    \"\"\"\n",
    "    Extracts the first Python code block from a string formatted like:\n",
    "    ```python\n",
    "    <code>\n",
    "    ```\n",
    "    \"\"\"\n",
    "    pattern = r\"```python\\s+(.*?)```\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return text\n",
    "codes_retrieved_new = []\n",
    "for i in range(10):\n",
    "    codes_retrieved_new.append([extract_python_code_block(codes_retrieved_all[i][0])])\n",
    "    # print(codes_retrieved_new[i])\n",
    "\n",
    "codes_retrieved_all = codes_retrieved_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell loads the Code_RAG_Bench dataset (crb) from SCODE_R\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "top_k = 1\n",
    "instances_num = 100\n",
    "scores_raw_all = np.zeros((instances_num, top_k))\n",
    "codes_retrieved_all = [] # list of instances_num lists, each list has top_k items\n",
    "codes_ground_truth_all = [] # list of instances_num code strings\n",
    "row = 0\n",
    "with open(\"/home/ubuntu/fancy_retriever/output_ret/coderagbench_500.json\") as f:\n",
    "        data = json.load(f)\n",
    "        for i in range(len(data)):\n",
    "            codes_ground_truth_all.append(data[i][\"answers\"])\n",
    "            scores_raw_all[row] = np.array([data[i][\"ctxs\"][j][\"score\"] for j in range(top_k)])\n",
    "            # codes_retrieved_all.append([data[i][\"ctxs\"][j][\"text\"] for j in range(top_k)])\n",
    "            row += 1\n",
    "            if row == instances_num:\n",
    "                break\n",
    "row = 0\n",
    "with open(\"/home/ubuntu/fancy_retriever/Generation/crb100_Llama31_8B_Instruct.json\") as f:\n",
    "        data = json.load(f)\n",
    "        for i in range(len(data)):\n",
    "            # for SCODE_G evaluation, \"code_retrieved_all\" comes from the code generation model\n",
    "            codes_retrieved_all.append([data[i][\"generated_text\"]])\n",
    "            row += 1\n",
    "            if row == instances_num:\n",
    "                break\n",
    "# data has 1000 items, \n",
    "# for each item \"data[i]\" i in [0, 999], there are associated \"question\" (code prompt, str), \"answers\" (ground truth code, str), and a \"ctxs\"\n",
    "# each \"ctxs\" is a list of dictionaries \"data[i][\"ctxs\"][j]\" j in [0, 199], each dictionary has 5 keys:\n",
    "# ['id' (path, str), 'title' (str), 'text' (retrieved code, str), 'score' (similarity, float), 'has_answer' (bool)]\n",
    "\n",
    "# check for json file loading\n",
    "# print(0 in scores_raw_all) # False\n",
    "# for i in range(len(codes_retrieved_all)): assert len(codes_retrieved_all[i]) == top_k\n",
    "assert len(codes_retrieved_all) == len(codes_ground_truth_all) == instances_num\n",
    "scores_raw_all = scores_raw_all[:100]\n",
    "codes_retrieved_all = codes_retrieved_all[:100]\n",
    "codes_ground_truth_all = codes_ground_truth_all[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_matches = 0\n",
    "for i in range(instances_num):\n",
    "    # Find the index of the top raw score for instance i\n",
    "    best_index = np.argmax(scores_raw_all[i])\n",
    "    # Compare the retrieved code at best_index to the ground truth code\n",
    "    if codes_retrieved_all[i][best_index] == codes_ground_truth_all[i]:\n",
    "        exact_matches += 1\n",
    "\n",
    "# Compute exact match rate\n",
    "exact_match_rate = exact_matches / instances_num\n",
    "print(f\"Exact Match Rate: {exact_match_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_candidates = []  # each will be a tokenized candidate code snippet\n",
    "all_references = []  # each will be a list containing one tokenized reference code snippet\n",
    "for i in tqdm(range(instances_num), desc=\"BLEU scoring...\"):\n",
    "    # Find index of highest raw score\n",
    "    best_index = np.argmax(scores_raw_all[i])\n",
    "\n",
    "    candidate_str = codes_retrieved_all[i][best_index]\n",
    "    reference_str = codes_ground_truth_all[i]\n",
    "\n",
    "    # Tokenize; this is a simplistic approach!\n",
    "    candidate_tokens = candidate_str.split()\n",
    "    reference_tokens = reference_str.split()\n",
    "\n",
    "    all_candidates.append(candidate_tokens)\n",
    "    # NLTKâ€™s corpus_bleu expects each reference to be a list-of-lists of tokens\n",
    "    all_references.append([reference_tokens])\n",
    "\n",
    "# Calculate corpus-level BLEU\n",
    "bleu_score = corpus_bleu(all_references, all_candidates)\n",
    "print(\"Corpus-level BLEU score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Assuming you have:\n",
    "#   top_k, instances_num\n",
    "#   scores_raw_all: shape (instances_num, top_k)\n",
    "#   codes_retrieved_all: list of length instances_num, each a list of length top_k\n",
    "#   codes_ground_truth_all: list of length instances_num\n",
    "\n",
    "# 1. Tokenizer (defined above)\n",
    "import re\n",
    "\n",
    "def tokenize_code(code_str):\n",
    "    tokens = re.findall(r\"[A-Za-z0-9_]+|\\S\", code_str)\n",
    "    return tokens\n",
    "\n",
    "smooth_fn = SmoothingFunction().method4  # method4 is a common smoothing method\n",
    "\n",
    "bleu_scores = []\n",
    "\n",
    "for i in range(instances_num):\n",
    "    # Index of the highest raw score\n",
    "    best_index = np.argmax(scores_raw_all[i])\n",
    "    \n",
    "    # Retrieve candidate and reference code strings\n",
    "    candidate_str = codes_retrieved_all[i][best_index]\n",
    "    reference_str = codes_ground_truth_all[i]\n",
    "    \n",
    "    # Tokenize\n",
    "    candidate_tokens = tokenize_code(candidate_str)\n",
    "    reference_tokens = tokenize_code(reference_str)\n",
    "    \n",
    "    # In NLTK's BLEU, reference is a list-of-lists\n",
    "    # here we only have one reference (the ground truth)\n",
    "    references = [reference_tokens]\n",
    "\n",
    "    # Calculate *sentence-level* BLEU with 1 reference\n",
    "    score = sentence_bleu(\n",
    "        references,               # list of references\n",
    "        candidate_tokens,         # candidate tokens\n",
    "        smoothing_function=smooth_fn\n",
    "    )\n",
    "    bleu_scores.append(score)\n",
    "\n",
    "# Finally, average these instance-level BLEU scores.\n",
    "average_bleu = np.mean(bleu_scores)\n",
    "# Often reported as a percentage or a float in [0..100].\n",
    "print(f\"Average sentence-level BLEU: {average_bleu:.4f} (i.e., {average_bleu*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter, deque\n",
    "\n",
    "##############################################################################\n",
    "# 1) Simple Code Tokenizer\n",
    "##############################################################################\n",
    "\n",
    "def tokenize_code(code_str):\n",
    "    \"\"\"\n",
    "    Splits code on boundaries between alphanumeric/underscore and non-alphanumeric.\n",
    "    E.g., \"def foo(x): return x+1\" -> [\"def\", \"foo\", \"(\", \"x\", \")\", \":\", \"return\", \"x\", \"+\", \"1\"]\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r\"[A-Za-z0-9_]+|\\S\", code_str)\n",
    "    return tokens\n",
    "\n",
    "##############################################################################\n",
    "# 2) N-Gram BLEU (token-based), approximate from-scratch\n",
    "##############################################################################\n",
    "\n",
    "def ngram_counts(tokens, n):\n",
    "    \"\"\"\n",
    "    Return a Counter of all n-grams of length `n` in `tokens`.\n",
    "    Example: tokens = ['def','foo','(',')']\n",
    "             n=2 -> Counter({('def','foo'):1, ('foo','('):1, ('(',')'):1})\n",
    "    \"\"\"\n",
    "    counts = Counter()\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[i:i+n])\n",
    "        counts[ngram] += 1\n",
    "    return counts\n",
    "\n",
    "def clipped_ngram_count(candidate_tokens, reference_tokens, n):\n",
    "    \"\"\"\n",
    "    Compute clipped count = sum of min(candidate_count(ngram), reference_count(ngram)) over all n-grams.\n",
    "    \"\"\"\n",
    "    cand_counter = ngram_counts(candidate_tokens, n)\n",
    "    ref_counter  = ngram_counts(reference_tokens, n)\n",
    "    clipped_sum = 0\n",
    "    for ngram, cand_val in cand_counter.items():\n",
    "        clipped_sum += min(cand_val, ref_counter.get(ngram, 0))\n",
    "    return clipped_sum\n",
    "\n",
    "def sentence_bleu(candidate_tokens, reference_tokens, max_n=4, smooth_eps=1e-14):\n",
    "    \"\"\"\n",
    "    Approximate sentence-level BLEU (no brevity penalty in this simplified version),\n",
    "    with basic smoothing to avoid zero counts.\n",
    "    \"\"\"\n",
    "    # Precision for each n-gram\n",
    "    precisions = []\n",
    "    for n in range(1, max_n+1):\n",
    "        numerator = clipped_ngram_count(candidate_tokens, reference_tokens, n)\n",
    "        denominator = max(len(candidate_tokens) - n + 1, 1)\n",
    "        # Add smoothing to avoid 0\n",
    "        prec_n = (numerator + smooth_eps) / (denominator + smooth_eps)\n",
    "        precisions.append(prec_n)\n",
    "\n",
    "    # Geometric mean of precisions\n",
    "    # BLEU = exp( (1/N) * sum(log p_n) ), N=4 typically\n",
    "    if any(p == 0 for p in precisions):\n",
    "        bleu = 0.0\n",
    "    else:\n",
    "        avg_log = sum(math.log(p) for p in precisions) / len(precisions)\n",
    "        bleu = math.exp(avg_log)\n",
    "\n",
    "    # Very naive length penalty if you want (optional).\n",
    "    # Let's skip or do a minimal ratio:\n",
    "    ratio = len(candidate_tokens) / (len(reference_tokens) + smooth_eps)\n",
    "    if ratio < 1.0:\n",
    "        # Simple brevity penalty\n",
    "        bleu *= math.exp(1 - 1/ratio)\n",
    "\n",
    "    return bleu\n",
    "\n",
    "##############################################################################\n",
    "# 3) Weighted N-Gram BLEU: example weighting by token type\n",
    "##############################################################################\n",
    "\n",
    "# A minimal set of Python keywords\n",
    "PY_KEYWORDS = {\n",
    "    \"def\", \"class\", \"return\", \"for\", \"while\", \"if\", \"else\", \"elif\", \"try\", \"except\",\n",
    "    \"with\", \"import\", \"as\", \"from\", \"pass\", \"break\", \"continue\", \"lambda\", \"yield\",\n",
    "    \"raise\", \"in\", \"and\", \"or\", \"not\", \"is\", \"global\", \"del\", \"assert\"\n",
    "}\n",
    "\n",
    "def token_type(token):\n",
    "    \"\"\"\n",
    "    Rough classification:\n",
    "    - \"KEY\" for Python keywords\n",
    "    - \"ID\" for variable/function identifiers (alphanumeric + underscore, not keyword)\n",
    "    - \"NUM\" if purely numeric\n",
    "    - \"OP\" for punctuation/operators\n",
    "    - \"STR\" if token starts/ends with quotes (naive check)\n",
    "    \"\"\"\n",
    "    if token in PY_KEYWORDS:\n",
    "        return \"KEY\"\n",
    "    # naive string check\n",
    "    if (token.startswith(\"'\") and token.endswith(\"'\")) or (token.startswith('\"') and token.endswith('\"')):\n",
    "        return \"STR\"\n",
    "    # numeric check\n",
    "    if token.isdigit():\n",
    "        return \"NUM\"\n",
    "    # check if alphanumeric+_ (identifier-like)\n",
    "    if re.match(r\"^[A-Za-z0-9_]+$\", token):\n",
    "        return \"ID\"\n",
    "    # otherwise operator/punctuation\n",
    "    return \"OP\"\n",
    "\n",
    "def weighted_ngram_bleu(candidate_tokens, reference_tokens, max_n=4, smooth_eps=1e-14):\n",
    "    \"\"\"\n",
    "    Weighted BLEU: if an n-gram is \"key\" or \"id\" or \"num\", we can weight it differently.\n",
    "    For simplicity, we do: KEY=1.2, ID=1.0, NUM=1.0, OP=0.8, STR=1.0\n",
    "    Then we do a clipped count but accumulate those weights.\n",
    "    \"\"\"\n",
    "    # weights for each token-type\n",
    "    weight_map = {\"KEY\":1.2, \"ID\":1.0, \"NUM\":1.0, \"OP\":0.8, \"STR\":1.0}\n",
    "\n",
    "    # We'll define a function to get the \"type-weight product\" for an n-gram\n",
    "    def ngram_weight(ngram):\n",
    "        # average or product? Let's do average of token-type weights\n",
    "        # so the n-gram weight is an average of the token-type weights.\n",
    "        tw_sum = 0.0\n",
    "        for tok in ngram:\n",
    "            tw_sum += weight_map.get(token_type(tok), 1.0)\n",
    "        return tw_sum / len(ngram)\n",
    "\n",
    "    # Count n-grams in candidate and reference\n",
    "    cand_counts = ngram_counts(candidate_tokens, 1)  # for denominator\n",
    "    precisions = []\n",
    "\n",
    "    for n in range(1, max_n+1):\n",
    "        cand_counter = ngram_counts(candidate_tokens, n)\n",
    "        ref_counter  = ngram_counts(reference_tokens, n)\n",
    "        # Weighted clipped sum\n",
    "        weighted_clip_sum = 0.0\n",
    "        total_candidate_ngrams = 0.0\n",
    "\n",
    "        for ngram, cand_val in cand_counter.items():\n",
    "            # how many times does it appear in cand vs ref?\n",
    "            clip_amount = min(cand_val, ref_counter.get(ngram, 0))\n",
    "            if clip_amount > 0:\n",
    "                weighted_clip_sum += clip_amount * ngram_weight(ngram)\n",
    "            total_candidate_ngrams += cand_val * ngram_weight(ngram)\n",
    "\n",
    "        if total_candidate_ngrams < smooth_eps:\n",
    "            prec_n = 0.0\n",
    "        else:\n",
    "            prec_n = (weighted_clip_sum + smooth_eps) / (total_candidate_ngrams + smooth_eps)\n",
    "        precisions.append(prec_n)\n",
    "\n",
    "    # geometric mean\n",
    "    if any(p == 0 for p in precisions):\n",
    "        wbleu = 0.0\n",
    "    else:\n",
    "        avg_log = sum(math.log(p) for p in precisions) / len(precisions)\n",
    "        wbleu = math.exp(avg_log)\n",
    "\n",
    "    # naive brevity penalty\n",
    "    ratio = len(candidate_tokens) / (len(reference_tokens) + smooth_eps)\n",
    "    if ratio < 1.0:\n",
    "        wbleu *= math.exp(1 - 1/ratio)\n",
    "\n",
    "    return wbleu\n",
    "\n",
    "##############################################################################\n",
    "# 4) Syntax (AST) Match: parse Python code + BFS of node types, do n-gram similarity\n",
    "##############################################################################\n",
    "\n",
    "def ast_node_bfs_types(root):\n",
    "    \"\"\"\n",
    "    BFS traversal of AST node types (as strings).\n",
    "    Returns a list of node type names, e.g. [\"Module\", \"FunctionDef\", \"arguments\", \"arg\", ...].\n",
    "    \"\"\"\n",
    "    queue = deque([root])\n",
    "    types = []\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        types.append(type(node).__name__)\n",
    "        for child in ast.iter_child_nodes(node):\n",
    "            queue.append(child)\n",
    "    return types\n",
    "\n",
    "def syntax_match_score(candidate_code, reference_code, smooth_eps=1e-14):\n",
    "    \"\"\"\n",
    "    Parse code into Python AST. Then BFS node types. Then measure 1-gram to 4-gram overlap \n",
    "    (similar to BLEU). Return a float in [0..1].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cand_root = ast.parse(candidate_code)\n",
    "        ref_root  = ast.parse(reference_code)\n",
    "    except Exception:\n",
    "        # If parse fails, return 0\n",
    "        return 0.0\n",
    "\n",
    "    cand_types = ast_node_bfs_types(cand_root)\n",
    "    ref_types  = ast_node_bfs_types(ref_root)\n",
    "\n",
    "    # We'll just reuse `sentence_bleu` logic on these \"type tokens\"\n",
    "    return sentence_bleu(cand_types, ref_types, max_n=4, smooth_eps=smooth_eps)\n",
    "\n",
    "##############################################################################\n",
    "# 5) Data-Flow Match (naive variable \"def\" lines)\n",
    "##############################################################################\n",
    "\n",
    "class DefUseVisitor(ast.NodeVisitor):\n",
    "    \"\"\"\n",
    "    Collect line info of variable definitions in Python code.\n",
    "    e.g. for i in range(5): i is \"defined\" or assigned. \n",
    "    We'll store them as (varname, first_def_line).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.defs = {}\n",
    "\n",
    "    def visit_Name(self, node):\n",
    "        # If this name is being assigned, store line number if not present\n",
    "        if isinstance(node.ctx, ast.Store):\n",
    "            var = node.id\n",
    "            if var not in self.defs:\n",
    "                self.defs[var] = node.lineno\n",
    "        self.generic_visit(node)\n",
    "\n",
    "def dataflow_match_score(candidate_code, reference_code):\n",
    "    \"\"\"\n",
    "    We do a naive approach: gather (var, line_of_def) for each code. \n",
    "    Then measure the overlap. Score ~ overlap / union in [0..1].\n",
    "    \"\"\"\n",
    "    def get_var_defs(code):\n",
    "        try:\n",
    "            root = ast.parse(code)\n",
    "        except:\n",
    "            return set()\n",
    "        v = DefUseVisitor()\n",
    "        v.visit(root)\n",
    "        # Return set of (var, line)\n",
    "        return set(v.defs.items())\n",
    "\n",
    "    cand_defs = get_var_defs(candidate_code)\n",
    "    ref_defs  = get_var_defs(reference_code)\n",
    "\n",
    "    if len(cand_defs) == 0 and len(ref_defs) == 0:\n",
    "        return 1.0  # trivially no variables in both\n",
    "    overlap = len(cand_defs.intersection(ref_defs))\n",
    "    union  = len(cand_defs.union(ref_defs))\n",
    "    return overlap / (union + 1e-14)\n",
    "\n",
    "##############################################################################\n",
    "# 6) Combine the 4 sub-scores into a final CodeBLEU\n",
    "##############################################################################\n",
    "\n",
    "def calc_codebleu(candidate_code, reference_code, alpha=0.25, beta=0.25, gamma=0.25, theta=0.25):\n",
    "    \"\"\"\n",
    "    Returns a dict with sub-scores + final code_bleu in [0..1].\n",
    "    \"\"\"\n",
    "    # 1) Tokenize\n",
    "    cand_tokens = tokenize_code(candidate_code)\n",
    "    ref_tokens  = tokenize_code(reference_code)\n",
    "\n",
    "    # 2) n-gram BLEU\n",
    "    ngram_score = sentence_bleu(cand_tokens, ref_tokens, max_n=4)\n",
    "\n",
    "    # 3) Weighted n-gram BLEU\n",
    "    wngram_score = weighted_ngram_bleu(cand_tokens, ref_tokens, max_n=4)\n",
    "\n",
    "    # 4) Syntax (AST) match\n",
    "    syntax_score = syntax_match_score(candidate_code, reference_code)\n",
    "\n",
    "    # 5) Data-flow\n",
    "    dataflow_score = dataflow_match_score(candidate_code, reference_code)\n",
    "\n",
    "    # Weighted sum\n",
    "    codebleu = (alpha*ngram_score \n",
    "               + beta*wngram_score\n",
    "               + gamma*syntax_score\n",
    "               + theta*dataflow_score)\n",
    "\n",
    "    return {\n",
    "        \"ngram_match_score\": ngram_score,\n",
    "        \"weighted_ngram_match_score\": wngram_score,\n",
    "        \"syntax_match_score\": syntax_score,\n",
    "        \"dataflow_match_score\": dataflow_score,\n",
    "        \"code_bleu\": codebleu\n",
    "    }\n",
    "\n",
    "##############################################################################\n",
    "# 7) Evaluate a code retriever: for each instance, pick the best snippet \n",
    "#    and compute average CodeBLEU\n",
    "##############################################################################\n",
    "\n",
    "def evaluate_codebleu_for_retriever(\n",
    "    scores_raw_all, \n",
    "    codes_retrieved_all, \n",
    "    codes_ground_truth_all,\n",
    "    alpha=0.25, beta=0.25, gamma=0.25, theta=0.25\n",
    "):\n",
    "    \"\"\"\n",
    "    - scores_raw_all: shape (num_instances, top_k)\n",
    "    - codes_retrieved_all: list of length num_instances, each a list of top_k code strings\n",
    "    - codes_ground_truth_all: list of length num_instances, each a single ground-truth code string\n",
    "    - returns a dict with average sub-scores and final code_bleu across all instances\n",
    "    \"\"\"\n",
    "    num_instances = scores_raw_all.shape[0]\n",
    "    ngram_vals = []\n",
    "    wngram_vals = []\n",
    "    syntax_vals = []\n",
    "    dataflow_vals = []\n",
    "    codebleu_vals = []\n",
    "\n",
    "    for i in tqdm(range(num_instances), desc=\"Evaluating CodeBLEU...\"):\n",
    "        # 1) Find best snippet\n",
    "        best_idx = np.argmax(scores_raw_all[i])\n",
    "        candidate_code = codes_retrieved_all[i][best_idx]\n",
    "        reference_code = codes_ground_truth_all[i]\n",
    "\n",
    "        # 2) Calculate sub-scores\n",
    "        result = calc_codebleu(\n",
    "            candidate_code, \n",
    "            reference_code, \n",
    "            alpha=alpha, \n",
    "            beta=beta, \n",
    "            gamma=gamma, \n",
    "            theta=theta\n",
    "        )\n",
    "        # But note: we used the same alpha,beta,gamma,theta in calc_codebleu \n",
    "        # for the final combination. You can also do sub-scores separately \n",
    "        # and combine them outside.\n",
    "\n",
    "        ngram_vals.append(result[\"ngram_match_score\"])\n",
    "        wngram_vals.append(result[\"weighted_ngram_match_score\"])\n",
    "        syntax_vals.append(result[\"syntax_match_score\"])\n",
    "        dataflow_vals.append(result[\"dataflow_match_score\"])\n",
    "        codebleu_vals.append(result[\"code_bleu\"])\n",
    "\n",
    "    # Average across all instances\n",
    "    avg_ngram   = np.mean(ngram_vals)\n",
    "    avg_wngram  = np.mean(wngram_vals)\n",
    "    avg_syntax  = np.mean(syntax_vals)\n",
    "    avg_dataflow= np.mean(dataflow_vals)\n",
    "    avg_codebleu= np.mean(codebleu_vals)\n",
    "\n",
    "    return {\n",
    "        \"avg_ngram_match_score\": avg_ngram,\n",
    "        \"avg_weighted_ngram_match_score\": avg_wngram,\n",
    "        \"avg_syntax_match_score\": avg_syntax,\n",
    "        \"avg_dataflow_match_score\": avg_dataflow,\n",
    "        \"avg_code_bleu\": avg_codebleu\n",
    "    }\n",
    "\n",
    "##############################################################################\n",
    "# 8) Run the evaluation for the retriever\n",
    "results = evaluate_codebleu_for_retriever(\n",
    "    scores_raw_all, codes_retrieved_all, codes_ground_truth_all,\n",
    "    alpha=0.25, beta=0.25, gamma=0.25, theta=0.25\n",
    ")\n",
    "\n",
    "print(\"CodeBLEU sub-scores:\")\n",
    "print(\"  ngram_match_score:\", results[\"avg_ngram_match_score\"])\n",
    "print(\"  weighted_ngram_match_score:\", results[\"avg_weighted_ngram_match_score\"])\n",
    "print(\"  syntax_match_score:\", results[\"avg_syntax_match_score\"])\n",
    "print(\"  dataflow_match_score:\", results[\"avg_dataflow_match_score\"])\n",
    "print(f\"Overall CodeBLEU ~ {results['avg_code_bleu']:.4f} in [0..1]\")\n",
    "print(f\"                ~ {results['avg_code_bleu']*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeLlama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
